{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Audio_6]화자_분리(Speaker_Diarization).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgeQ7hPvqwks"
      },
      "source": [
        "# 화자 분리(Speaker Diarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMmUI654-3im"
      },
      "source": [
        "* Speaker Diarization(화자 분리)는 오디오에서 각 부분에서의 화자를 인식하는 기술\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "LfPTvN50-q47"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK5Su1qK4UH-"
      },
      "source": [
        "## UIS-RNN\n",
        "- 대표적인 모델 Unbounded Interleaved-State Recurrent Neural Network (UIS-RNN)\n",
        "    - https://github.com/google/uis-rnn\n",
        "- 2019년 'Fully Supervised Speaker Diarization'란 이름으로 발표\n",
        "- 'Fully'는 화자 수 추정을 포함하여 화자 분리 시스템의 모든 구성 요소가 감독되는 방식으로 교육되어 사용 가능한 레이블이 지정된 데이터의 양을 늘림으로써 이점을 얻을 수 있음을 의미\n",
        "- 모든 화자의 임베딩이 매개 변수 공유 RNN (Recurrent Neural Network)에 의해 모델링되고 시간 도메인에서 인터리브 된 서로 다른 RNN 상태를 사용하여 서로 다른\n",
        "화자를 구별\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "fT3AcPRPBT1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 파란색, 노란색, 분홍색 및 녹색의 네 가지 가능한 화자가 있는 위의 예를 보면 (이는 임의적이며 실제로 더\n",
        "많을 수 있다.) \n",
        "- 각 화자는 자체 RNN 인스턴스 (모든 화자간에 공유되는 공통 초기 상태)로 시작\n",
        "- 이 화자의 새 임베딩이 주어지면 RNN 상태를 계속 업데이트\n",
        "- 위 예에서 파란색 화자는 다른 화자인 노란색이 들어올 때까지 RNN 상태를 계속 업데이트\n",
        "- 새 스피커 녹색이 입력되면 새 RNN 인스턴스로 시작"
      ],
      "metadata": {
        "id": "zP-ze2kcGQ4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- waveform에 화자 정보를 label에 maping하기에는 상당히 어려움이 있다.\n",
        "- UIS-RNN에서는 데이터 셋 제공\n",
        "```\n",
        "https://github.com/google/uis-rnn/blob/master/data/toy_training_data.npz?raw=True\n",
        "https://github.com/google/uis-rnn/blob/master/data/toy_testing_data.npz?raw=True\n",
        "```\n",
        "- waveform을 Embedding은 d-vector을 이용했다고 git과 논문에 표시.   \n",
        "※ d-vector 는 심층 신경망으로부터 추출된 특징"
      ],
      "metadata": {
        "id": "X8Z5JMuRCX9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고하여 진행\n",
        "https://pypi.org/project/uisrnn/"
      ],
      "metadata": {
        "id": "mK2TfjeAH_rk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD9SW7L24gxo"
      },
      "source": [
        "## 라이브러리 설치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei2VWoib-yIz"
      },
      "source": [
        "\n",
        "*   uisrnn을 간편하게 구현할 수 있는 uisrnn 라이브러리가 존재\n",
        "*   여기서는 uisrnn 라이브러리를 통해 uisrnn을 구현하고 학습, 평가\n",
        "*   실습을 위해 uisrnn과 easydict 라이브러리를 설치\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMoTU9k-q3qX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e60f85-1297-47bb-d35a-c0c07f8e25b4"
      },
      "source": [
        "!pip install uisrnn easydict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uisrnn\n",
            "  Downloading uisrnn-0.1.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n",
            "Installing collected packages: uisrnn\n",
            "Successfully installed uisrnn-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb4Th_h64eoX"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD8-pfRw-mcs"
      },
      "source": [
        "* 학습 및 평가에는 uisrnn 라이브러리에서 제공하는 sample dataset을 사용\n",
        "* urlretrieve를 통해 url에서 데이터를 받아옴\n",
        "* https://github.com/google/uis-rnn/blob/master/data/toy_training_data.npz?raw=True\n",
        "* https://github.com/google/uis-rnn/blob/master/data/toy_testing_data.npz?raw=True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a7r3t1Dt-RD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544376c1-58e0-46b1-c212-0eb8c2188372"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "training_url = 'https://github.com/google/uis-rnn/blob/master/data/toy_training_data.npz?raw=True'\n",
        "urllib.request.urlretrieve(training_url, './toy_training_data.npz')\n",
        "\n",
        "testing_url = 'https://github.com/google/uis-rnn/blob/master/data/toy_testing_data.npz?raw=True'\n",
        "urllib.request.urlretrieve(testing_url, './toy_testing_data.npz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./toy_testing_data.npz', <http.client.HTTPMessage at 0x7fb03bcd9490>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECZnZSqb_8oT"
      },
      "source": [
        "\n",
        "\n",
        "*   다운로드한 데이터에서 데이터를 받아오고 sequence와 label을 분리\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import uisrnn\n",
        "\n",
        "train_data = np.load('/content/toy_training_data.npz', allow_pickle = True)\n",
        "test_data = np.load('/content/toy_testing_data.npz', allow_pickle = True)"
      ],
      "metadata": {
        "id": "_xuhe-1EvO8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3izLMgLGENk",
        "outputId": "b2e9c0f7-7424-4cb4-dc3e-31d4018d4aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_sequence', 'train_cluster_id']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtKo3Ncz4Z42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e04dd4-eb37-4ed4-9148-f527885fd0fe"
      },
      "source": [
        "train_sequences = train_data['train_sequence']\n",
        "train_cluster_ids = train_data['train_cluster_id']\n",
        "\n",
        "test_sequences = test_data['test_sequences']\n",
        "test_cluster_ids = test_data['test_cluster_ids']\n",
        "\n",
        "\n",
        "print(train_sequences[0])\n",
        "print(train_cluster_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.13890201e-03  3.82177185e-02 -6.48594588e-03  9.89223027e-03\n",
            "  2.46802228e-02 -7.36468230e-02  2.03830034e-01  1.03609769e-02\n",
            "  2.78913034e-02 -8.29202631e-02  7.39009311e-02 -2.79328340e-02\n",
            " -6.23055992e-02 -1.47791947e-02  2.20468904e-02 -3.72584180e-02\n",
            " -3.73022909e-02  5.63643239e-02 -1.46196790e-02  1.10949987e-01\n",
            "  5.04364679e-03  1.40099865e-02  1.29598757e-02 -4.43333542e-02\n",
            " -3.59590614e-02  2.20498834e-02  4.16737052e-02  7.49229353e-02\n",
            " -3.58573846e-02 -4.25351661e-02  5.25247907e-02 -4.11656415e-02\n",
            " -1.82226172e-02 -1.56861532e-02  5.64970734e-02 -2.42418862e-02\n",
            " -1.11625780e-02  7.54742372e-02 -5.10478402e-02 -3.42293490e-02\n",
            "  1.46031251e-02  7.86327315e-03 -1.16638640e-01  3.91309886e-02\n",
            "  2.32791874e-02  3.27887268e-02  4.13308623e-02  4.39213114e-02\n",
            " -5.47886145e-02 -5.87707558e-02 -1.85377181e-01 -1.35295973e-02\n",
            "  3.29825421e-02 -5.95737140e-02 -2.20608949e-03 -1.70537428e-02\n",
            " -8.79762787e-03 -1.62059722e-02 -5.61071781e-02 -1.97304611e-02\n",
            " -2.90384742e-02 -3.11128119e-02  7.27262851e-02  5.83151991e-02\n",
            "  1.24445067e-03  5.69109371e-02 -2.69696806e-02 -5.70648984e-02\n",
            " -3.89104186e-03 -5.59011290e-02  2.31921134e-02  5.88487088e-02\n",
            "  7.15712017e-02 -6.46032758e-03 -1.09089480e-01 -7.78854191e-03\n",
            "  1.49371501e-01 -2.84559884e-02  1.23169173e-02  1.90116366e-02\n",
            "  3.44064052e-02 -5.65435003e-02  6.46177502e-02  1.09285109e-01\n",
            " -6.96435833e-02  7.47680169e-02 -1.59355923e-02  4.55065656e-02\n",
            " -1.95520173e-02  1.12448958e-01  1.11984080e-01  3.18457020e-02\n",
            "  1.41072395e-02  4.45936626e-02  2.84863978e-02  1.67042688e-02\n",
            "  7.78393061e-02  3.51518431e-02  8.23092471e-02 -7.89623372e-02\n",
            "  1.16515357e-01  5.98508995e-02  2.42911922e-02 -1.63803383e-02\n",
            "  6.48714811e-02  5.82638530e-03 -4.30380773e-02  8.66615585e-02\n",
            "  7.71146994e-02  1.11770389e-02  2.28161773e-02 -3.62089868e-02\n",
            " -2.70689258e-02 -3.43603518e-02 -3.73734491e-03 -1.40260241e-02\n",
            "  1.60467176e-01 -1.01266015e-01 -1.06352109e-02 -4.59805046e-02\n",
            " -6.40409646e-02  2.80046097e-02  3.47216708e-02 -7.68884641e-02\n",
            " -8.05516877e-03 -3.44963380e-02  5.67872210e-02 -7.48184006e-02\n",
            "  3.35263498e-02 -8.57011985e-03 -1.45064358e-02  4.30031803e-02\n",
            " -8.01933846e-02 -7.55845193e-02  1.52430526e-01 -3.42324984e-02\n",
            " -5.04349925e-03  8.89537665e-02 -3.33970485e-02 -2.64865908e-03\n",
            " -8.85357368e-02  2.81727668e-02  2.62973125e-02  1.53502363e-01\n",
            "  4.44135742e-02 -1.48051021e-02  2.89924297e-02 -4.79096446e-02\n",
            " -1.12376463e-01 -3.75842516e-02 -7.78818550e-02 -2.47028016e-02\n",
            " -4.65210788e-02  8.34330066e-02 -4.48634072e-02 -3.94807831e-02\n",
            " -5.37377447e-02  8.08237551e-02 -3.06198905e-02  3.83747456e-02\n",
            "  1.37858769e-03  1.29848231e-02 -9.46807795e-05 -7.36074585e-03\n",
            " -6.30042917e-03 -1.88709146e-02 -1.48698329e-01  8.13458607e-03\n",
            "  1.65407849e-02  6.31337903e-04  7.44744108e-02 -5.17792828e-02\n",
            " -6.34876255e-02 -1.97336393e-01 -3.91558886e-02 -8.75773225e-02\n",
            "  4.53107728e-02  6.25870224e-02  1.11955490e-01  8.38318546e-02\n",
            "  3.70591637e-02 -9.15689610e-02 -2.28415665e-02  2.44753145e-03\n",
            "  8.34348113e-02 -1.35686095e-01  6.18970238e-02  8.83896312e-02\n",
            " -7.89094672e-02 -6.01713468e-02 -1.00628207e-02 -4.57322174e-02\n",
            " -5.24059403e-03  6.91367180e-04 -1.18645519e-02  1.60358149e-04\n",
            "  5.80649620e-02  1.74292118e-02  3.25197806e-02 -3.11941644e-02\n",
            "  2.47937892e-02 -1.62473360e-02  4.74140784e-02 -5.55715777e-02\n",
            "  3.10377096e-02  4.66428099e-03 -7.39122121e-02 -7.37978138e-02\n",
            "  9.34073590e-02 -9.95848524e-03 -4.43712147e-02  6.21250546e-02\n",
            " -1.82244816e-02 -5.30676720e-02 -3.69113891e-02 -3.60222694e-02\n",
            " -1.73960179e-01  1.48354699e-02 -1.17646739e-01  3.31408357e-03\n",
            " -3.83672366e-03 -1.13886633e-01 -2.55781120e-02  8.91637310e-02\n",
            "  5.45950272e-02  7.53426752e-02 -3.27377093e-02  1.35807086e-01\n",
            "  8.81926422e-03  1.19892056e-01  6.59417143e-02  4.06727265e-02\n",
            " -1.43336666e-01  2.77102308e-02 -5.01893388e-02 -2.03588063e-02\n",
            "  8.63048162e-02 -6.40443281e-02  4.91898878e-03 -6.15497167e-02\n",
            " -1.21216253e-01 -2.01845150e-02  1.60328894e-02  8.52579412e-02\n",
            " -4.26533396e-03  1.12592504e-02  1.42418142e-02  6.94664688e-02\n",
            "  8.45033411e-02 -7.87899395e-02 -4.01380399e-03 -4.14169392e-02\n",
            "  8.05479187e-02 -2.14914644e-02  3.29872041e-03 -1.00716735e-01]\n",
            "0_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njq-3a7d4k4O"
      },
      "source": [
        "## 파라미터 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIHC1c0U-lq7"
      },
      "source": [
        "*   model, training, inference에 필요한 인자들은 uisrnn.parse_argement()를 통해 얻을 수 있음\n",
        "*   colab 환경에서는 argument를 사용할 수 없기 때문에 easydict로 대체\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G66tYcOA4VAi"
      },
      "source": [
        "import easydict\n",
        "model_args = easydict.EasyDict({'observation_dim' : 256,\n",
        "                                'rnn_hidden_size' : 512,\n",
        "                                'rnn_depth' : 1,\n",
        "                                'rnn_dropout' : 0.2 ,\n",
        "                                'transition_bias' : None,\n",
        "                                'crp_alpha' : 1.0,\n",
        "                                'sigma2' : None,\n",
        "                                'verbosity' : 2,\n",
        "                                'enable_cuda' : True})\n",
        "\n",
        "training_args = easydict.EasyDict({'optimizer' : 'adam',\n",
        "                                   'learning_rate' : 1e-3,\n",
        "                                   'train_iteration' : 20000,\n",
        "                                   'batch_size' : 10,\n",
        "                                   'num_permutations' : 10,\n",
        "                                   'sigma_alpha' : 1.0,\n",
        "                                   'sigma_beta' : 1.0,\n",
        "                                   'regularization_weight' : 1e-5,\n",
        "                                   'grad_max_norm' : 5.0,\n",
        "                                   'enforce_cluster_id_uniqueness' : True})\n",
        "\n",
        "inference_args = easydict.EasyDict({'beam_size' : 10,\n",
        "                                    'look_ahead' : 1,\n",
        "                                    'test_iteration' : 2})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIhMOXVcyc5e",
        "outputId": "ea30fdb1-8416-44fb-8a97-1e0c9dfe1cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'crp_alpha': 1.0,\n",
              " 'enable_cuda': True,\n",
              " 'observation_dim': 256,\n",
              " 'rnn_depth': 1,\n",
              " 'rnn_dropout': 0.2,\n",
              " 'rnn_hidden_size': 512,\n",
              " 'sigma2': None,\n",
              " 'transition_bias': None,\n",
              " 'verbosity': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4tag-_q4q6n"
      },
      "source": [
        "## UISRNN 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOBG5e-8AHJM"
      },
      "source": [
        "\n",
        "\n",
        "*   앞서 구성한 argment를 사용해 모델을 구성\n",
        "*   데이터를 입력해 모델을 훈련\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKow4oSh4e2-"
      },
      "source": [
        "model = uisrnn.UISRNN(model_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_sequences, train_cluster_ids, training_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UEKjr84ZK34",
        "outputId": "25b25315-41af-4b26-b519-9c9431866c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: transition_bias cannot be correctly estimated from a concatenated sequence; train_sequences will be treated as a single sequence. This can lead to inaccurate estimation of transition_bias. Please, consider estimating transition_bias before concatenating the sequences and passing it as argument.\n",
            "Iter: 0  \tTraining Loss: -284.3778    \n",
            "    Negative Log Likelihood: 6.0634\tSigma2 Prior: -290.4418\tRegularization: 0.0006\n",
            "Iter: 10  \tTraining Loss: -299.1331    \n",
            "    Negative Log Likelihood: 5.6336\tSigma2 Prior: -304.7673\tRegularization: 0.0006\n",
            "Iter: 20  \tTraining Loss: -312.1155    \n",
            "    Negative Log Likelihood: 6.2205\tSigma2 Prior: -318.3366\tRegularization: 0.0006\n",
            "Iter: 30  \tTraining Loss: -324.3802    \n",
            "    Negative Log Likelihood: 7.1646\tSigma2 Prior: -331.5455\tRegularization: 0.0006\n",
            "Iter: 40  \tTraining Loss: -348.7893    \n",
            "    Negative Log Likelihood: 8.2098\tSigma2 Prior: -356.9997\tRegularization: 0.0006\n",
            "Iter: 50  \tTraining Loss: -370.3113    \n",
            "    Negative Log Likelihood: 10.4452\tSigma2 Prior: -380.7571\tRegularization: 0.0007\n",
            "Iter: 60  \tTraining Loss: -401.0984    \n",
            "    Negative Log Likelihood: 13.6053\tSigma2 Prior: -414.7043\tRegularization: 0.0007\n",
            "Iter: 70  \tTraining Loss: -434.1375    \n",
            "    Negative Log Likelihood: 22.3686\tSigma2 Prior: -456.5068\tRegularization: 0.0007\n",
            "Iter: 80  \tTraining Loss: -428.0693    \n",
            "    Negative Log Likelihood: 62.7528\tSigma2 Prior: -490.8228\tRegularization: 0.0007\n",
            "Iter: 90  \tTraining Loss: -428.0221    \n",
            "    Negative Log Likelihood: 45.0200\tSigma2 Prior: -473.0428\tRegularization: 0.0007\n",
            "Iter: 100  \tTraining Loss: -480.0834    \n",
            "    Negative Log Likelihood: 41.5895\tSigma2 Prior: -521.6736\tRegularization: 0.0007\n",
            "Iter: 110  \tTraining Loss: -478.8602    \n",
            "    Negative Log Likelihood: 57.9871\tSigma2 Prior: -536.8480\tRegularization: 0.0007\n",
            "Iter: 120  \tTraining Loss: -472.1013    \n",
            "    Negative Log Likelihood: 42.0504\tSigma2 Prior: -514.1525\tRegularization: 0.0007\n",
            "Iter: 130  \tTraining Loss: -468.6468    \n",
            "    Negative Log Likelihood: 42.2432\tSigma2 Prior: -510.8908\tRegularization: 0.0008\n",
            "Iter: 140  \tTraining Loss: -464.6963    \n",
            "    Negative Log Likelihood: 51.2123\tSigma2 Prior: -515.9094\tRegularization: 0.0008\n",
            "Iter: 150  \tTraining Loss: -448.7160    \n",
            "    Negative Log Likelihood: 43.2188\tSigma2 Prior: -491.9356\tRegularization: 0.0008\n",
            "Iter: 160  \tTraining Loss: -471.4143    \n",
            "    Negative Log Likelihood: 39.8104\tSigma2 Prior: -511.2255\tRegularization: 0.0008\n",
            "Iter: 170  \tTraining Loss: -494.4005    \n",
            "    Negative Log Likelihood: 45.4983\tSigma2 Prior: -539.8997\tRegularization: 0.0008\n",
            "Iter: 180  \tTraining Loss: -454.5550    \n",
            "    Negative Log Likelihood: 53.8607\tSigma2 Prior: -508.4166\tRegularization: 0.0008\n",
            "Iter: 190  \tTraining Loss: -482.8770    \n",
            "    Negative Log Likelihood: 44.1722\tSigma2 Prior: -527.0500\tRegularization: 0.0009\n",
            "Iter: 200  \tTraining Loss: -491.9660    \n",
            "    Negative Log Likelihood: 44.1717\tSigma2 Prior: -536.1385\tRegularization: 0.0009\n",
            "Iter: 210  \tTraining Loss: -474.5721    \n",
            "    Negative Log Likelihood: 42.0490\tSigma2 Prior: -516.6219\tRegularization: 0.0009\n",
            "Iter: 220  \tTraining Loss: -459.7119    \n",
            "    Negative Log Likelihood: 42.7989\tSigma2 Prior: -502.5117\tRegularization: 0.0009\n",
            "Iter: 230  \tTraining Loss: -493.9992    \n",
            "    Negative Log Likelihood: 34.8414\tSigma2 Prior: -528.8416\tRegularization: 0.0009\n",
            "Iter: 240  \tTraining Loss: -499.3581    \n",
            "    Negative Log Likelihood: 44.0927\tSigma2 Prior: -543.4517\tRegularization: 0.0009\n",
            "Iter: 250  \tTraining Loss: -469.0239    \n",
            "    Negative Log Likelihood: 44.0382\tSigma2 Prior: -513.0630\tRegularization: 0.0010\n",
            "Iter: 260  \tTraining Loss: -492.2026    \n",
            "    Negative Log Likelihood: 41.1446\tSigma2 Prior: -533.3482\tRegularization: 0.0010\n",
            "Iter: 270  \tTraining Loss: -496.8707    \n",
            "    Negative Log Likelihood: 36.8861\tSigma2 Prior: -533.7578\tRegularization: 0.0010\n",
            "Iter: 280  \tTraining Loss: -509.9235    \n",
            "    Negative Log Likelihood: 42.7086\tSigma2 Prior: -552.6331\tRegularization: 0.0010\n",
            "Iter: 290  \tTraining Loss: -429.3977    \n",
            "    Negative Log Likelihood: 39.5807\tSigma2 Prior: -468.9794\tRegularization: 0.0010\n",
            "Iter: 300  \tTraining Loss: -476.3674    \n",
            "    Negative Log Likelihood: 30.8658\tSigma2 Prior: -507.2342\tRegularization: 0.0010\n",
            "Iter: 310  \tTraining Loss: -474.2496    \n",
            "    Negative Log Likelihood: 47.2059\tSigma2 Prior: -521.4565\tRegularization: 0.0010\n",
            "Iter: 320  \tTraining Loss: -472.3662    \n",
            "    Negative Log Likelihood: 34.3525\tSigma2 Prior: -506.7198\tRegularization: 0.0011\n",
            "Iter: 330  \tTraining Loss: -479.2755    \n",
            "    Negative Log Likelihood: 34.1293\tSigma2 Prior: -513.4059\tRegularization: 0.0011\n",
            "Iter: 340  \tTraining Loss: -462.4677    \n",
            "    Negative Log Likelihood: 50.4173\tSigma2 Prior: -512.8860\tRegularization: 0.0011\n",
            "Iter: 350  \tTraining Loss: -468.4008    \n",
            "    Negative Log Likelihood: 33.9866\tSigma2 Prior: -502.3884\tRegularization: 0.0011\n",
            "Iter: 360  \tTraining Loss: -490.7991    \n",
            "    Negative Log Likelihood: 37.6500\tSigma2 Prior: -528.4502\tRegularization: 0.0011\n",
            "Iter: 370  \tTraining Loss: -490.4577    \n",
            "    Negative Log Likelihood: 39.5889\tSigma2 Prior: -530.0477\tRegularization: 0.0011\n",
            "Iter: 380  \tTraining Loss: -472.2157    \n",
            "    Negative Log Likelihood: 40.8952\tSigma2 Prior: -513.1119\tRegularization: 0.0011\n",
            "Iter: 390  \tTraining Loss: -483.8009    \n",
            "    Negative Log Likelihood: 38.0581\tSigma2 Prior: -521.8602\tRegularization: 0.0011\n",
            "Iter: 400  \tTraining Loss: -498.8920    \n",
            "    Negative Log Likelihood: 39.2654\tSigma2 Prior: -538.1585\tRegularization: 0.0011\n",
            "Iter: 410  \tTraining Loss: -476.5848    \n",
            "    Negative Log Likelihood: 40.0172\tSigma2 Prior: -516.6031\tRegularization: 0.0012\n",
            "Iter: 420  \tTraining Loss: -463.8106    \n",
            "    Negative Log Likelihood: 42.8363\tSigma2 Prior: -506.6480\tRegularization: 0.0012\n",
            "Iter: 430  \tTraining Loss: -477.1013    \n",
            "    Negative Log Likelihood: 32.7201\tSigma2 Prior: -509.8226\tRegularization: 0.0012\n",
            "Iter: 440  \tTraining Loss: -482.4568    \n",
            "    Negative Log Likelihood: 44.2215\tSigma2 Prior: -526.6795\tRegularization: 0.0012\n",
            "Iter: 450  \tTraining Loss: -492.7372    \n",
            "    Negative Log Likelihood: 38.4767\tSigma2 Prior: -531.2151\tRegularization: 0.0012\n",
            "Iter: 460  \tTraining Loss: -475.2499    \n",
            "    Negative Log Likelihood: 39.1687\tSigma2 Prior: -514.4198\tRegularization: 0.0012\n",
            "Iter: 470  \tTraining Loss: -469.3134    \n",
            "    Negative Log Likelihood: 32.5240\tSigma2 Prior: -501.8385\tRegularization: 0.0012\n",
            "Iter: 480  \tTraining Loss: -493.7121    \n",
            "    Negative Log Likelihood: 42.0954\tSigma2 Prior: -535.8087\tRegularization: 0.0012\n",
            "Iter: 490  \tTraining Loss: -476.6653    \n",
            "    Negative Log Likelihood: 32.6529\tSigma2 Prior: -509.3194\tRegularization: 0.0012\n",
            "Iter: 500  \tTraining Loss: -494.6378    \n",
            "    Negative Log Likelihood: 36.6877\tSigma2 Prior: -531.3267\tRegularization: 0.0012\n",
            "Iter: 510  \tTraining Loss: -463.8429    \n",
            "    Negative Log Likelihood: 46.7075\tSigma2 Prior: -510.5516\tRegularization: 0.0012\n",
            "Iter: 520  \tTraining Loss: -487.0047    \n",
            "    Negative Log Likelihood: 35.0280\tSigma2 Prior: -522.0339\tRegularization: 0.0012\n",
            "Iter: 530  \tTraining Loss: -484.2981    \n",
            "    Negative Log Likelihood: 39.1020\tSigma2 Prior: -523.4014\tRegularization: 0.0012\n",
            "Iter: 540  \tTraining Loss: -495.1955    \n",
            "    Negative Log Likelihood: 45.1938\tSigma2 Prior: -540.3906\tRegularization: 0.0012\n",
            "Iter: 550  \tTraining Loss: -460.4832    \n",
            "    Negative Log Likelihood: 39.6210\tSigma2 Prior: -500.1054\tRegularization: 0.0013\n",
            "Iter: 560  \tTraining Loss: -475.8131    \n",
            "    Negative Log Likelihood: 36.6432\tSigma2 Prior: -512.4575\tRegularization: 0.0013\n",
            "Iter: 570  \tTraining Loss: -495.7007    \n",
            "    Negative Log Likelihood: 37.4630\tSigma2 Prior: -533.1650\tRegularization: 0.0013\n",
            "Iter: 580  \tTraining Loss: -478.5262    \n",
            "    Negative Log Likelihood: 42.5657\tSigma2 Prior: -521.0932\tRegularization: 0.0013\n",
            "Iter: 590  \tTraining Loss: -477.9830    \n",
            "    Negative Log Likelihood: 36.7170\tSigma2 Prior: -514.7013\tRegularization: 0.0013\n",
            "Iter: 600  \tTraining Loss: -486.0420    \n",
            "    Negative Log Likelihood: 42.9570\tSigma2 Prior: -529.0002\tRegularization: 0.0013\n",
            "Iter: 610  \tTraining Loss: -505.8036    \n",
            "    Negative Log Likelihood: 31.8043\tSigma2 Prior: -537.6091\tRegularization: 0.0013\n",
            "Iter: 620  \tTraining Loss: -482.7611    \n",
            "    Negative Log Likelihood: 42.7060\tSigma2 Prior: -525.4684\tRegularization: 0.0013\n",
            "Iter: 630  \tTraining Loss: -507.6947    \n",
            "    Negative Log Likelihood: 37.6177\tSigma2 Prior: -545.3137\tRegularization: 0.0013\n",
            "Iter: 640  \tTraining Loss: -491.4355    \n",
            "    Negative Log Likelihood: 36.2194\tSigma2 Prior: -527.6561\tRegularization: 0.0013\n",
            "Iter: 650  \tTraining Loss: -482.0020    \n",
            "    Negative Log Likelihood: 38.7210\tSigma2 Prior: -520.7244\tRegularization: 0.0013\n",
            "Iter: 660  \tTraining Loss: -471.2048    \n",
            "    Negative Log Likelihood: 42.2334\tSigma2 Prior: -513.4396\tRegularization: 0.0013\n",
            "Iter: 670  \tTraining Loss: -438.1900    \n",
            "    Negative Log Likelihood: 37.8293\tSigma2 Prior: -476.0207\tRegularization: 0.0013\n",
            "Iter: 680  \tTraining Loss: -499.9783    \n",
            "    Negative Log Likelihood: 32.3692\tSigma2 Prior: -532.3489\tRegularization: 0.0013\n",
            "Iter: 690  \tTraining Loss: -419.1176    \n",
            "    Negative Log Likelihood: 43.2041\tSigma2 Prior: -462.3230\tRegularization: 0.0013\n",
            "Iter: 700  \tTraining Loss: -480.3037    \n",
            "    Negative Log Likelihood: 32.0484\tSigma2 Prior: -512.3535\tRegularization: 0.0013\n",
            "Iter: 710  \tTraining Loss: -496.9573    \n",
            "    Negative Log Likelihood: 36.4863\tSigma2 Prior: -533.4449\tRegularization: 0.0013\n",
            "Iter: 720  \tTraining Loss: -485.2400    \n",
            "    Negative Log Likelihood: 31.3973\tSigma2 Prior: -516.6387\tRegularization: 0.0013\n",
            "Iter: 730  \tTraining Loss: -467.4177    \n",
            "    Negative Log Likelihood: 39.1710\tSigma2 Prior: -506.5900\tRegularization: 0.0014\n",
            "Iter: 740  \tTraining Loss: -503.3280    \n",
            "    Negative Log Likelihood: 37.7516\tSigma2 Prior: -541.0809\tRegularization: 0.0014\n",
            "Iter: 750  \tTraining Loss: -486.8367    \n",
            "    Negative Log Likelihood: 38.0267\tSigma2 Prior: -524.8647\tRegularization: 0.0014\n",
            "Iter: 760  \tTraining Loss: -497.4284    \n",
            "    Negative Log Likelihood: 33.3404\tSigma2 Prior: -530.7701\tRegularization: 0.0014\n",
            "Iter: 770  \tTraining Loss: -508.2155    \n",
            "    Negative Log Likelihood: 40.0034\tSigma2 Prior: -548.2203\tRegularization: 0.0014\n",
            "Iter: 780  \tTraining Loss: -491.4743    \n",
            "    Negative Log Likelihood: 36.3874\tSigma2 Prior: -527.8632\tRegularization: 0.0014\n",
            "Iter: 790  \tTraining Loss: -488.3558    \n",
            "    Negative Log Likelihood: 38.6723\tSigma2 Prior: -527.0294\tRegularization: 0.0014\n",
            "Iter: 800  \tTraining Loss: -478.2259    \n",
            "    Negative Log Likelihood: 37.7027\tSigma2 Prior: -515.9299\tRegularization: 0.0014\n",
            "Iter: 810  \tTraining Loss: -512.0151    \n",
            "    Negative Log Likelihood: 33.9441\tSigma2 Prior: -545.9606\tRegularization: 0.0014\n",
            "Iter: 820  \tTraining Loss: -521.0038    \n",
            "    Negative Log Likelihood: 41.1795\tSigma2 Prior: -562.1847\tRegularization: 0.0014\n",
            "Iter: 830  \tTraining Loss: -505.0423    \n",
            "    Negative Log Likelihood: 40.3260\tSigma2 Prior: -545.3697\tRegularization: 0.0014\n",
            "Iter: 840  \tTraining Loss: -465.1144    \n",
            "    Negative Log Likelihood: 38.0297\tSigma2 Prior: -503.1454\tRegularization: 0.0014\n",
            "Iter: 850  \tTraining Loss: -504.2291    \n",
            "    Negative Log Likelihood: 34.4472\tSigma2 Prior: -538.6777\tRegularization: 0.0014\n",
            "Iter: 860  \tTraining Loss: -452.7341    \n",
            "    Negative Log Likelihood: 41.1261\tSigma2 Prior: -493.8616\tRegularization: 0.0014\n",
            "Iter: 870  \tTraining Loss: -483.3169    \n",
            "    Negative Log Likelihood: 30.9922\tSigma2 Prior: -514.3105\tRegularization: 0.0014\n",
            "Iter: 880  \tTraining Loss: -524.5044    \n",
            "    Negative Log Likelihood: 36.7847\tSigma2 Prior: -561.2905\tRegularization: 0.0014\n",
            "Iter: 890  \tTraining Loss: -509.3192    \n",
            "    Negative Log Likelihood: 36.3456\tSigma2 Prior: -545.6663\tRegularization: 0.0014\n",
            "Iter: 900  \tTraining Loss: -484.9670    \n",
            "    Negative Log Likelihood: 32.9914\tSigma2 Prior: -517.9598\tRegularization: 0.0014\n",
            "Iter: 910  \tTraining Loss: -458.1774    \n",
            "    Negative Log Likelihood: 40.7306\tSigma2 Prior: -498.9094\tRegularization: 0.0014\n",
            "Iter: 920  \tTraining Loss: -466.9592    \n",
            "    Negative Log Likelihood: 35.2189\tSigma2 Prior: -502.1796\tRegularization: 0.0014\n",
            "Iter: 930  \tTraining Loss: -472.9114    \n",
            "    Negative Log Likelihood: 32.5801\tSigma2 Prior: -505.4929\tRegularization: 0.0014\n",
            "Iter: 940  \tTraining Loss: -459.9509    \n",
            "    Negative Log Likelihood: 35.4667\tSigma2 Prior: -495.4191\tRegularization: 0.0015\n",
            "Iter: 950  \tTraining Loss: -516.5342    \n",
            "    Negative Log Likelihood: 36.9879\tSigma2 Prior: -553.5236\tRegularization: 0.0015\n",
            "Iter: 960  \tTraining Loss: -462.2900    \n",
            "    Negative Log Likelihood: 33.0018\tSigma2 Prior: -495.2932\tRegularization: 0.0015\n",
            "Iter: 970  \tTraining Loss: -490.5009    \n",
            "    Negative Log Likelihood: 33.8727\tSigma2 Prior: -524.3751\tRegularization: 0.0015\n",
            "Iter: 980  \tTraining Loss: -483.0400    \n",
            "    Negative Log Likelihood: 49.5080\tSigma2 Prior: -532.5494\tRegularization: 0.0015\n",
            "Iter: 990  \tTraining Loss: -469.8599    \n",
            "    Negative Log Likelihood: 32.3564\tSigma2 Prior: -502.2177\tRegularization: 0.0015\n",
            "Iter: 1000  \tTraining Loss: -491.4062    \n",
            "    Negative Log Likelihood: 40.3660\tSigma2 Prior: -531.7737\tRegularization: 0.0015\n",
            "Iter: 1010  \tTraining Loss: -471.7902    \n",
            "    Negative Log Likelihood: 36.6911\tSigma2 Prior: -508.4827\tRegularization: 0.0015\n",
            "Iter: 1020  \tTraining Loss: -474.1864    \n",
            "    Negative Log Likelihood: 33.8816\tSigma2 Prior: -508.0695\tRegularization: 0.0015\n",
            "Iter: 1030  \tTraining Loss: -486.6658    \n",
            "    Negative Log Likelihood: 38.9401\tSigma2 Prior: -525.6074\tRegularization: 0.0015\n",
            "Iter: 1040  \tTraining Loss: -506.7085    \n",
            "    Negative Log Likelihood: 31.6813\tSigma2 Prior: -538.3913\tRegularization: 0.0015\n",
            "Iter: 1050  \tTraining Loss: -518.2580    \n",
            "    Negative Log Likelihood: 43.0799\tSigma2 Prior: -561.3394\tRegularization: 0.0015\n",
            "Iter: 1060  \tTraining Loss: -496.7222    \n",
            "    Negative Log Likelihood: 28.4264\tSigma2 Prior: -525.1501\tRegularization: 0.0015\n",
            "Iter: 1070  \tTraining Loss: -489.6855    \n",
            "    Negative Log Likelihood: 36.5311\tSigma2 Prior: -526.2181\tRegularization: 0.0015\n",
            "Iter: 1080  \tTraining Loss: -442.8451    \n",
            "    Negative Log Likelihood: 37.0215\tSigma2 Prior: -479.8681\tRegularization: 0.0015\n",
            "Iter: 1090  \tTraining Loss: -520.0391    \n",
            "    Negative Log Likelihood: 28.8859\tSigma2 Prior: -548.9266\tRegularization: 0.0015\n",
            "Iter: 1100  \tTraining Loss: -490.8531    \n",
            "    Negative Log Likelihood: 36.1640\tSigma2 Prior: -527.0187\tRegularization: 0.0015\n",
            "Iter: 1110  \tTraining Loss: -467.6614    \n",
            "    Negative Log Likelihood: 32.7027\tSigma2 Prior: -500.3656\tRegularization: 0.0015\n",
            "Iter: 1120  \tTraining Loss: -496.6110    \n",
            "    Negative Log Likelihood: 38.5741\tSigma2 Prior: -535.1866\tRegularization: 0.0015\n",
            "Iter: 1130  \tTraining Loss: -487.9289    \n",
            "    Negative Log Likelihood: 31.7596\tSigma2 Prior: -519.6900\tRegularization: 0.0015\n",
            "Iter: 1140  \tTraining Loss: -475.4128    \n",
            "    Negative Log Likelihood: 38.9520\tSigma2 Prior: -514.3663\tRegularization: 0.0015\n",
            "Iter: 1150  \tTraining Loss: -515.4608    \n",
            "    Negative Log Likelihood: 39.9129\tSigma2 Prior: -555.3753\tRegularization: 0.0015\n",
            "Iter: 1160  \tTraining Loss: -484.5645    \n",
            "    Negative Log Likelihood: 29.9718\tSigma2 Prior: -514.5378\tRegularization: 0.0015\n",
            "Iter: 1170  \tTraining Loss: -498.7630    \n",
            "    Negative Log Likelihood: 35.2560\tSigma2 Prior: -534.0204\tRegularization: 0.0015\n",
            "Iter: 1180  \tTraining Loss: -499.3956    \n",
            "    Negative Log Likelihood: 35.8506\tSigma2 Prior: -535.2477\tRegularization: 0.0015\n",
            "Iter: 1190  \tTraining Loss: -447.0865    \n",
            "    Negative Log Likelihood: 32.2000\tSigma2 Prior: -479.2880\tRegularization: 0.0015\n",
            "Iter: 1200  \tTraining Loss: -499.3615    \n",
            "    Negative Log Likelihood: 39.2903\tSigma2 Prior: -538.6534\tRegularization: 0.0016\n",
            "Iter: 1210  \tTraining Loss: -450.9835    \n",
            "    Negative Log Likelihood: 36.7705\tSigma2 Prior: -487.7555\tRegularization: 0.0016\n",
            "Iter: 1220  \tTraining Loss: -496.5909    \n",
            "    Negative Log Likelihood: 33.4319\tSigma2 Prior: -530.0243\tRegularization: 0.0016\n",
            "Iter: 1230  \tTraining Loss: -514.6248    \n",
            "    Negative Log Likelihood: 38.7221\tSigma2 Prior: -553.3485\tRegularization: 0.0016\n",
            "Iter: 1240  \tTraining Loss: -472.8225    \n",
            "    Negative Log Likelihood: 38.8177\tSigma2 Prior: -511.6418\tRegularization: 0.0016\n",
            "Iter: 1250  \tTraining Loss: -485.7204    \n",
            "    Negative Log Likelihood: 31.7999\tSigma2 Prior: -517.5219\tRegularization: 0.0016\n",
            "Iter: 1260  \tTraining Loss: -420.4333    \n",
            "    Negative Log Likelihood: 38.3068\tSigma2 Prior: -458.7418\tRegularization: 0.0016\n",
            "Iter: 1270  \tTraining Loss: -479.9967    \n",
            "    Negative Log Likelihood: 32.3003\tSigma2 Prior: -512.2986\tRegularization: 0.0016\n",
            "Iter: 1280  \tTraining Loss: -459.8606    \n",
            "    Negative Log Likelihood: 44.0031\tSigma2 Prior: -503.8652\tRegularization: 0.0016\n",
            "Iter: 1290  \tTraining Loss: -442.2909    \n",
            "    Negative Log Likelihood: 33.8749\tSigma2 Prior: -476.1674\tRegularization: 0.0016\n",
            "Iter: 1300  \tTraining Loss: -502.4891    \n",
            "    Negative Log Likelihood: 32.0910\tSigma2 Prior: -534.5817\tRegularization: 0.0016\n",
            "Iter: 1310  \tTraining Loss: -490.8847    \n",
            "    Negative Log Likelihood: 47.1318\tSigma2 Prior: -538.0181\tRegularization: 0.0016\n",
            "Iter: 1320  \tTraining Loss: -478.5757    \n",
            "    Negative Log Likelihood: 30.8823\tSigma2 Prior: -509.4597\tRegularization: 0.0016\n",
            "Iter: 1330  \tTraining Loss: -500.8705    \n",
            "    Negative Log Likelihood: 33.2782\tSigma2 Prior: -534.1503\tRegularization: 0.0016\n",
            "Iter: 1340  \tTraining Loss: -468.4869    \n",
            "    Negative Log Likelihood: 44.9973\tSigma2 Prior: -513.4857\tRegularization: 0.0016\n",
            "Iter: 1350  \tTraining Loss: -458.4978    \n",
            "    Negative Log Likelihood: 31.6712\tSigma2 Prior: -490.1706\tRegularization: 0.0016\n",
            "Iter: 1360  \tTraining Loss: -480.4078    \n",
            "    Negative Log Likelihood: 36.7006\tSigma2 Prior: -517.1100\tRegularization: 0.0016\n",
            "Iter: 1370  \tTraining Loss: -512.2128    \n",
            "    Negative Log Likelihood: 33.5187\tSigma2 Prior: -545.7332\tRegularization: 0.0016\n",
            "Iter: 1380  \tTraining Loss: -494.4802    \n",
            "    Negative Log Likelihood: 31.5205\tSigma2 Prior: -526.0023\tRegularization: 0.0016\n",
            "Iter: 1390  \tTraining Loss: -500.7463    \n",
            "    Negative Log Likelihood: 33.7817\tSigma2 Prior: -534.5297\tRegularization: 0.0016\n",
            "Iter: 1400  \tTraining Loss: -503.7562    \n",
            "    Negative Log Likelihood: 38.7060\tSigma2 Prior: -542.4638\tRegularization: 0.0016\n",
            "Iter: 1410  \tTraining Loss: -444.1459    \n",
            "    Negative Log Likelihood: 44.1920\tSigma2 Prior: -488.3395\tRegularization: 0.0016\n",
            "Iter: 1420  \tTraining Loss: -482.1903    \n",
            "    Negative Log Likelihood: 34.6941\tSigma2 Prior: -516.8860\tRegularization: 0.0016\n",
            "Iter: 1430  \tTraining Loss: -477.8092    \n",
            "    Negative Log Likelihood: 30.3425\tSigma2 Prior: -508.1533\tRegularization: 0.0016\n",
            "Iter: 1440  \tTraining Loss: -526.2067    \n",
            "    Negative Log Likelihood: 32.5703\tSigma2 Prior: -558.7786\tRegularization: 0.0016\n",
            "Iter: 1450  \tTraining Loss: -492.6617    \n",
            "    Negative Log Likelihood: 36.7331\tSigma2 Prior: -529.3965\tRegularization: 0.0016\n",
            "Iter: 1460  \tTraining Loss: -511.3364    \n",
            "    Negative Log Likelihood: 32.0536\tSigma2 Prior: -543.3916\tRegularization: 0.0016\n",
            "Iter: 1470  \tTraining Loss: -515.8943    \n",
            "    Negative Log Likelihood: 34.5063\tSigma2 Prior: -550.4023\tRegularization: 0.0016\n",
            "Iter: 1480  \tTraining Loss: -453.5590    \n",
            "    Negative Log Likelihood: 37.3885\tSigma2 Prior: -490.9491\tRegularization: 0.0016\n",
            "Iter: 1490  \tTraining Loss: -469.3431    \n",
            "    Negative Log Likelihood: 30.9273\tSigma2 Prior: -500.2720\tRegularization: 0.0017\n",
            "Iter: 1500  \tTraining Loss: -491.2982    \n",
            "    Negative Log Likelihood: 36.6203\tSigma2 Prior: -527.9202\tRegularization: 0.0017\n",
            "Iter: 1510  \tTraining Loss: -504.4913    \n",
            "    Negative Log Likelihood: 36.1696\tSigma2 Prior: -540.6625\tRegularization: 0.0017\n",
            "Iter: 1520  \tTraining Loss: -525.2142    \n",
            "    Negative Log Likelihood: 35.2926\tSigma2 Prior: -560.5084\tRegularization: 0.0017\n",
            "Iter: 1530  \tTraining Loss: -471.4282    \n",
            "    Negative Log Likelihood: 38.8830\tSigma2 Prior: -510.3129\tRegularization: 0.0017\n",
            "Iter: 1540  \tTraining Loss: -493.0395    \n",
            "    Negative Log Likelihood: 32.3467\tSigma2 Prior: -525.3879\tRegularization: 0.0017\n",
            "Iter: 1550  \tTraining Loss: -515.8011    \n",
            "    Negative Log Likelihood: 39.5309\tSigma2 Prior: -555.3337\tRegularization: 0.0017\n",
            "Iter: 1560  \tTraining Loss: -495.6908    \n",
            "    Negative Log Likelihood: 34.1480\tSigma2 Prior: -529.8405\tRegularization: 0.0017\n",
            "Iter: 1570  \tTraining Loss: -504.1191    \n",
            "    Negative Log Likelihood: 33.3025\tSigma2 Prior: -537.4233\tRegularization: 0.0017\n",
            "Iter: 1580  \tTraining Loss: -468.3115    \n",
            "    Negative Log Likelihood: 37.0884\tSigma2 Prior: -505.4016\tRegularization: 0.0017\n",
            "Iter: 1590  \tTraining Loss: -495.1073    \n",
            "    Negative Log Likelihood: 33.8985\tSigma2 Prior: -529.0076\tRegularization: 0.0017\n",
            "Iter: 1600  \tTraining Loss: -482.1151    \n",
            "    Negative Log Likelihood: 36.6277\tSigma2 Prior: -518.7444\tRegularization: 0.0017\n",
            "Iter: 1610  \tTraining Loss: -491.9936    \n",
            "    Negative Log Likelihood: 34.7921\tSigma2 Prior: -526.7874\tRegularization: 0.0017\n",
            "Iter: 1620  \tTraining Loss: -471.1532    \n",
            "    Negative Log Likelihood: 38.7333\tSigma2 Prior: -509.8882\tRegularization: 0.0017\n",
            "Iter: 1630  \tTraining Loss: -503.6781    \n",
            "    Negative Log Likelihood: 36.1401\tSigma2 Prior: -539.8198\tRegularization: 0.0017\n",
            "Iter: 1640  \tTraining Loss: -483.3860    \n",
            "    Negative Log Likelihood: 32.7766\tSigma2 Prior: -516.1642\tRegularization: 0.0017\n",
            "Iter: 1650  \tTraining Loss: -480.3168    \n",
            "    Negative Log Likelihood: 41.0101\tSigma2 Prior: -521.3287\tRegularization: 0.0017\n",
            "Iter: 1660  \tTraining Loss: -441.3482    \n",
            "    Negative Log Likelihood: 33.0073\tSigma2 Prior: -474.3572\tRegularization: 0.0017\n",
            "Iter: 1670  \tTraining Loss: -489.8221    \n",
            "    Negative Log Likelihood: 34.6508\tSigma2 Prior: -524.4745\tRegularization: 0.0017\n",
            "Iter: 1680  \tTraining Loss: -506.2812    \n",
            "    Negative Log Likelihood: 40.6226\tSigma2 Prior: -546.9056\tRegularization: 0.0017\n",
            "Iter: 1690  \tTraining Loss: -492.8030    \n",
            "    Negative Log Likelihood: 33.9995\tSigma2 Prior: -526.8043\tRegularization: 0.0017\n",
            "Iter: 1700  \tTraining Loss: -500.4050    \n",
            "    Negative Log Likelihood: 38.6505\tSigma2 Prior: -539.0572\tRegularization: 0.0017\n",
            "Iter: 1710  \tTraining Loss: -490.1760    \n",
            "    Negative Log Likelihood: 28.7673\tSigma2 Prior: -518.9450\tRegularization: 0.0017\n",
            "Iter: 1720  \tTraining Loss: -481.2245    \n",
            "    Negative Log Likelihood: 33.5095\tSigma2 Prior: -514.7358\tRegularization: 0.0017\n",
            "Iter: 1730  \tTraining Loss: -476.0804    \n",
            "    Negative Log Likelihood: 41.8981\tSigma2 Prior: -517.9802\tRegularization: 0.0017\n",
            "Iter: 1740  \tTraining Loss: -479.6375    \n",
            "    Negative Log Likelihood: 35.5122\tSigma2 Prior: -515.1514\tRegularization: 0.0017\n",
            "Iter: 1750  \tTraining Loss: -479.4420    \n",
            "    Negative Log Likelihood: 28.6748\tSigma2 Prior: -508.1186\tRegularization: 0.0017\n",
            "Iter: 1760  \tTraining Loss: -479.0681    \n",
            "    Negative Log Likelihood: 37.5354\tSigma2 Prior: -516.6052\tRegularization: 0.0017\n",
            "Iter: 1770  \tTraining Loss: -511.5113    \n",
            "    Negative Log Likelihood: 35.5652\tSigma2 Prior: -547.0782\tRegularization: 0.0017\n",
            "Iter: 1780  \tTraining Loss: -492.9731    \n",
            "    Negative Log Likelihood: 37.4000\tSigma2 Prior: -530.3748\tRegularization: 0.0017\n",
            "Iter: 1790  \tTraining Loss: -496.8253    \n",
            "    Negative Log Likelihood: 30.3763\tSigma2 Prior: -527.2033\tRegularization: 0.0017\n",
            "Iter: 1800  \tTraining Loss: -470.2736    \n",
            "    Negative Log Likelihood: 33.8282\tSigma2 Prior: -504.1035\tRegularization: 0.0017\n",
            "Iter: 1810  \tTraining Loss: -468.4781    \n",
            "    Negative Log Likelihood: 37.0557\tSigma2 Prior: -505.5355\tRegularization: 0.0018\n",
            "Iter: 1820  \tTraining Loss: -492.8804    \n",
            "    Negative Log Likelihood: 29.1687\tSigma2 Prior: -522.0508\tRegularization: 0.0018\n",
            "Iter: 1830  \tTraining Loss: -511.0824    \n",
            "    Negative Log Likelihood: 38.4979\tSigma2 Prior: -549.5821\tRegularization: 0.0018\n",
            "Iter: 1840  \tTraining Loss: -505.8438    \n",
            "    Negative Log Likelihood: 31.8957\tSigma2 Prior: -537.7413\tRegularization: 0.0018\n",
            "Iter: 1850  \tTraining Loss: -503.8107    \n",
            "    Negative Log Likelihood: 36.9423\tSigma2 Prior: -540.7548\tRegularization: 0.0018\n",
            "Iter: 1860  \tTraining Loss: -505.0842    \n",
            "    Negative Log Likelihood: 35.0560\tSigma2 Prior: -540.1420\tRegularization: 0.0018\n",
            "Iter: 1870  \tTraining Loss: -486.7854    \n",
            "    Negative Log Likelihood: 35.8352\tSigma2 Prior: -522.6224\tRegularization: 0.0018\n",
            "Iter: 1880  \tTraining Loss: -500.6227    \n",
            "    Negative Log Likelihood: 32.5085\tSigma2 Prior: -533.1330\tRegularization: 0.0018\n",
            "Iter: 1890  \tTraining Loss: -499.2120    \n",
            "    Negative Log Likelihood: 36.9106\tSigma2 Prior: -536.1244\tRegularization: 0.0018\n",
            "Iter: 1900  \tTraining Loss: -518.2995    \n",
            "    Negative Log Likelihood: 33.6347\tSigma2 Prior: -551.9359\tRegularization: 0.0018\n",
            "Iter: 1910  \tTraining Loss: -514.6843    \n",
            "    Negative Log Likelihood: 34.7340\tSigma2 Prior: -549.4200\tRegularization: 0.0018\n",
            "Iter: 1920  \tTraining Loss: -503.1581    \n",
            "    Negative Log Likelihood: 32.3701\tSigma2 Prior: -535.5300\tRegularization: 0.0018\n",
            "Iter: 1930  \tTraining Loss: -473.9516    \n",
            "    Negative Log Likelihood: 37.9382\tSigma2 Prior: -511.8916\tRegularization: 0.0018\n",
            "Iter: 1940  \tTraining Loss: -441.4518    \n",
            "    Negative Log Likelihood: 33.8423\tSigma2 Prior: -475.2958\tRegularization: 0.0018\n",
            "Iter: 1950  \tTraining Loss: -473.1708    \n",
            "    Negative Log Likelihood: 38.7415\tSigma2 Prior: -511.9142\tRegularization: 0.0018\n",
            "Iter: 1960  \tTraining Loss: -504.2728    \n",
            "    Negative Log Likelihood: 31.9199\tSigma2 Prior: -536.1945\tRegularization: 0.0018\n",
            "Iter: 1970  \tTraining Loss: -500.5013    \n",
            "    Negative Log Likelihood: 31.7780\tSigma2 Prior: -532.2811\tRegularization: 0.0018\n",
            "Iter: 1980  \tTraining Loss: -474.1803    \n",
            "    Negative Log Likelihood: 39.7977\tSigma2 Prior: -513.9798\tRegularization: 0.0018\n",
            "Iter: 1990  \tTraining Loss: -493.2805    \n",
            "    Negative Log Likelihood: 32.1285\tSigma2 Prior: -525.4108\tRegularization: 0.0018\n",
            "Iter: 2000  \tTraining Loss: -490.7523    \n",
            "    Negative Log Likelihood: 37.1427\tSigma2 Prior: -527.8969\tRegularization: 0.0018\n",
            "Iter: 2010  \tTraining Loss: -486.0000    \n",
            "    Negative Log Likelihood: 34.2057\tSigma2 Prior: -520.2075\tRegularization: 0.0018\n",
            "Iter: 2020  \tTraining Loss: -493.3486    \n",
            "    Negative Log Likelihood: 37.8374\tSigma2 Prior: -531.1878\tRegularization: 0.0018\n",
            "Iter: 2030  \tTraining Loss: -495.0213    \n",
            "    Negative Log Likelihood: 32.6598\tSigma2 Prior: -527.6829\tRegularization: 0.0018\n",
            "Iter: 2040  \tTraining Loss: -486.2406    \n",
            "    Negative Log Likelihood: 31.9155\tSigma2 Prior: -518.1579\tRegularization: 0.0018\n",
            "Iter: 2050  \tTraining Loss: -503.8167    \n",
            "    Negative Log Likelihood: 34.7953\tSigma2 Prior: -538.6138\tRegularization: 0.0018\n",
            "Iter: 2060  \tTraining Loss: -491.2563    \n",
            "    Negative Log Likelihood: 35.9038\tSigma2 Prior: -527.1619\tRegularization: 0.0018\n",
            "Iter: 2070  \tTraining Loss: -496.3077    \n",
            "    Negative Log Likelihood: 34.1814\tSigma2 Prior: -530.4910\tRegularization: 0.0018\n",
            "Iter: 2080  \tTraining Loss: -498.2558    \n",
            "    Negative Log Likelihood: 35.0565\tSigma2 Prior: -533.3141\tRegularization: 0.0018\n",
            "Iter: 2090  \tTraining Loss: -458.2790    \n",
            "    Negative Log Likelihood: 31.0207\tSigma2 Prior: -489.3015\tRegularization: 0.0018\n",
            "Iter: 2100  \tTraining Loss: -477.5658    \n",
            "    Negative Log Likelihood: 37.2525\tSigma2 Prior: -514.8201\tRegularization: 0.0018\n",
            "Iter: 2110  \tTraining Loss: -514.6036    \n",
            "    Negative Log Likelihood: 38.7446\tSigma2 Prior: -553.3500\tRegularization: 0.0018\n",
            "Iter: 2120  \tTraining Loss: -496.1830    \n",
            "    Negative Log Likelihood: 37.7353\tSigma2 Prior: -533.9202\tRegularization: 0.0018\n",
            "Iter: 2130  \tTraining Loss: -513.1322    \n",
            "    Negative Log Likelihood: 33.0806\tSigma2 Prior: -546.2147\tRegularization: 0.0018\n",
            "Iter: 2140  \tTraining Loss: -494.1533    \n",
            "    Negative Log Likelihood: 33.2297\tSigma2 Prior: -527.3848\tRegularization: 0.0019\n",
            "Iter: 2150  \tTraining Loss: -506.1395    \n",
            "    Negative Log Likelihood: 36.1094\tSigma2 Prior: -542.2508\tRegularization: 0.0019\n",
            "Iter: 2160  \tTraining Loss: -460.4824    \n",
            "    Negative Log Likelihood: 32.2589\tSigma2 Prior: -492.7432\tRegularization: 0.0019\n",
            "Iter: 2170  \tTraining Loss: -509.4265    \n",
            "    Negative Log Likelihood: 37.3632\tSigma2 Prior: -546.7915\tRegularization: 0.0019\n",
            "Iter: 2180  \tTraining Loss: -498.1363    \n",
            "    Negative Log Likelihood: 34.4547\tSigma2 Prior: -532.5928\tRegularization: 0.0019\n",
            "Iter: 2190  \tTraining Loss: -489.6262    \n",
            "    Negative Log Likelihood: 32.4950\tSigma2 Prior: -522.1230\tRegularization: 0.0019\n",
            "Iter: 2200  \tTraining Loss: -470.9723    \n",
            "    Negative Log Likelihood: 32.4619\tSigma2 Prior: -503.4361\tRegularization: 0.0019\n",
            "Iter: 2210  \tTraining Loss: -519.3927    \n",
            "    Negative Log Likelihood: 37.3482\tSigma2 Prior: -556.7428\tRegularization: 0.0019\n",
            "Iter: 2220  \tTraining Loss: -496.2720    \n",
            "    Negative Log Likelihood: 32.4438\tSigma2 Prior: -528.7177\tRegularization: 0.0019\n",
            "Iter: 2230  \tTraining Loss: -500.8426    \n",
            "    Negative Log Likelihood: 33.0904\tSigma2 Prior: -533.9349\tRegularization: 0.0019\n",
            "Iter: 2240  \tTraining Loss: -472.2617    \n",
            "    Negative Log Likelihood: 35.9423\tSigma2 Prior: -508.2059\tRegularization: 0.0019\n",
            "Iter: 2250  \tTraining Loss: -492.5587    \n",
            "    Negative Log Likelihood: 30.7408\tSigma2 Prior: -523.3015\tRegularization: 0.0019\n",
            "Iter: 2260  \tTraining Loss: -472.5036    \n",
            "    Negative Log Likelihood: 33.6132\tSigma2 Prior: -506.1187\tRegularization: 0.0019\n",
            "Iter: 2270  \tTraining Loss: -469.0771    \n",
            "    Negative Log Likelihood: 31.6531\tSigma2 Prior: -500.7321\tRegularization: 0.0019\n",
            "Iter: 2280  \tTraining Loss: -426.8246    \n",
            "    Negative Log Likelihood: 36.8179\tSigma2 Prior: -463.6445\tRegularization: 0.0019\n",
            "Iter: 2290  \tTraining Loss: -489.8369    \n",
            "    Negative Log Likelihood: 33.2964\tSigma2 Prior: -523.1351\tRegularization: 0.0019\n",
            "Iter: 2300  \tTraining Loss: -469.1550    \n",
            "    Negative Log Likelihood: 32.7462\tSigma2 Prior: -501.9031\tRegularization: 0.0019\n",
            "Iter: 2310  \tTraining Loss: -469.9729    \n",
            "    Negative Log Likelihood: 34.4953\tSigma2 Prior: -504.4700\tRegularization: 0.0019\n",
            "Iter: 2320  \tTraining Loss: -430.5823    \n",
            "    Negative Log Likelihood: 41.8545\tSigma2 Prior: -472.4388\tRegularization: 0.0019\n",
            "Iter: 2330  \tTraining Loss: -483.6957    \n",
            "    Negative Log Likelihood: 31.6155\tSigma2 Prior: -515.3131\tRegularization: 0.0019\n",
            "Iter: 2340  \tTraining Loss: -495.5578    \n",
            "    Negative Log Likelihood: 37.7433\tSigma2 Prior: -533.3030\tRegularization: 0.0019\n",
            "Iter: 2350  \tTraining Loss: -514.3614    \n",
            "    Negative Log Likelihood: 30.9283\tSigma2 Prior: -545.2916\tRegularization: 0.0019\n",
            "Iter: 2360  \tTraining Loss: -434.9564    \n",
            "    Negative Log Likelihood: 37.9871\tSigma2 Prior: -472.9454\tRegularization: 0.0019\n",
            "Iter: 2370  \tTraining Loss: -483.3251    \n",
            "    Negative Log Likelihood: 29.2747\tSigma2 Prior: -512.6017\tRegularization: 0.0019\n",
            "Iter: 2380  \tTraining Loss: -475.6095    \n",
            "    Negative Log Likelihood: 33.4175\tSigma2 Prior: -509.0289\tRegularization: 0.0019\n",
            "Iter: 2390  \tTraining Loss: -471.6325    \n",
            "    Negative Log Likelihood: 41.4232\tSigma2 Prior: -513.0577\tRegularization: 0.0019\n",
            "Iter: 2400  \tTraining Loss: -488.2744    \n",
            "    Negative Log Likelihood: 30.7040\tSigma2 Prior: -518.9803\tRegularization: 0.0019\n",
            "Iter: 2410  \tTraining Loss: -481.8295    \n",
            "    Negative Log Likelihood: 30.9375\tSigma2 Prior: -512.7689\tRegularization: 0.0019\n",
            "Iter: 2420  \tTraining Loss: -512.9329    \n",
            "    Negative Log Likelihood: 42.1990\tSigma2 Prior: -555.1338\tRegularization: 0.0019\n",
            "Iter: 2430  \tTraining Loss: -514.3085    \n",
            "    Negative Log Likelihood: 30.2952\tSigma2 Prior: -544.6057\tRegularization: 0.0019\n",
            "Iter: 2440  \tTraining Loss: -509.6721    \n",
            "    Negative Log Likelihood: 32.4997\tSigma2 Prior: -542.1737\tRegularization: 0.0019\n",
            "Iter: 2450  \tTraining Loss: -506.5092    \n",
            "    Negative Log Likelihood: 34.9191\tSigma2 Prior: -541.4303\tRegularization: 0.0019\n",
            "Iter: 2460  \tTraining Loss: -512.1355    \n",
            "    Negative Log Likelihood: 34.0217\tSigma2 Prior: -546.1591\tRegularization: 0.0019\n",
            "Iter: 2470  \tTraining Loss: -504.7982    \n",
            "    Negative Log Likelihood: 35.9958\tSigma2 Prior: -540.7960\tRegularization: 0.0020\n",
            "Iter: 2480  \tTraining Loss: -495.2316    \n",
            "    Negative Log Likelihood: 33.3383\tSigma2 Prior: -528.5718\tRegularization: 0.0020\n",
            "Iter: 2490  \tTraining Loss: -488.9519    \n",
            "    Negative Log Likelihood: 37.3632\tSigma2 Prior: -526.3170\tRegularization: 0.0020\n",
            "Iter: 2500  \tTraining Loss: -469.9005    \n",
            "    Negative Log Likelihood: 33.6887\tSigma2 Prior: -503.5911\tRegularization: 0.0020\n",
            "Iter: 2510  \tTraining Loss: -478.2779    \n",
            "    Negative Log Likelihood: 36.0687\tSigma2 Prior: -514.3486\tRegularization: 0.0020\n",
            "Iter: 2520  \tTraining Loss: -493.5339    \n",
            "    Negative Log Likelihood: 37.9348\tSigma2 Prior: -531.4707\tRegularization: 0.0020\n",
            "Iter: 2530  \tTraining Loss: -490.3199    \n",
            "    Negative Log Likelihood: 38.2241\tSigma2 Prior: -528.5460\tRegularization: 0.0020\n",
            "Iter: 2540  \tTraining Loss: -519.3874    \n",
            "    Negative Log Likelihood: 29.3673\tSigma2 Prior: -548.7567\tRegularization: 0.0020\n",
            "Iter: 2550  \tTraining Loss: -495.6206    \n",
            "    Negative Log Likelihood: 28.7668\tSigma2 Prior: -524.3894\tRegularization: 0.0020\n",
            "Iter: 2560  \tTraining Loss: -511.2352    \n",
            "    Negative Log Likelihood: 36.6878\tSigma2 Prior: -547.9250\tRegularization: 0.0020\n",
            "Iter: 2570  \tTraining Loss: -489.9565    \n",
            "    Negative Log Likelihood: 32.6106\tSigma2 Prior: -522.5691\tRegularization: 0.0020\n",
            "Iter: 2580  \tTraining Loss: -507.9937    \n",
            "    Negative Log Likelihood: 30.5080\tSigma2 Prior: -538.5037\tRegularization: 0.0020\n",
            "Iter: 2590  \tTraining Loss: -483.9972    \n",
            "    Negative Log Likelihood: 36.2238\tSigma2 Prior: -520.2230\tRegularization: 0.0020\n",
            "Iter: 2600  \tTraining Loss: -511.8024    \n",
            "    Negative Log Likelihood: 38.2461\tSigma2 Prior: -550.0505\tRegularization: 0.0020\n",
            "Iter: 2610  \tTraining Loss: -462.3388    \n",
            "    Negative Log Likelihood: 33.0850\tSigma2 Prior: -495.4258\tRegularization: 0.0020\n",
            "Iter: 2620  \tTraining Loss: -490.8089    \n",
            "    Negative Log Likelihood: 30.1383\tSigma2 Prior: -520.9492\tRegularization: 0.0020\n",
            "Iter: 2630  \tTraining Loss: -480.6599    \n",
            "    Negative Log Likelihood: 37.2207\tSigma2 Prior: -517.8826\tRegularization: 0.0020\n",
            "Iter: 2640  \tTraining Loss: -490.5913    \n",
            "    Negative Log Likelihood: 38.4945\tSigma2 Prior: -529.0878\tRegularization: 0.0020\n",
            "Iter: 2650  \tTraining Loss: -499.9623    \n",
            "    Negative Log Likelihood: 30.2474\tSigma2 Prior: -530.2117\tRegularization: 0.0020\n",
            "Iter: 2660  \tTraining Loss: -486.2504    \n",
            "    Negative Log Likelihood: 34.3304\tSigma2 Prior: -520.5828\tRegularization: 0.0020\n",
            "Iter: 2670  \tTraining Loss: -513.3824    \n",
            "    Negative Log Likelihood: 34.4203\tSigma2 Prior: -547.8047\tRegularization: 0.0020\n",
            "Iter: 2680  \tTraining Loss: -500.3869    \n",
            "    Negative Log Likelihood: 35.6411\tSigma2 Prior: -536.0300\tRegularization: 0.0020\n",
            "Iter: 2690  \tTraining Loss: -485.8195    \n",
            "    Negative Log Likelihood: 41.2416\tSigma2 Prior: -527.0631\tRegularization: 0.0020\n",
            "Iter: 2700  \tTraining Loss: -515.1165    \n",
            "    Negative Log Likelihood: 33.2647\tSigma2 Prior: -548.3832\tRegularization: 0.0020\n",
            "Iter: 2710  \tTraining Loss: -454.4807    \n",
            "    Negative Log Likelihood: 33.6963\tSigma2 Prior: -488.1790\tRegularization: 0.0020\n",
            "Iter: 2720  \tTraining Loss: -480.0143    \n",
            "    Negative Log Likelihood: 36.1690\tSigma2 Prior: -516.1852\tRegularization: 0.0020\n",
            "Iter: 2730  \tTraining Loss: -510.0710    \n",
            "    Negative Log Likelihood: 35.8138\tSigma2 Prior: -545.8869\tRegularization: 0.0020\n",
            "Iter: 2740  \tTraining Loss: -496.3939    \n",
            "    Negative Log Likelihood: 34.8053\tSigma2 Prior: -531.2012\tRegularization: 0.0020\n",
            "Iter: 2750  \tTraining Loss: -503.3470    \n",
            "    Negative Log Likelihood: 31.6949\tSigma2 Prior: -535.0439\tRegularization: 0.0020\n",
            "Iter: 2760  \tTraining Loss: -469.7741    \n",
            "    Negative Log Likelihood: 36.3345\tSigma2 Prior: -506.1107\tRegularization: 0.0020\n",
            "Iter: 2770  \tTraining Loss: -484.3163    \n",
            "    Negative Log Likelihood: 32.3171\tSigma2 Prior: -516.6354\tRegularization: 0.0020\n",
            "Iter: 2780  \tTraining Loss: -504.0424    \n",
            "    Negative Log Likelihood: 32.4968\tSigma2 Prior: -536.5413\tRegularization: 0.0020\n",
            "Iter: 2790  \tTraining Loss: -478.1881    \n",
            "    Negative Log Likelihood: 35.8507\tSigma2 Prior: -514.0409\tRegularization: 0.0020\n",
            "Iter: 2800  \tTraining Loss: -499.1971    \n",
            "    Negative Log Likelihood: 31.3839\tSigma2 Prior: -530.5830\tRegularization: 0.0021\n",
            "Iter: 2810  \tTraining Loss: -430.3930    \n",
            "    Negative Log Likelihood: 36.7035\tSigma2 Prior: -467.0985\tRegularization: 0.0021\n",
            "Iter: 2820  \tTraining Loss: -510.2054    \n",
            "    Negative Log Likelihood: 29.2045\tSigma2 Prior: -539.4119\tRegularization: 0.0021\n",
            "Iter: 2830  \tTraining Loss: -499.6511    \n",
            "    Negative Log Likelihood: 41.7254\tSigma2 Prior: -541.3786\tRegularization: 0.0021\n",
            "Iter: 2840  \tTraining Loss: -519.4578    \n",
            "    Negative Log Likelihood: 32.3034\tSigma2 Prior: -551.7632\tRegularization: 0.0021\n",
            "Iter: 2850  \tTraining Loss: -486.8824    \n",
            "    Negative Log Likelihood: 41.8220\tSigma2 Prior: -528.7065\tRegularization: 0.0021\n",
            "Iter: 2860  \tTraining Loss: -518.0856    \n",
            "    Negative Log Likelihood: 32.3148\tSigma2 Prior: -550.4025\tRegularization: 0.0021\n",
            "Iter: 2870  \tTraining Loss: -515.1489    \n",
            "    Negative Log Likelihood: 35.3321\tSigma2 Prior: -550.4830\tRegularization: 0.0021\n",
            "Iter: 2880  \tTraining Loss: -486.2485    \n",
            "    Negative Log Likelihood: 38.8721\tSigma2 Prior: -525.1226\tRegularization: 0.0021\n",
            "Iter: 2890  \tTraining Loss: -515.5396    \n",
            "    Negative Log Likelihood: 33.9688\tSigma2 Prior: -549.5105\tRegularization: 0.0021\n",
            "Iter: 2900  \tTraining Loss: -497.3254    \n",
            "    Negative Log Likelihood: 33.2871\tSigma2 Prior: -530.6145\tRegularization: 0.0021\n",
            "Iter: 2910  \tTraining Loss: -481.8029    \n",
            "    Negative Log Likelihood: 39.5782\tSigma2 Prior: -521.3831\tRegularization: 0.0021\n",
            "Iter: 2920  \tTraining Loss: -492.2885    \n",
            "    Negative Log Likelihood: 36.0410\tSigma2 Prior: -528.3316\tRegularization: 0.0021\n",
            "Iter: 2930  \tTraining Loss: -493.4849    \n",
            "    Negative Log Likelihood: 36.6434\tSigma2 Prior: -530.1304\tRegularization: 0.0021\n",
            "Iter: 2940  \tTraining Loss: -505.0249    \n",
            "    Negative Log Likelihood: 32.6472\tSigma2 Prior: -537.6742\tRegularization: 0.0021\n",
            "Iter: 2950  \tTraining Loss: -494.3826    \n",
            "    Negative Log Likelihood: 36.4694\tSigma2 Prior: -530.8541\tRegularization: 0.0021\n",
            "Iter: 2960  \tTraining Loss: -499.5811    \n",
            "    Negative Log Likelihood: 29.6815\tSigma2 Prior: -529.2646\tRegularization: 0.0021\n",
            "Iter: 2970  \tTraining Loss: -472.0649    \n",
            "    Negative Log Likelihood: 41.9057\tSigma2 Prior: -513.9728\tRegularization: 0.0021\n",
            "Iter: 2980  \tTraining Loss: -505.4883    \n",
            "    Negative Log Likelihood: 30.3326\tSigma2 Prior: -535.8230\tRegularization: 0.0021\n",
            "Iter: 2990  \tTraining Loss: -457.5148    \n",
            "    Negative Log Likelihood: 30.7222\tSigma2 Prior: -488.2391\tRegularization: 0.0021\n",
            "Iter: 3000  \tTraining Loss: -460.2384    \n",
            "    Negative Log Likelihood: 40.0683\tSigma2 Prior: -500.3088\tRegularization: 0.0021\n",
            "Iter: 3010  \tTraining Loss: -485.2755    \n",
            "    Negative Log Likelihood: 27.7245\tSigma2 Prior: -513.0021\tRegularization: 0.0021\n",
            "Iter: 3020  \tTraining Loss: -496.5847    \n",
            "    Negative Log Likelihood: 32.1542\tSigma2 Prior: -528.7410\tRegularization: 0.0021\n",
            "Iter: 3030  \tTraining Loss: -493.4468    \n",
            "    Negative Log Likelihood: 32.2465\tSigma2 Prior: -525.6954\tRegularization: 0.0021\n",
            "Iter: 3040  \tTraining Loss: -381.0577    \n",
            "    Negative Log Likelihood: 41.6915\tSigma2 Prior: -422.7513\tRegularization: 0.0021\n",
            "Iter: 3050  \tTraining Loss: -483.8027    \n",
            "    Negative Log Likelihood: 31.4499\tSigma2 Prior: -515.2548\tRegularization: 0.0021\n",
            "Iter: 3060  \tTraining Loss: -500.5341    \n",
            "    Negative Log Likelihood: 35.6089\tSigma2 Prior: -536.1452\tRegularization: 0.0021\n",
            "Iter: 3070  \tTraining Loss: -439.6666    \n",
            "    Negative Log Likelihood: 40.4140\tSigma2 Prior: -480.0828\tRegularization: 0.0021\n",
            "Iter: 3080  \tTraining Loss: -498.3490    \n",
            "    Negative Log Likelihood: 32.1826\tSigma2 Prior: -530.5337\tRegularization: 0.0021\n",
            "Iter: 3090  \tTraining Loss: -504.2532    \n",
            "    Negative Log Likelihood: 34.8095\tSigma2 Prior: -539.0648\tRegularization: 0.0021\n",
            "Iter: 3100  \tTraining Loss: -513.7275    \n",
            "    Negative Log Likelihood: 40.1920\tSigma2 Prior: -553.9216\tRegularization: 0.0021\n",
            "Iter: 3110  \tTraining Loss: -469.7252    \n",
            "    Negative Log Likelihood: 36.1548\tSigma2 Prior: -505.8821\tRegularization: 0.0021\n",
            "Iter: 3120  \tTraining Loss: -489.2753    \n",
            "    Negative Log Likelihood: 33.4614\tSigma2 Prior: -522.7388\tRegularization: 0.0022\n",
            "Iter: 3130  \tTraining Loss: -485.3458    \n",
            "    Negative Log Likelihood: 34.9602\tSigma2 Prior: -520.3081\tRegularization: 0.0022\n",
            "Iter: 3140  \tTraining Loss: -495.8578    \n",
            "    Negative Log Likelihood: 31.4526\tSigma2 Prior: -527.3126\tRegularization: 0.0022\n",
            "Iter: 3150  \tTraining Loss: -484.6471    \n",
            "    Negative Log Likelihood: 41.5572\tSigma2 Prior: -526.2064\tRegularization: 0.0022\n",
            "Iter: 3160  \tTraining Loss: -483.5173    \n",
            "    Negative Log Likelihood: 31.6765\tSigma2 Prior: -515.1959\tRegularization: 0.0022\n",
            "Iter: 3170  \tTraining Loss: -471.3997    \n",
            "    Negative Log Likelihood: 33.1922\tSigma2 Prior: -504.5941\tRegularization: 0.0022\n",
            "Iter: 3180  \tTraining Loss: -492.0540    \n",
            "    Negative Log Likelihood: 38.4573\tSigma2 Prior: -530.5135\tRegularization: 0.0022\n",
            "Iter: 3190  \tTraining Loss: -488.3761    \n",
            "    Negative Log Likelihood: 32.4785\tSigma2 Prior: -520.8568\tRegularization: 0.0022\n",
            "Iter: 3200  \tTraining Loss: -501.5467    \n",
            "    Negative Log Likelihood: 36.9495\tSigma2 Prior: -538.4984\tRegularization: 0.0022\n",
            "Iter: 3210  \tTraining Loss: -474.7919    \n",
            "    Negative Log Likelihood: 38.0108\tSigma2 Prior: -512.8049\tRegularization: 0.0022\n",
            "Iter: 3220  \tTraining Loss: -508.1169    \n",
            "    Negative Log Likelihood: 35.6819\tSigma2 Prior: -543.8010\tRegularization: 0.0022\n",
            "Iter: 3230  \tTraining Loss: -483.5889    \n",
            "    Negative Log Likelihood: 37.7969\tSigma2 Prior: -521.3880\tRegularization: 0.0022\n",
            "Iter: 3240  \tTraining Loss: -464.7668    \n",
            "    Negative Log Likelihood: 27.1884\tSigma2 Prior: -491.9575\tRegularization: 0.0022\n",
            "Iter: 3250  \tTraining Loss: -502.5118    \n",
            "    Negative Log Likelihood: 35.4108\tSigma2 Prior: -537.9248\tRegularization: 0.0022\n",
            "Iter: 3260  \tTraining Loss: -485.4360    \n",
            "    Negative Log Likelihood: 41.4201\tSigma2 Prior: -526.8583\tRegularization: 0.0022\n",
            "Iter: 3270  \tTraining Loss: -503.1613    \n",
            "    Negative Log Likelihood: 31.5614\tSigma2 Prior: -534.7249\tRegularization: 0.0022\n",
            "Iter: 3280  \tTraining Loss: -526.4275    \n",
            "    Negative Log Likelihood: 33.5849\tSigma2 Prior: -560.0146\tRegularization: 0.0022\n",
            "Iter: 3290  \tTraining Loss: -455.2312    \n",
            "    Negative Log Likelihood: 33.0666\tSigma2 Prior: -488.3000\tRegularization: 0.0022\n",
            "Iter: 3300  \tTraining Loss: -455.0403    \n",
            "    Negative Log Likelihood: 34.2958\tSigma2 Prior: -489.3383\tRegularization: 0.0022\n",
            "Iter: 3310  \tTraining Loss: -508.0711    \n",
            "    Negative Log Likelihood: 32.9583\tSigma2 Prior: -541.0316\tRegularization: 0.0022\n",
            "Iter: 3320  \tTraining Loss: -485.7313    \n",
            "    Negative Log Likelihood: 30.0194\tSigma2 Prior: -515.7529\tRegularization: 0.0022\n",
            "Iter: 3330  \tTraining Loss: -508.1523    \n",
            "    Negative Log Likelihood: 36.9141\tSigma2 Prior: -545.0687\tRegularization: 0.0022\n",
            "Iter: 3340  \tTraining Loss: -478.6767    \n",
            "    Negative Log Likelihood: 33.1951\tSigma2 Prior: -511.8740\tRegularization: 0.0022\n",
            "Iter: 3350  \tTraining Loss: -502.0767    \n",
            "    Negative Log Likelihood: 31.2394\tSigma2 Prior: -533.3184\tRegularization: 0.0022\n",
            "Iter: 3360  \tTraining Loss: -471.6388    \n",
            "    Negative Log Likelihood: 39.3113\tSigma2 Prior: -510.9523\tRegularization: 0.0022\n",
            "Iter: 3370  \tTraining Loss: -502.3212    \n",
            "    Negative Log Likelihood: 35.4678\tSigma2 Prior: -537.7913\tRegularization: 0.0022\n",
            "Iter: 3380  \tTraining Loss: -464.5478    \n",
            "    Negative Log Likelihood: 34.5553\tSigma2 Prior: -499.1053\tRegularization: 0.0022\n",
            "Iter: 3390  \tTraining Loss: -493.1819    \n",
            "    Negative Log Likelihood: 35.6601\tSigma2 Prior: -528.8442\tRegularization: 0.0022\n",
            "Iter: 3400  \tTraining Loss: -507.8257    \n",
            "    Negative Log Likelihood: 33.4732\tSigma2 Prior: -541.3011\tRegularization: 0.0022\n",
            "Iter: 3410  \tTraining Loss: -477.0613    \n",
            "    Negative Log Likelihood: 35.3330\tSigma2 Prior: -512.3965\tRegularization: 0.0022\n",
            "Iter: 3420  \tTraining Loss: -507.1135    \n",
            "    Negative Log Likelihood: 33.4875\tSigma2 Prior: -540.6033\tRegularization: 0.0022\n",
            "Iter: 3430  \tTraining Loss: -510.0728    \n",
            "    Negative Log Likelihood: 32.9625\tSigma2 Prior: -543.0375\tRegularization: 0.0022\n",
            "Iter: 3440  \tTraining Loss: -506.5721    \n",
            "    Negative Log Likelihood: 36.9500\tSigma2 Prior: -543.5244\tRegularization: 0.0022\n",
            "Iter: 3450  \tTraining Loss: -432.3520    \n",
            "    Negative Log Likelihood: 35.2151\tSigma2 Prior: -467.5693\tRegularization: 0.0023\n",
            "Iter: 3460  \tTraining Loss: -471.1209    \n",
            "    Negative Log Likelihood: 32.0415\tSigma2 Prior: -503.1646\tRegularization: 0.0023\n",
            "Iter: 3470  \tTraining Loss: -497.3477    \n",
            "    Negative Log Likelihood: 35.7798\tSigma2 Prior: -533.1298\tRegularization: 0.0023\n",
            "Iter: 3480  \tTraining Loss: -464.8320    \n",
            "    Negative Log Likelihood: 34.3787\tSigma2 Prior: -499.2130\tRegularization: 0.0023\n",
            "Iter: 3490  \tTraining Loss: -494.5952    \n",
            "    Negative Log Likelihood: 39.1209\tSigma2 Prior: -533.7184\tRegularization: 0.0023\n",
            "Iter: 3500  \tTraining Loss: -471.3662    \n",
            "    Negative Log Likelihood: 33.8756\tSigma2 Prior: -505.2440\tRegularization: 0.0023\n",
            "Iter: 3510  \tTraining Loss: -480.1025    \n",
            "    Negative Log Likelihood: 32.9994\tSigma2 Prior: -513.1042\tRegularization: 0.0023\n",
            "Iter: 3520  \tTraining Loss: -474.7760    \n",
            "    Negative Log Likelihood: 34.6539\tSigma2 Prior: -509.4322\tRegularization: 0.0023\n",
            "Iter: 3530  \tTraining Loss: -509.3921    \n",
            "    Negative Log Likelihood: 29.9512\tSigma2 Prior: -539.3456\tRegularization: 0.0023\n",
            "Iter: 3540  \tTraining Loss: -474.7201    \n",
            "    Negative Log Likelihood: 32.8573\tSigma2 Prior: -507.5797\tRegularization: 0.0023\n",
            "Iter: 3550  \tTraining Loss: -491.6459    \n",
            "    Negative Log Likelihood: 35.0994\tSigma2 Prior: -526.7477\tRegularization: 0.0023\n",
            "Iter: 3560  \tTraining Loss: -500.3747    \n",
            "    Negative Log Likelihood: 35.5174\tSigma2 Prior: -535.8945\tRegularization: 0.0023\n",
            "Iter: 3570  \tTraining Loss: -506.6507    \n",
            "    Negative Log Likelihood: 31.8915\tSigma2 Prior: -538.5445\tRegularization: 0.0023\n",
            "Iter: 3580  \tTraining Loss: -500.1424    \n",
            "    Negative Log Likelihood: 33.4362\tSigma2 Prior: -533.5809\tRegularization: 0.0023\n",
            "Iter: 3590  \tTraining Loss: -495.1924    \n",
            "    Negative Log Likelihood: 36.7204\tSigma2 Prior: -531.9151\tRegularization: 0.0023\n",
            "Iter: 3600  \tTraining Loss: -506.0645    \n",
            "    Negative Log Likelihood: 30.7152\tSigma2 Prior: -536.7820\tRegularization: 0.0023\n",
            "Iter: 3610  \tTraining Loss: -520.8721    \n",
            "    Negative Log Likelihood: 33.1671\tSigma2 Prior: -554.0416\tRegularization: 0.0023\n",
            "Iter: 3620  \tTraining Loss: -498.8385    \n",
            "    Negative Log Likelihood: 37.9743\tSigma2 Prior: -536.8151\tRegularization: 0.0023\n",
            "Iter: 3630  \tTraining Loss: -444.7482    \n",
            "    Negative Log Likelihood: 34.4596\tSigma2 Prior: -479.2102\tRegularization: 0.0023\n",
            "Iter: 3640  \tTraining Loss: -470.7527    \n",
            "    Negative Log Likelihood: 29.9538\tSigma2 Prior: -500.7088\tRegularization: 0.0023\n",
            "Iter: 3650  \tTraining Loss: -503.7783    \n",
            "    Negative Log Likelihood: 38.9202\tSigma2 Prior: -542.7007\tRegularization: 0.0023\n",
            "Iter: 3660  \tTraining Loss: -502.6086    \n",
            "    Negative Log Likelihood: 29.5458\tSigma2 Prior: -532.1566\tRegularization: 0.0023\n",
            "Iter: 3670  \tTraining Loss: -517.6008    \n",
            "    Negative Log Likelihood: 36.7493\tSigma2 Prior: -554.3524\tRegularization: 0.0023\n",
            "Iter: 3680  \tTraining Loss: -512.4243    \n",
            "    Negative Log Likelihood: 34.9273\tSigma2 Prior: -547.3539\tRegularization: 0.0023\n",
            "Iter: 3690  \tTraining Loss: -424.9765    \n",
            "    Negative Log Likelihood: 37.2773\tSigma2 Prior: -462.2561\tRegularization: 0.0023\n",
            "Iter: 3700  \tTraining Loss: -469.7118    \n",
            "    Negative Log Likelihood: 34.3527\tSigma2 Prior: -504.0668\tRegularization: 0.0023\n",
            "Iter: 3710  \tTraining Loss: -467.3698    \n",
            "    Negative Log Likelihood: 32.7994\tSigma2 Prior: -500.1716\tRegularization: 0.0023\n",
            "Iter: 3720  \tTraining Loss: -494.1668    \n",
            "    Negative Log Likelihood: 35.0043\tSigma2 Prior: -529.1735\tRegularization: 0.0023\n",
            "Iter: 3730  \tTraining Loss: -480.3819    \n",
            "    Negative Log Likelihood: 36.4108\tSigma2 Prior: -516.7950\tRegularization: 0.0023\n",
            "Iter: 3740  \tTraining Loss: -462.9676    \n",
            "    Negative Log Likelihood: 33.8633\tSigma2 Prior: -496.8332\tRegularization: 0.0023\n",
            "Iter: 3750  \tTraining Loss: -501.1988    \n",
            "    Negative Log Likelihood: 27.8724\tSigma2 Prior: -529.0735\tRegularization: 0.0023\n",
            "Iter: 3760  \tTraining Loss: -504.2257    \n",
            "    Negative Log Likelihood: 37.7185\tSigma2 Prior: -541.9465\tRegularization: 0.0023\n",
            "Iter: 3770  \tTraining Loss: -483.9314    \n",
            "    Negative Log Likelihood: 34.5240\tSigma2 Prior: -518.4578\tRegularization: 0.0024\n",
            "Iter: 3780  \tTraining Loss: -493.2373    \n",
            "    Negative Log Likelihood: 29.6680\tSigma2 Prior: -522.9076\tRegularization: 0.0024\n",
            "Iter: 3790  \tTraining Loss: -479.1225    \n",
            "    Negative Log Likelihood: 40.4248\tSigma2 Prior: -519.5497\tRegularization: 0.0024\n",
            "Iter: 3800  \tTraining Loss: -485.5388    \n",
            "    Negative Log Likelihood: 26.2029\tSigma2 Prior: -511.7440\tRegularization: 0.0024\n",
            "Iter: 3810  \tTraining Loss: -478.6900    \n",
            "    Negative Log Likelihood: 35.2449\tSigma2 Prior: -513.9373\tRegularization: 0.0024\n",
            "Iter: 3820  \tTraining Loss: -506.7925    \n",
            "    Negative Log Likelihood: 33.7207\tSigma2 Prior: -540.5156\tRegularization: 0.0024\n",
            "Iter: 3830  \tTraining Loss: -486.7292    \n",
            "    Negative Log Likelihood: 27.1702\tSigma2 Prior: -513.9018\tRegularization: 0.0024\n",
            "Iter: 3840  \tTraining Loss: -475.5503    \n",
            "    Negative Log Likelihood: 38.3991\tSigma2 Prior: -513.9518\tRegularization: 0.0024\n",
            "Iter: 3850  \tTraining Loss: -513.9702    \n",
            "    Negative Log Likelihood: 32.9444\tSigma2 Prior: -546.9171\tRegularization: 0.0024\n",
            "Iter: 3860  \tTraining Loss: -487.7552    \n",
            "    Negative Log Likelihood: 39.3686\tSigma2 Prior: -527.1262\tRegularization: 0.0024\n",
            "Iter: 3870  \tTraining Loss: -470.9198    \n",
            "    Negative Log Likelihood: 32.2499\tSigma2 Prior: -503.1721\tRegularization: 0.0024\n",
            "Iter: 3880  \tTraining Loss: -504.9849    \n",
            "    Negative Log Likelihood: 33.0832\tSigma2 Prior: -538.0706\tRegularization: 0.0024\n",
            "Iter: 3890  \tTraining Loss: -452.3625    \n",
            "    Negative Log Likelihood: 37.7715\tSigma2 Prior: -490.1364\tRegularization: 0.0024\n",
            "Iter: 3900  \tTraining Loss: -486.9366    \n",
            "    Negative Log Likelihood: 32.3909\tSigma2 Prior: -519.3299\tRegularization: 0.0024\n",
            "Iter: 3910  \tTraining Loss: -507.5297    \n",
            "    Negative Log Likelihood: 32.2730\tSigma2 Prior: -539.8052\tRegularization: 0.0024\n",
            "Iter: 3920  \tTraining Loss: -509.3271    \n",
            "    Negative Log Likelihood: 36.9313\tSigma2 Prior: -546.2607\tRegularization: 0.0024\n",
            "Iter: 3930  \tTraining Loss: -494.6559    \n",
            "    Negative Log Likelihood: 32.6708\tSigma2 Prior: -527.3291\tRegularization: 0.0024\n",
            "Iter: 3940  \tTraining Loss: -509.3146    \n",
            "    Negative Log Likelihood: 31.0900\tSigma2 Prior: -540.4070\tRegularization: 0.0024\n",
            "Iter: 3950  \tTraining Loss: -475.5833    \n",
            "    Negative Log Likelihood: 39.3662\tSigma2 Prior: -514.9519\tRegularization: 0.0024\n",
            "Iter: 3960  \tTraining Loss: -363.3024    \n",
            "    Negative Log Likelihood: 36.9613\tSigma2 Prior: -400.2661\tRegularization: 0.0024\n",
            "Iter: 3970  \tTraining Loss: -437.1328    \n",
            "    Negative Log Likelihood: 34.8219\tSigma2 Prior: -471.9571\tRegularization: 0.0024\n",
            "Iter: 3980  \tTraining Loss: -478.6256    \n",
            "    Negative Log Likelihood: 34.7071\tSigma2 Prior: -513.3352\tRegularization: 0.0024\n",
            "Iter: 3990  \tTraining Loss: -497.9386    \n",
            "    Negative Log Likelihood: 33.8848\tSigma2 Prior: -531.8259\tRegularization: 0.0024\n",
            "Iter: 4000  \tTraining Loss: -491.0983    \n",
            "    Negative Log Likelihood: 30.2189\tSigma2 Prior: -521.3196\tRegularization: 0.0024\n",
            "Iter: 4010  \tTraining Loss: -477.8115    \n",
            "    Negative Log Likelihood: 33.1594\tSigma2 Prior: -510.9734\tRegularization: 0.0024\n",
            "Iter: 4020  \tTraining Loss: -495.1534    \n",
            "    Negative Log Likelihood: 32.4718\tSigma2 Prior: -527.6277\tRegularization: 0.0024\n",
            "Iter: 4030  \tTraining Loss: -424.9733    \n",
            "    Negative Log Likelihood: 37.3378\tSigma2 Prior: -462.3135\tRegularization: 0.0024\n",
            "Iter: 4040  \tTraining Loss: -514.2150    \n",
            "    Negative Log Likelihood: 33.5203\tSigma2 Prior: -547.7377\tRegularization: 0.0024\n",
            "Iter: 4050  \tTraining Loss: -517.4710    \n",
            "    Negative Log Likelihood: 39.5938\tSigma2 Prior: -557.0673\tRegularization: 0.0024\n",
            "Iter: 4060  \tTraining Loss: -470.3662    \n",
            "    Negative Log Likelihood: 32.3262\tSigma2 Prior: -502.6948\tRegularization: 0.0024\n",
            "Iter: 4070  \tTraining Loss: -477.3207    \n",
            "    Negative Log Likelihood: 34.6076\tSigma2 Prior: -511.9307\tRegularization: 0.0024\n",
            "Iter: 4080  \tTraining Loss: -482.1827    \n",
            "    Negative Log Likelihood: 33.4163\tSigma2 Prior: -515.6014\tRegularization: 0.0025\n",
            "Iter: 4090  \tTraining Loss: -512.5361    \n",
            "    Negative Log Likelihood: 33.3900\tSigma2 Prior: -545.9286\tRegularization: 0.0025\n",
            "Iter: 4100  \tTraining Loss: -513.7455    \n",
            "    Negative Log Likelihood: 38.1088\tSigma2 Prior: -551.8567\tRegularization: 0.0025\n",
            "Iter: 4110  \tTraining Loss: -503.3257    \n",
            "    Negative Log Likelihood: 35.7474\tSigma2 Prior: -539.0756\tRegularization: 0.0025\n",
            "Iter: 4120  \tTraining Loss: -470.1432    \n",
            "    Negative Log Likelihood: 35.6379\tSigma2 Prior: -505.7836\tRegularization: 0.0025\n",
            "Iter: 4130  \tTraining Loss: -451.7784    \n",
            "    Negative Log Likelihood: 33.9068\tSigma2 Prior: -485.6877\tRegularization: 0.0025\n",
            "Iter: 4140  \tTraining Loss: -486.5017    \n",
            "    Negative Log Likelihood: 32.8495\tSigma2 Prior: -519.3536\tRegularization: 0.0025\n",
            "Iter: 4150  \tTraining Loss: -504.6774    \n",
            "    Negative Log Likelihood: 34.7239\tSigma2 Prior: -539.4038\tRegularization: 0.0025\n",
            "Iter: 4160  \tTraining Loss: -493.3941    \n",
            "    Negative Log Likelihood: 33.6422\tSigma2 Prior: -527.0387\tRegularization: 0.0025\n",
            "Iter: 4170  \tTraining Loss: -503.5425    \n",
            "    Negative Log Likelihood: 38.7471\tSigma2 Prior: -542.2921\tRegularization: 0.0025\n",
            "Iter: 4180  \tTraining Loss: -506.6369    \n",
            "    Negative Log Likelihood: 34.2949\tSigma2 Prior: -540.9343\tRegularization: 0.0025\n",
            "Iter: 4190  \tTraining Loss: -520.7350    \n",
            "    Negative Log Likelihood: 34.1065\tSigma2 Prior: -554.8441\tRegularization: 0.0025\n",
            "Iter: 4200  \tTraining Loss: -498.1233    \n",
            "    Negative Log Likelihood: 31.1190\tSigma2 Prior: -529.2448\tRegularization: 0.0025\n",
            "Iter: 4210  \tTraining Loss: -503.1569    \n",
            "    Negative Log Likelihood: 33.9653\tSigma2 Prior: -537.1248\tRegularization: 0.0025\n",
            "Iter: 4220  \tTraining Loss: -511.0568    \n",
            "    Negative Log Likelihood: 36.0004\tSigma2 Prior: -547.0596\tRegularization: 0.0025\n",
            "Iter: 4230  \tTraining Loss: -500.5876    \n",
            "    Negative Log Likelihood: 31.0882\tSigma2 Prior: -531.6783\tRegularization: 0.0025\n",
            "Iter: 4240  \tTraining Loss: -497.3505    \n",
            "    Negative Log Likelihood: 40.5453\tSigma2 Prior: -537.8984\tRegularization: 0.0025\n",
            "Iter: 4250  \tTraining Loss: -494.6104    \n",
            "    Negative Log Likelihood: 33.2875\tSigma2 Prior: -527.9003\tRegularization: 0.0025\n",
            "Iter: 4260  \tTraining Loss: -470.4920    \n",
            "    Negative Log Likelihood: 36.0283\tSigma2 Prior: -506.5229\tRegularization: 0.0025\n",
            "Iter: 4270  \tTraining Loss: -482.2183    \n",
            "    Negative Log Likelihood: 31.3834\tSigma2 Prior: -513.6042\tRegularization: 0.0025\n",
            "Iter: 4280  \tTraining Loss: -477.5238    \n",
            "    Negative Log Likelihood: 36.8676\tSigma2 Prior: -514.3939\tRegularization: 0.0025\n",
            "Iter: 4290  \tTraining Loss: -525.2615    \n",
            "    Negative Log Likelihood: 33.1408\tSigma2 Prior: -558.4048\tRegularization: 0.0025\n",
            "Iter: 4300  \tTraining Loss: -498.2220    \n",
            "    Negative Log Likelihood: 37.4355\tSigma2 Prior: -535.6600\tRegularization: 0.0025\n",
            "Iter: 4310  \tTraining Loss: -507.2474    \n",
            "    Negative Log Likelihood: 27.3160\tSigma2 Prior: -534.5660\tRegularization: 0.0025\n",
            "Iter: 4320  \tTraining Loss: -462.0815    \n",
            "    Negative Log Likelihood: 47.8663\tSigma2 Prior: -509.9503\tRegularization: 0.0025\n",
            "Iter: 4330  \tTraining Loss: -507.1092    \n",
            "    Negative Log Likelihood: 29.7073\tSigma2 Prior: -536.8190\tRegularization: 0.0025\n",
            "Iter: 4340  \tTraining Loss: -519.4081    \n",
            "    Negative Log Likelihood: 36.4531\tSigma2 Prior: -555.8637\tRegularization: 0.0025\n",
            "Iter: 4350  \tTraining Loss: -473.5943    \n",
            "    Negative Log Likelihood: 32.4597\tSigma2 Prior: -506.0565\tRegularization: 0.0025\n",
            "Iter: 4360  \tTraining Loss: -500.2059    \n",
            "    Negative Log Likelihood: 35.0673\tSigma2 Prior: -535.2758\tRegularization: 0.0025\n",
            "Iter: 4370  \tTraining Loss: -508.5135    \n",
            "    Negative Log Likelihood: 33.5358\tSigma2 Prior: -542.0518\tRegularization: 0.0025\n",
            "Iter: 4380  \tTraining Loss: -510.4688    \n",
            "    Negative Log Likelihood: 36.6705\tSigma2 Prior: -547.1418\tRegularization: 0.0025\n",
            "Iter: 4390  \tTraining Loss: -447.1114    \n",
            "    Negative Log Likelihood: 34.9456\tSigma2 Prior: -482.0596\tRegularization: 0.0026\n",
            "Iter: 4400  \tTraining Loss: -497.1647    \n",
            "    Negative Log Likelihood: 33.4492\tSigma2 Prior: -530.6165\tRegularization: 0.0026\n",
            "Iter: 4410  \tTraining Loss: -477.9341    \n",
            "    Negative Log Likelihood: 38.7645\tSigma2 Prior: -516.7011\tRegularization: 0.0026\n",
            "Iter: 4420  \tTraining Loss: -507.1122    \n",
            "    Negative Log Likelihood: 29.2592\tSigma2 Prior: -536.3740\tRegularization: 0.0026\n",
            "Iter: 4430  \tTraining Loss: -474.6117    \n",
            "    Negative Log Likelihood: 36.7778\tSigma2 Prior: -511.3921\tRegularization: 0.0026\n",
            "Iter: 4440  \tTraining Loss: -465.1894    \n",
            "    Negative Log Likelihood: 37.5336\tSigma2 Prior: -502.7256\tRegularization: 0.0026\n",
            "Iter: 4450  \tTraining Loss: -453.3940    \n",
            "    Negative Log Likelihood: 34.3555\tSigma2 Prior: -487.7520\tRegularization: 0.0026\n",
            "Iter: 4460  \tTraining Loss: -515.3336    \n",
            "    Negative Log Likelihood: 38.9726\tSigma2 Prior: -554.3087\tRegularization: 0.0026\n",
            "Iter: 4470  \tTraining Loss: -451.0469    \n",
            "    Negative Log Likelihood: 34.0679\tSigma2 Prior: -485.1174\tRegularization: 0.0026\n",
            "Iter: 4480  \tTraining Loss: -486.0834    \n",
            "    Negative Log Likelihood: 31.9183\tSigma2 Prior: -518.0043\tRegularization: 0.0026\n",
            "Iter: 4490  \tTraining Loss: -495.7982    \n",
            "    Negative Log Likelihood: 33.4691\tSigma2 Prior: -529.2698\tRegularization: 0.0026\n",
            "Iter: 4500  \tTraining Loss: -474.8557    \n",
            "    Negative Log Likelihood: 40.0876\tSigma2 Prior: -514.9458\tRegularization: 0.0026\n",
            "Iter: 4510  \tTraining Loss: -483.4589    \n",
            "    Negative Log Likelihood: 31.2794\tSigma2 Prior: -514.7408\tRegularization: 0.0026\n",
            "Iter: 4520  \tTraining Loss: -503.9742    \n",
            "    Negative Log Likelihood: 40.9715\tSigma2 Prior: -544.9483\tRegularization: 0.0026\n",
            "Iter: 4530  \tTraining Loss: -497.7675    \n",
            "    Negative Log Likelihood: 35.2049\tSigma2 Prior: -532.9750\tRegularization: 0.0026\n",
            "Iter: 4540  \tTraining Loss: -479.4174    \n",
            "    Negative Log Likelihood: 32.4003\tSigma2 Prior: -511.8203\tRegularization: 0.0026\n",
            "Iter: 4550  \tTraining Loss: -447.9016    \n",
            "    Negative Log Likelihood: 39.1455\tSigma2 Prior: -487.0497\tRegularization: 0.0026\n",
            "Iter: 4560  \tTraining Loss: -494.1317    \n",
            "    Negative Log Likelihood: 33.6540\tSigma2 Prior: -527.7883\tRegularization: 0.0026\n",
            "Iter: 4570  \tTraining Loss: -498.0991    \n",
            "    Negative Log Likelihood: 34.2375\tSigma2 Prior: -532.3392\tRegularization: 0.0026\n",
            "Iter: 4580  \tTraining Loss: -493.3607    \n",
            "    Negative Log Likelihood: 38.1098\tSigma2 Prior: -531.4731\tRegularization: 0.0026\n",
            "Iter: 4590  \tTraining Loss: -475.4243    \n",
            "    Negative Log Likelihood: 35.0081\tSigma2 Prior: -510.4351\tRegularization: 0.0026\n",
            "Iter: 4600  \tTraining Loss: -461.7004    \n",
            "    Negative Log Likelihood: 30.7895\tSigma2 Prior: -492.4925\tRegularization: 0.0026\n",
            "Iter: 4610  \tTraining Loss: -504.0602    \n",
            "    Negative Log Likelihood: 36.3208\tSigma2 Prior: -540.3836\tRegularization: 0.0026\n",
            "Iter: 4620  \tTraining Loss: -503.4043    \n",
            "    Negative Log Likelihood: 32.1469\tSigma2 Prior: -535.5538\tRegularization: 0.0026\n",
            "Iter: 4630  \tTraining Loss: -499.6075    \n",
            "    Negative Log Likelihood: 38.6816\tSigma2 Prior: -538.2917\tRegularization: 0.0026\n",
            "Iter: 4640  \tTraining Loss: -503.9794    \n",
            "    Negative Log Likelihood: 32.1269\tSigma2 Prior: -536.1089\tRegularization: 0.0026\n",
            "Iter: 4650  \tTraining Loss: -510.8788    \n",
            "    Negative Log Likelihood: 37.0447\tSigma2 Prior: -547.9260\tRegularization: 0.0026\n",
            "Iter: 4660  \tTraining Loss: -497.4303    \n",
            "    Negative Log Likelihood: 38.2365\tSigma2 Prior: -535.6694\tRegularization: 0.0026\n",
            "Iter: 4670  \tTraining Loss: -497.7282    \n",
            "    Negative Log Likelihood: 38.2048\tSigma2 Prior: -535.9357\tRegularization: 0.0026\n",
            "Iter: 4680  \tTraining Loss: -510.0125    \n",
            "    Negative Log Likelihood: 29.3630\tSigma2 Prior: -539.3782\tRegularization: 0.0026\n",
            "Iter: 4690  \tTraining Loss: -502.0450    \n",
            "    Negative Log Likelihood: 33.8341\tSigma2 Prior: -535.8817\tRegularization: 0.0026\n",
            "Iter: 4700  \tTraining Loss: -487.0383    \n",
            "    Negative Log Likelihood: 41.4898\tSigma2 Prior: -528.5308\tRegularization: 0.0026\n",
            "Iter: 4710  \tTraining Loss: -495.5588    \n",
            "    Negative Log Likelihood: 29.4713\tSigma2 Prior: -525.0328\tRegularization: 0.0027\n",
            "Iter: 4720  \tTraining Loss: -514.7918    \n",
            "    Negative Log Likelihood: 36.6611\tSigma2 Prior: -551.4556\tRegularization: 0.0027\n",
            "Iter: 4730  \tTraining Loss: -497.6960    \n",
            "    Negative Log Likelihood: 32.8305\tSigma2 Prior: -530.5292\tRegularization: 0.0027\n",
            "Iter: 4740  \tTraining Loss: -471.1711    \n",
            "    Negative Log Likelihood: 34.4942\tSigma2 Prior: -505.6680\tRegularization: 0.0027\n",
            "Iter: 4750  \tTraining Loss: -481.0221    \n",
            "    Negative Log Likelihood: 33.7358\tSigma2 Prior: -514.7606\tRegularization: 0.0027\n",
            "Iter: 4760  \tTraining Loss: -512.2986    \n",
            "    Negative Log Likelihood: 33.7375\tSigma2 Prior: -546.0389\tRegularization: 0.0027\n",
            "Iter: 4770  \tTraining Loss: -497.5676    \n",
            "    Negative Log Likelihood: 29.7969\tSigma2 Prior: -527.3672\tRegularization: 0.0027\n",
            "Iter: 4780  \tTraining Loss: -472.7599    \n",
            "    Negative Log Likelihood: 26.7681\tSigma2 Prior: -499.5307\tRegularization: 0.0027\n",
            "Iter: 4790  \tTraining Loss: -473.6888    \n",
            "    Negative Log Likelihood: 36.8870\tSigma2 Prior: -510.5785\tRegularization: 0.0027\n",
            "Iter: 4800  \tTraining Loss: -426.4385    \n",
            "    Negative Log Likelihood: 41.6824\tSigma2 Prior: -468.1236\tRegularization: 0.0027\n",
            "Iter: 4810  \tTraining Loss: -464.3158    \n",
            "    Negative Log Likelihood: 29.0547\tSigma2 Prior: -493.3731\tRegularization: 0.0027\n",
            "Iter: 4820  \tTraining Loss: -490.8883    \n",
            "    Negative Log Likelihood: 35.9128\tSigma2 Prior: -526.8037\tRegularization: 0.0027\n",
            "Iter: 4830  \tTraining Loss: -503.3884    \n",
            "    Negative Log Likelihood: 31.7748\tSigma2 Prior: -535.1658\tRegularization: 0.0027\n",
            "Iter: 4840  \tTraining Loss: -497.6022    \n",
            "    Negative Log Likelihood: 34.5317\tSigma2 Prior: -532.1366\tRegularization: 0.0027\n",
            "Iter: 4850  \tTraining Loss: -499.5672    \n",
            "    Negative Log Likelihood: 36.2441\tSigma2 Prior: -535.8140\tRegularization: 0.0027\n",
            "Iter: 4860  \tTraining Loss: -495.8678    \n",
            "    Negative Log Likelihood: 33.9557\tSigma2 Prior: -529.8262\tRegularization: 0.0027\n",
            "Iter: 4870  \tTraining Loss: -521.4783    \n",
            "    Negative Log Likelihood: 29.9264\tSigma2 Prior: -551.4073\tRegularization: 0.0027\n",
            "Iter: 4880  \tTraining Loss: -498.4242    \n",
            "    Negative Log Likelihood: 28.6875\tSigma2 Prior: -527.1144\tRegularization: 0.0027\n",
            "Iter: 4890  \tTraining Loss: -499.9880    \n",
            "    Negative Log Likelihood: 30.7310\tSigma2 Prior: -530.7217\tRegularization: 0.0027\n",
            "Iter: 4900  \tTraining Loss: -495.1142    \n",
            "    Negative Log Likelihood: 33.3899\tSigma2 Prior: -528.5068\tRegularization: 0.0027\n",
            "Iter: 4910  \tTraining Loss: -514.1433    \n",
            "    Negative Log Likelihood: 26.5591\tSigma2 Prior: -540.7051\tRegularization: 0.0027\n",
            "Iter: 4920  \tTraining Loss: -489.3091    \n",
            "    Negative Log Likelihood: 38.4678\tSigma2 Prior: -527.7796\tRegularization: 0.0027\n",
            "Iter: 4930  \tTraining Loss: -487.0993    \n",
            "    Negative Log Likelihood: 32.1422\tSigma2 Prior: -519.2442\tRegularization: 0.0027\n",
            "Iter: 4940  \tTraining Loss: -479.4956    \n",
            "    Negative Log Likelihood: 31.1111\tSigma2 Prior: -510.6095\tRegularization: 0.0027\n",
            "Iter: 4950  \tTraining Loss: -504.7397    \n",
            "    Negative Log Likelihood: 31.5869\tSigma2 Prior: -536.3293\tRegularization: 0.0027\n",
            "Iter: 4960  \tTraining Loss: -506.5507    \n",
            "    Negative Log Likelihood: 32.5309\tSigma2 Prior: -539.0844\tRegularization: 0.0027\n",
            "Iter: 4970  \tTraining Loss: -492.2231    \n",
            "    Negative Log Likelihood: 33.0477\tSigma2 Prior: -525.2736\tRegularization: 0.0027\n",
            "Iter: 4980  \tTraining Loss: -450.1975    \n",
            "    Negative Log Likelihood: 37.6839\tSigma2 Prior: -487.8842\tRegularization: 0.0027\n",
            "Iter: 4990  \tTraining Loss: -488.9258    \n",
            "    Negative Log Likelihood: 34.1075\tSigma2 Prior: -523.0361\tRegularization: 0.0027\n",
            "Iter: 5000  \tTraining Loss: -480.2396    \n",
            "    Negative Log Likelihood: 32.9783\tSigma2 Prior: -513.2206\tRegularization: 0.0027\n",
            "Iter: 5010  \tTraining Loss: -508.7491    \n",
            "    Negative Log Likelihood: 31.8980\tSigma2 Prior: -540.6498\tRegularization: 0.0027\n",
            "Iter: 5020  \tTraining Loss: -494.2427    \n",
            "    Negative Log Likelihood: 39.0180\tSigma2 Prior: -533.2634\tRegularization: 0.0028\n",
            "Iter: 5030  \tTraining Loss: -501.4336    \n",
            "    Negative Log Likelihood: 32.7064\tSigma2 Prior: -534.1427\tRegularization: 0.0028\n",
            "Iter: 5040  \tTraining Loss: -494.4726    \n",
            "    Negative Log Likelihood: 32.2328\tSigma2 Prior: -526.7081\tRegularization: 0.0028\n",
            "Iter: 5050  \tTraining Loss: -507.5886    \n",
            "    Negative Log Likelihood: 39.8060\tSigma2 Prior: -547.3973\tRegularization: 0.0028\n",
            "Iter: 5060  \tTraining Loss: -526.0973    \n",
            "    Negative Log Likelihood: 31.2385\tSigma2 Prior: -557.3386\tRegularization: 0.0028\n",
            "Iter: 5070  \tTraining Loss: -514.6703    \n",
            "    Negative Log Likelihood: 34.4992\tSigma2 Prior: -549.1722\tRegularization: 0.0028\n",
            "Iter: 5080  \tTraining Loss: -461.4587    \n",
            "    Negative Log Likelihood: 29.7708\tSigma2 Prior: -491.2323\tRegularization: 0.0028\n",
            "Iter: 5090  \tTraining Loss: -455.8457    \n",
            "    Negative Log Likelihood: 37.9296\tSigma2 Prior: -493.7781\tRegularization: 0.0028\n",
            "Iter: 5100  \tTraining Loss: -493.0787    \n",
            "    Negative Log Likelihood: 36.5577\tSigma2 Prior: -529.6392\tRegularization: 0.0028\n",
            "Iter: 5110  \tTraining Loss: -508.5045    \n",
            "    Negative Log Likelihood: 27.8715\tSigma2 Prior: -536.3787\tRegularization: 0.0028\n",
            "Iter: 5120  \tTraining Loss: -509.2057    \n",
            "    Negative Log Likelihood: 33.3120\tSigma2 Prior: -542.5204\tRegularization: 0.0028\n",
            "Iter: 5130  \tTraining Loss: -466.3642    \n",
            "    Negative Log Likelihood: 33.0567\tSigma2 Prior: -499.4236\tRegularization: 0.0028\n",
            "Iter: 5140  \tTraining Loss: -494.9567    \n",
            "    Negative Log Likelihood: 36.1517\tSigma2 Prior: -531.1111\tRegularization: 0.0028\n",
            "Iter: 5150  \tTraining Loss: -407.3448    \n",
            "    Negative Log Likelihood: 36.4664\tSigma2 Prior: -443.8140\tRegularization: 0.0028\n",
            "Iter: 5160  \tTraining Loss: -508.8259    \n",
            "    Negative Log Likelihood: 27.5315\tSigma2 Prior: -536.3602\tRegularization: 0.0028\n",
            "Iter: 5170  \tTraining Loss: -505.7437    \n",
            "    Negative Log Likelihood: 38.4098\tSigma2 Prior: -544.1563\tRegularization: 0.0028\n",
            "Iter: 5180  \tTraining Loss: -506.4847    \n",
            "    Negative Log Likelihood: 31.1980\tSigma2 Prior: -537.6855\tRegularization: 0.0028\n",
            "Iter: 5190  \tTraining Loss: -476.5341    \n",
            "    Negative Log Likelihood: 42.1034\tSigma2 Prior: -518.6403\tRegularization: 0.0028\n",
            "Iter: 5200  \tTraining Loss: -490.8075    \n",
            "    Negative Log Likelihood: 28.3017\tSigma2 Prior: -519.1119\tRegularization: 0.0028\n",
            "Iter: 5210  \tTraining Loss: -480.2589    \n",
            "    Negative Log Likelihood: 29.0539\tSigma2 Prior: -509.3156\tRegularization: 0.0028\n",
            "Iter: 5220  \tTraining Loss: -481.8817    \n",
            "    Negative Log Likelihood: 36.3069\tSigma2 Prior: -518.1914\tRegularization: 0.0028\n",
            "Iter: 5230  \tTraining Loss: -466.9428    \n",
            "    Negative Log Likelihood: 32.0625\tSigma2 Prior: -499.0081\tRegularization: 0.0028\n",
            "Iter: 5240  \tTraining Loss: -453.5318    \n",
            "    Negative Log Likelihood: 34.2897\tSigma2 Prior: -487.8243\tRegularization: 0.0028\n",
            "Iter: 5250  \tTraining Loss: -466.6938    \n",
            "    Negative Log Likelihood: 30.1198\tSigma2 Prior: -496.8164\tRegularization: 0.0028\n",
            "Iter: 5260  \tTraining Loss: -503.3618    \n",
            "    Negative Log Likelihood: 40.5358\tSigma2 Prior: -543.9005\tRegularization: 0.0028\n",
            "Iter: 5270  \tTraining Loss: -517.8831    \n",
            "    Negative Log Likelihood: 35.1822\tSigma2 Prior: -553.0681\tRegularization: 0.0028\n",
            "Iter: 5280  \tTraining Loss: -473.6534    \n",
            "    Negative Log Likelihood: 39.7204\tSigma2 Prior: -513.3767\tRegularization: 0.0028\n",
            "Iter: 5290  \tTraining Loss: -487.4367    \n",
            "    Negative Log Likelihood: 33.6393\tSigma2 Prior: -521.0789\tRegularization: 0.0028\n",
            "Iter: 5300  \tTraining Loss: -500.8943    \n",
            "    Negative Log Likelihood: 28.8499\tSigma2 Prior: -529.7469\tRegularization: 0.0028\n",
            "Iter: 5310  \tTraining Loss: -472.6846    \n",
            "    Negative Log Likelihood: 40.8557\tSigma2 Prior: -513.5431\tRegularization: 0.0028\n",
            "Iter: 5320  \tTraining Loss: -469.3494    \n",
            "    Negative Log Likelihood: 33.4700\tSigma2 Prior: -502.8222\tRegularization: 0.0028\n",
            "Iter: 5330  \tTraining Loss: -523.3537    \n",
            "    Negative Log Likelihood: 35.0466\tSigma2 Prior: -558.4032\tRegularization: 0.0028\n",
            "Iter: 5340  \tTraining Loss: -479.6907    \n",
            "    Negative Log Likelihood: 34.7830\tSigma2 Prior: -514.4766\tRegularization: 0.0029\n",
            "Iter: 5350  \tTraining Loss: -506.4873    \n",
            "    Negative Log Likelihood: 32.7105\tSigma2 Prior: -539.2006\tRegularization: 0.0029\n",
            "Iter: 5360  \tTraining Loss: -526.1295    \n",
            "    Negative Log Likelihood: 35.3601\tSigma2 Prior: -561.4924\tRegularization: 0.0029\n",
            "Iter: 5370  \tTraining Loss: -502.1698    \n",
            "    Negative Log Likelihood: 30.7961\tSigma2 Prior: -532.9688\tRegularization: 0.0029\n",
            "Iter: 5380  \tTraining Loss: -488.2122    \n",
            "    Negative Log Likelihood: 33.6189\tSigma2 Prior: -521.8340\tRegularization: 0.0029\n",
            "Iter: 5390  \tTraining Loss: -482.2090    \n",
            "    Negative Log Likelihood: 33.1152\tSigma2 Prior: -515.3271\tRegularization: 0.0029\n",
            "Iter: 5400  \tTraining Loss: -469.1084    \n",
            "    Negative Log Likelihood: 34.7344\tSigma2 Prior: -503.8456\tRegularization: 0.0029\n",
            "Iter: 5410  \tTraining Loss: -499.4334    \n",
            "    Negative Log Likelihood: 31.6220\tSigma2 Prior: -531.0583\tRegularization: 0.0029\n",
            "Iter: 5420  \tTraining Loss: -509.9865    \n",
            "    Negative Log Likelihood: 40.6067\tSigma2 Prior: -550.5961\tRegularization: 0.0029\n",
            "Iter: 5430  \tTraining Loss: -513.6237    \n",
            "    Negative Log Likelihood: 29.1037\tSigma2 Prior: -542.7303\tRegularization: 0.0029\n",
            "Iter: 5440  \tTraining Loss: -484.1147    \n",
            "    Negative Log Likelihood: 31.3331\tSigma2 Prior: -515.4507\tRegularization: 0.0029\n",
            "Iter: 5450  \tTraining Loss: -510.2130    \n",
            "    Negative Log Likelihood: 34.9307\tSigma2 Prior: -545.1466\tRegularization: 0.0029\n",
            "Iter: 5460  \tTraining Loss: -476.7753    \n",
            "    Negative Log Likelihood: 30.3208\tSigma2 Prior: -507.0990\tRegularization: 0.0029\n",
            "Iter: 5470  \tTraining Loss: -468.7903    \n",
            "    Negative Log Likelihood: 30.5383\tSigma2 Prior: -499.3315\tRegularization: 0.0029\n",
            "Iter: 5480  \tTraining Loss: -490.8976    \n",
            "    Negative Log Likelihood: 36.2953\tSigma2 Prior: -527.1958\tRegularization: 0.0029\n",
            "Iter: 5490  \tTraining Loss: -486.2734    \n",
            "    Negative Log Likelihood: 35.3778\tSigma2 Prior: -521.6542\tRegularization: 0.0029\n",
            "Iter: 5500  \tTraining Loss: -464.4914    \n",
            "    Negative Log Likelihood: 36.3504\tSigma2 Prior: -500.8447\tRegularization: 0.0029\n",
            "Iter: 5510  \tTraining Loss: -509.8321    \n",
            "    Negative Log Likelihood: 27.5633\tSigma2 Prior: -537.3983\tRegularization: 0.0029\n",
            "Iter: 5520  \tTraining Loss: -460.2094    \n",
            "    Negative Log Likelihood: 34.2864\tSigma2 Prior: -494.4988\tRegularization: 0.0029\n",
            "Iter: 5530  \tTraining Loss: -506.1883    \n",
            "    Negative Log Likelihood: 32.8318\tSigma2 Prior: -539.0230\tRegularization: 0.0029\n",
            "Iter: 5540  \tTraining Loss: -479.4916    \n",
            "    Negative Log Likelihood: 37.5675\tSigma2 Prior: -517.0620\tRegularization: 0.0029\n",
            "Iter: 5550  \tTraining Loss: -508.0759    \n",
            "    Negative Log Likelihood: 31.7206\tSigma2 Prior: -539.7994\tRegularization: 0.0029\n",
            "Iter: 5560  \tTraining Loss: -500.6119    \n",
            "    Negative Log Likelihood: 36.6872\tSigma2 Prior: -537.3020\tRegularization: 0.0029\n",
            "Iter: 5570  \tTraining Loss: -495.3702    \n",
            "    Negative Log Likelihood: 34.5467\tSigma2 Prior: -529.9198\tRegularization: 0.0029\n",
            "Iter: 5580  \tTraining Loss: -487.5743    \n",
            "    Negative Log Likelihood: 32.2559\tSigma2 Prior: -519.8332\tRegularization: 0.0029\n",
            "Iter: 5590  \tTraining Loss: -488.0741    \n",
            "    Negative Log Likelihood: 38.5532\tSigma2 Prior: -526.6302\tRegularization: 0.0029\n",
            "Iter: 5600  \tTraining Loss: -490.5505    \n",
            "    Negative Log Likelihood: 29.6750\tSigma2 Prior: -520.2285\tRegularization: 0.0029\n",
            "Iter: 5610  \tTraining Loss: -505.4711    \n",
            "    Negative Log Likelihood: 32.9789\tSigma2 Prior: -538.4530\tRegularization: 0.0029\n",
            "Iter: 5620  \tTraining Loss: -474.5354    \n",
            "    Negative Log Likelihood: 37.9122\tSigma2 Prior: -512.4506\tRegularization: 0.0029\n",
            "Iter: 5630  \tTraining Loss: -478.3146    \n",
            "    Negative Log Likelihood: 34.5352\tSigma2 Prior: -512.8527\tRegularization: 0.0029\n",
            "Iter: 5640  \tTraining Loss: -478.2978    \n",
            "    Negative Log Likelihood: 38.1406\tSigma2 Prior: -516.4413\tRegularization: 0.0029\n",
            "Iter: 5650  \tTraining Loss: -509.7421    \n",
            "    Negative Log Likelihood: 30.8049\tSigma2 Prior: -540.5499\tRegularization: 0.0029\n",
            "Iter: 5660  \tTraining Loss: -486.8102    \n",
            "    Negative Log Likelihood: 35.7846\tSigma2 Prior: -522.5977\tRegularization: 0.0030\n",
            "Iter: 5670  \tTraining Loss: -499.5013    \n",
            "    Negative Log Likelihood: 31.5896\tSigma2 Prior: -531.0939\tRegularization: 0.0030\n",
            "Iter: 5680  \tTraining Loss: -498.9039    \n",
            "    Negative Log Likelihood: 35.5804\tSigma2 Prior: -534.4872\tRegularization: 0.0030\n",
            "Iter: 5690  \tTraining Loss: -508.3438    \n",
            "    Negative Log Likelihood: 33.9425\tSigma2 Prior: -542.2892\tRegularization: 0.0030\n",
            "Iter: 5700  \tTraining Loss: -513.9321    \n",
            "    Negative Log Likelihood: 38.4256\tSigma2 Prior: -552.3607\tRegularization: 0.0030\n",
            "Iter: 5710  \tTraining Loss: -469.8040    \n",
            "    Negative Log Likelihood: 34.5950\tSigma2 Prior: -504.4020\tRegularization: 0.0030\n",
            "Iter: 5720  \tTraining Loss: -515.4579    \n",
            "    Negative Log Likelihood: 29.5630\tSigma2 Prior: -545.0239\tRegularization: 0.0030\n",
            "Iter: 5730  \tTraining Loss: -450.7371    \n",
            "    Negative Log Likelihood: 34.3725\tSigma2 Prior: -485.1125\tRegularization: 0.0030\n",
            "Iter: 5740  \tTraining Loss: -497.2381    \n",
            "    Negative Log Likelihood: 35.2212\tSigma2 Prior: -532.4623\tRegularization: 0.0030\n",
            "Iter: 5750  \tTraining Loss: -475.2397    \n",
            "    Negative Log Likelihood: 29.5213\tSigma2 Prior: -504.7640\tRegularization: 0.0030\n",
            "Iter: 5760  \tTraining Loss: -510.4203    \n",
            "    Negative Log Likelihood: 34.8102\tSigma2 Prior: -545.2335\tRegularization: 0.0030\n",
            "Iter: 5770  \tTraining Loss: -490.8814    \n",
            "    Negative Log Likelihood: 39.2143\tSigma2 Prior: -530.0986\tRegularization: 0.0030\n",
            "Iter: 5780  \tTraining Loss: -489.6713    \n",
            "    Negative Log Likelihood: 34.9193\tSigma2 Prior: -524.5936\tRegularization: 0.0030\n",
            "Iter: 5790  \tTraining Loss: -470.2176    \n",
            "    Negative Log Likelihood: 37.8558\tSigma2 Prior: -508.0763\tRegularization: 0.0030\n",
            "Iter: 5800  \tTraining Loss: -504.2868    \n",
            "    Negative Log Likelihood: 35.5757\tSigma2 Prior: -539.8655\tRegularization: 0.0030\n",
            "Iter: 5810  \tTraining Loss: -484.1443    \n",
            "    Negative Log Likelihood: 33.1130\tSigma2 Prior: -517.2603\tRegularization: 0.0030\n",
            "Iter: 5820  \tTraining Loss: -511.0292    \n",
            "    Negative Log Likelihood: 32.8490\tSigma2 Prior: -543.8812\tRegularization: 0.0030\n",
            "Iter: 5830  \tTraining Loss: -473.2283    \n",
            "    Negative Log Likelihood: 35.1173\tSigma2 Prior: -508.3485\tRegularization: 0.0030\n",
            "Iter: 5840  \tTraining Loss: -500.9634    \n",
            "    Negative Log Likelihood: 31.1722\tSigma2 Prior: -532.1386\tRegularization: 0.0030\n",
            "Iter: 5850  \tTraining Loss: -476.9483    \n",
            "    Negative Log Likelihood: 32.9467\tSigma2 Prior: -509.8980\tRegularization: 0.0030\n",
            "Iter: 5860  \tTraining Loss: -496.2427    \n",
            "    Negative Log Likelihood: 36.1216\tSigma2 Prior: -532.3674\tRegularization: 0.0030\n",
            "Iter: 5870  \tTraining Loss: -508.4711    \n",
            "    Negative Log Likelihood: 34.1384\tSigma2 Prior: -542.6125\tRegularization: 0.0030\n",
            "Iter: 5880  \tTraining Loss: -514.1040    \n",
            "    Negative Log Likelihood: 31.8926\tSigma2 Prior: -545.9996\tRegularization: 0.0030\n",
            "Iter: 5890  \tTraining Loss: -439.2520    \n",
            "    Negative Log Likelihood: 37.1934\tSigma2 Prior: -476.4483\tRegularization: 0.0030\n",
            "Iter: 5900  \tTraining Loss: -497.7535    \n",
            "    Negative Log Likelihood: 33.2323\tSigma2 Prior: -530.9889\tRegularization: 0.0030\n",
            "Iter: 5910  \tTraining Loss: -513.4313    \n",
            "    Negative Log Likelihood: 31.7281\tSigma2 Prior: -545.1625\tRegularization: 0.0030\n",
            "Iter: 5920  \tTraining Loss: -513.8577    \n",
            "    Negative Log Likelihood: 30.1156\tSigma2 Prior: -543.9763\tRegularization: 0.0030\n",
            "Iter: 5930  \tTraining Loss: -507.4304    \n",
            "    Negative Log Likelihood: 35.5232\tSigma2 Prior: -542.9567\tRegularization: 0.0030\n",
            "Iter: 5940  \tTraining Loss: -505.0740    \n",
            "    Negative Log Likelihood: 30.5131\tSigma2 Prior: -535.5901\tRegularization: 0.0030\n",
            "Iter: 5950  \tTraining Loss: -524.9150    \n",
            "    Negative Log Likelihood: 31.3678\tSigma2 Prior: -556.2859\tRegularization: 0.0030\n",
            "Iter: 5960  \tTraining Loss: -495.3869    \n",
            "    Negative Log Likelihood: 35.0463\tSigma2 Prior: -530.4362\tRegularization: 0.0030\n",
            "Iter: 5970  \tTraining Loss: -482.8997    \n",
            "    Negative Log Likelihood: 35.4227\tSigma2 Prior: -518.3254\tRegularization: 0.0031\n",
            "Iter: 5980  \tTraining Loss: -511.4655    \n",
            "    Negative Log Likelihood: 33.2781\tSigma2 Prior: -544.7466\tRegularization: 0.0031\n",
            "Iter: 5990  \tTraining Loss: -510.7480    \n",
            "    Negative Log Likelihood: 30.3331\tSigma2 Prior: -541.0842\tRegularization: 0.0031\n",
            "Iter: 6000  \tTraining Loss: -501.2531    \n",
            "    Negative Log Likelihood: 33.5940\tSigma2 Prior: -534.8502\tRegularization: 0.0031\n",
            "Iter: 6010  \tTraining Loss: -474.9503    \n",
            "    Negative Log Likelihood: 37.3618\tSigma2 Prior: -512.3151\tRegularization: 0.0031\n",
            "Iter: 6020  \tTraining Loss: -498.9139    \n",
            "    Negative Log Likelihood: 31.1549\tSigma2 Prior: -530.0718\tRegularization: 0.0031\n",
            "Iter: 6030  \tTraining Loss: -460.1534    \n",
            "    Negative Log Likelihood: 32.5474\tSigma2 Prior: -492.7039\tRegularization: 0.0031\n",
            "Iter: 6040  \tTraining Loss: -497.0037    \n",
            "    Negative Log Likelihood: 35.8509\tSigma2 Prior: -532.8577\tRegularization: 0.0031\n",
            "Iter: 6050  \tTraining Loss: -485.5364    \n",
            "    Negative Log Likelihood: 41.5807\tSigma2 Prior: -527.1202\tRegularization: 0.0031\n",
            "Iter: 6060  \tTraining Loss: -465.0171    \n",
            "    Negative Log Likelihood: 31.7007\tSigma2 Prior: -496.7209\tRegularization: 0.0031\n",
            "Iter: 6070  \tTraining Loss: -452.7680    \n",
            "    Negative Log Likelihood: 33.9734\tSigma2 Prior: -486.7445\tRegularization: 0.0031\n",
            "Iter: 6080  \tTraining Loss: -486.5392    \n",
            "    Negative Log Likelihood: 34.8862\tSigma2 Prior: -521.4285\tRegularization: 0.0031\n",
            "Iter: 6090  \tTraining Loss: -512.4727    \n",
            "    Negative Log Likelihood: 26.0328\tSigma2 Prior: -538.5086\tRegularization: 0.0031\n",
            "Iter: 6100  \tTraining Loss: -468.8316    \n",
            "    Negative Log Likelihood: 38.5347\tSigma2 Prior: -507.3694\tRegularization: 0.0031\n",
            "Iter: 6110  \tTraining Loss: -422.2942    \n",
            "    Negative Log Likelihood: 31.8459\tSigma2 Prior: -454.1432\tRegularization: 0.0031\n",
            "Iter: 6120  \tTraining Loss: -507.3275    \n",
            "    Negative Log Likelihood: 31.0073\tSigma2 Prior: -538.3379\tRegularization: 0.0031\n",
            "Iter: 6130  \tTraining Loss: -480.3421    \n",
            "    Negative Log Likelihood: 38.4928\tSigma2 Prior: -518.8380\tRegularization: 0.0031\n",
            "Iter: 6140  \tTraining Loss: -482.5560    \n",
            "    Negative Log Likelihood: 30.8461\tSigma2 Prior: -513.4052\tRegularization: 0.0031\n",
            "Iter: 6150  \tTraining Loss: -476.6379    \n",
            "    Negative Log Likelihood: 34.9460\tSigma2 Prior: -511.5870\tRegularization: 0.0031\n",
            "Iter: 6160  \tTraining Loss: -492.4035    \n",
            "    Negative Log Likelihood: 30.6599\tSigma2 Prior: -523.0665\tRegularization: 0.0031\n",
            "Iter: 6170  \tTraining Loss: -517.6051    \n",
            "    Negative Log Likelihood: 33.5864\tSigma2 Prior: -551.1946\tRegularization: 0.0031\n",
            "Iter: 6180  \tTraining Loss: -505.4580    \n",
            "    Negative Log Likelihood: 34.1217\tSigma2 Prior: -539.5828\tRegularization: 0.0031\n",
            "Iter: 6190  \tTraining Loss: -512.2586    \n",
            "    Negative Log Likelihood: 30.9542\tSigma2 Prior: -543.2159\tRegularization: 0.0031\n",
            "Iter: 6200  \tTraining Loss: -474.6030    \n",
            "    Negative Log Likelihood: 34.6051\tSigma2 Prior: -509.2112\tRegularization: 0.0031\n",
            "Iter: 6210  \tTraining Loss: -492.3742    \n",
            "    Negative Log Likelihood: 34.7884\tSigma2 Prior: -527.1657\tRegularization: 0.0031\n",
            "Iter: 6220  \tTraining Loss: -475.3672    \n",
            "    Negative Log Likelihood: 34.5795\tSigma2 Prior: -509.9498\tRegularization: 0.0031\n",
            "Iter: 6230  \tTraining Loss: -501.1405    \n",
            "    Negative Log Likelihood: 32.6253\tSigma2 Prior: -533.7690\tRegularization: 0.0031\n",
            "Iter: 6240  \tTraining Loss: -492.7854    \n",
            "    Negative Log Likelihood: 31.6442\tSigma2 Prior: -524.4327\tRegularization: 0.0031\n",
            "Iter: 6250  \tTraining Loss: -483.3318    \n",
            "    Negative Log Likelihood: 34.7257\tSigma2 Prior: -518.0607\tRegularization: 0.0031\n",
            "Iter: 6260  \tTraining Loss: -484.6604    \n",
            "    Negative Log Likelihood: 36.3777\tSigma2 Prior: -521.0413\tRegularization: 0.0031\n",
            "Iter: 6270  \tTraining Loss: -495.9371    \n",
            "    Negative Log Likelihood: 30.3532\tSigma2 Prior: -526.2935\tRegularization: 0.0031\n",
            "Iter: 6280  \tTraining Loss: -499.5269    \n",
            "    Negative Log Likelihood: 33.9168\tSigma2 Prior: -533.4469\tRegularization: 0.0031\n",
            "Iter: 6290  \tTraining Loss: -514.4506    \n",
            "    Negative Log Likelihood: 37.2855\tSigma2 Prior: -551.7393\tRegularization: 0.0031\n",
            "Iter: 6300  \tTraining Loss: -490.3295    \n",
            "    Negative Log Likelihood: 31.2212\tSigma2 Prior: -521.5538\tRegularization: 0.0032\n",
            "Iter: 6310  \tTraining Loss: -495.0278    \n",
            "    Negative Log Likelihood: 33.5665\tSigma2 Prior: -528.5974\tRegularization: 0.0032\n",
            "Iter: 6320  \tTraining Loss: -479.5221    \n",
            "    Negative Log Likelihood: 34.0093\tSigma2 Prior: -513.5345\tRegularization: 0.0032\n",
            "Iter: 6330  \tTraining Loss: -495.9214    \n",
            "    Negative Log Likelihood: 32.8926\tSigma2 Prior: -528.8172\tRegularization: 0.0032\n",
            "Iter: 6340  \tTraining Loss: -506.9283    \n",
            "    Negative Log Likelihood: 39.5502\tSigma2 Prior: -546.4817\tRegularization: 0.0032\n",
            "Iter: 6350  \tTraining Loss: -466.0343    \n",
            "    Negative Log Likelihood: 33.8187\tSigma2 Prior: -499.8561\tRegularization: 0.0032\n",
            "Iter: 6360  \tTraining Loss: -481.4688    \n",
            "    Negative Log Likelihood: 27.9084\tSigma2 Prior: -509.3804\tRegularization: 0.0032\n",
            "Iter: 6370  \tTraining Loss: -500.8828    \n",
            "    Negative Log Likelihood: 36.7822\tSigma2 Prior: -537.6681\tRegularization: 0.0032\n",
            "Iter: 6380  \tTraining Loss: -502.5865    \n",
            "    Negative Log Likelihood: 37.7504\tSigma2 Prior: -540.3400\tRegularization: 0.0032\n",
            "Iter: 6390  \tTraining Loss: -496.8417    \n",
            "    Negative Log Likelihood: 27.5934\tSigma2 Prior: -524.4383\tRegularization: 0.0032\n",
            "Iter: 6400  \tTraining Loss: -506.9857    \n",
            "    Negative Log Likelihood: 35.3826\tSigma2 Prior: -542.3715\tRegularization: 0.0032\n",
            "Iter: 6410  \tTraining Loss: -454.3002    \n",
            "    Negative Log Likelihood: 32.9761\tSigma2 Prior: -487.2795\tRegularization: 0.0032\n",
            "Iter: 6420  \tTraining Loss: -456.5913    \n",
            "    Negative Log Likelihood: 30.4858\tSigma2 Prior: -487.0803\tRegularization: 0.0032\n",
            "Iter: 6430  \tTraining Loss: -499.4807    \n",
            "    Negative Log Likelihood: 37.1699\tSigma2 Prior: -536.6538\tRegularization: 0.0032\n",
            "Iter: 6440  \tTraining Loss: -474.3436    \n",
            "    Negative Log Likelihood: 34.0502\tSigma2 Prior: -508.3970\tRegularization: 0.0032\n",
            "Iter: 6450  \tTraining Loss: -518.4277    \n",
            "    Negative Log Likelihood: 33.4501\tSigma2 Prior: -551.8810\tRegularization: 0.0032\n",
            "Iter: 6460  \tTraining Loss: -510.7349    \n",
            "    Negative Log Likelihood: 33.1806\tSigma2 Prior: -543.9187\tRegularization: 0.0032\n",
            "Iter: 6470  \tTraining Loss: -451.8445    \n",
            "    Negative Log Likelihood: 37.3409\tSigma2 Prior: -489.1886\tRegularization: 0.0032\n",
            "Iter: 6480  \tTraining Loss: -480.9228    \n",
            "    Negative Log Likelihood: 31.2364\tSigma2 Prior: -512.1624\tRegularization: 0.0032\n",
            "Iter: 6490  \tTraining Loss: -492.8135    \n",
            "    Negative Log Likelihood: 30.5684\tSigma2 Prior: -523.3852\tRegularization: 0.0032\n",
            "Iter: 6500  \tTraining Loss: -509.8106    \n",
            "    Negative Log Likelihood: 32.5349\tSigma2 Prior: -542.3488\tRegularization: 0.0032\n",
            "Iter: 6510  \tTraining Loss: -467.3954    \n",
            "    Negative Log Likelihood: 31.3151\tSigma2 Prior: -498.7137\tRegularization: 0.0032\n",
            "Iter: 6520  \tTraining Loss: -482.0867    \n",
            "    Negative Log Likelihood: 31.9559\tSigma2 Prior: -514.0458\tRegularization: 0.0032\n",
            "Iter: 6530  \tTraining Loss: -517.0608    \n",
            "    Negative Log Likelihood: 35.0495\tSigma2 Prior: -552.1135\tRegularization: 0.0032\n",
            "Iter: 6540  \tTraining Loss: -511.8792    \n",
            "    Negative Log Likelihood: 31.3247\tSigma2 Prior: -543.2072\tRegularization: 0.0032\n",
            "Iter: 6550  \tTraining Loss: -508.0624    \n",
            "    Negative Log Likelihood: 29.0438\tSigma2 Prior: -537.1094\tRegularization: 0.0032\n",
            "Iter: 6560  \tTraining Loss: -495.9062    \n",
            "    Negative Log Likelihood: 39.2232\tSigma2 Prior: -535.1326\tRegularization: 0.0032\n",
            "Iter: 6570  \tTraining Loss: -485.0974    \n",
            "    Negative Log Likelihood: 28.3973\tSigma2 Prior: -513.4979\tRegularization: 0.0032\n",
            "Iter: 6580  \tTraining Loss: -496.6685    \n",
            "    Negative Log Likelihood: 32.1367\tSigma2 Prior: -528.8084\tRegularization: 0.0032\n",
            "Iter: 6590  \tTraining Loss: -529.4183    \n",
            "    Negative Log Likelihood: 37.4897\tSigma2 Prior: -566.9113\tRegularization: 0.0032\n",
            "Iter: 6600  \tTraining Loss: -444.1713    \n",
            "    Negative Log Likelihood: 37.8199\tSigma2 Prior: -481.9944\tRegularization: 0.0032\n",
            "Iter: 6610  \tTraining Loss: -498.2771    \n",
            "    Negative Log Likelihood: 32.4442\tSigma2 Prior: -530.7245\tRegularization: 0.0032\n",
            "Iter: 6620  \tTraining Loss: -483.3517    \n",
            "    Negative Log Likelihood: 45.2214\tSigma2 Prior: -528.5763\tRegularization: 0.0033\n",
            "Iter: 6630  \tTraining Loss: -509.2267    \n",
            "    Negative Log Likelihood: 32.5289\tSigma2 Prior: -541.7589\tRegularization: 0.0033\n",
            "Iter: 6640  \tTraining Loss: -523.4807    \n",
            "    Negative Log Likelihood: 30.1340\tSigma2 Prior: -553.6179\tRegularization: 0.0033\n",
            "Iter: 6650  \tTraining Loss: -519.0227    \n",
            "    Negative Log Likelihood: 39.2444\tSigma2 Prior: -558.2704\tRegularization: 0.0033\n",
            "Iter: 6660  \tTraining Loss: -478.3647    \n",
            "    Negative Log Likelihood: 31.1081\tSigma2 Prior: -509.4760\tRegularization: 0.0033\n",
            "Iter: 6670  \tTraining Loss: -471.1301    \n",
            "    Negative Log Likelihood: 31.4250\tSigma2 Prior: -502.5584\tRegularization: 0.0033\n",
            "Iter: 6680  \tTraining Loss: -470.7545    \n",
            "    Negative Log Likelihood: 41.8197\tSigma2 Prior: -512.5774\tRegularization: 0.0033\n",
            "Iter: 6690  \tTraining Loss: -514.2733    \n",
            "    Negative Log Likelihood: 28.0835\tSigma2 Prior: -542.3600\tRegularization: 0.0033\n",
            "Iter: 6700  \tTraining Loss: -514.7515    \n",
            "    Negative Log Likelihood: 34.8845\tSigma2 Prior: -549.6393\tRegularization: 0.0033\n",
            "Iter: 6710  \tTraining Loss: -477.2451    \n",
            "    Negative Log Likelihood: 37.4880\tSigma2 Prior: -514.7363\tRegularization: 0.0033\n",
            "Iter: 6720  \tTraining Loss: -508.8023    \n",
            "    Negative Log Likelihood: 32.8099\tSigma2 Prior: -541.6155\tRegularization: 0.0033\n",
            "Iter: 6730  \tTraining Loss: -513.1293    \n",
            "    Negative Log Likelihood: 36.1915\tSigma2 Prior: -549.3242\tRegularization: 0.0033\n",
            "Iter: 6740  \tTraining Loss: -534.3116    \n",
            "    Negative Log Likelihood: 34.7530\tSigma2 Prior: -569.0679\tRegularization: 0.0033\n",
            "Iter: 6750  \tTraining Loss: -497.8295    \n",
            "    Negative Log Likelihood: 31.5211\tSigma2 Prior: -529.3539\tRegularization: 0.0033\n",
            "Iter: 6760  \tTraining Loss: -505.4389    \n",
            "    Negative Log Likelihood: 28.8996\tSigma2 Prior: -534.3418\tRegularization: 0.0033\n",
            "Iter: 6770  \tTraining Loss: -470.6109    \n",
            "    Negative Log Likelihood: 38.9992\tSigma2 Prior: -509.6134\tRegularization: 0.0033\n",
            "Iter: 6780  \tTraining Loss: -466.0222    \n",
            "    Negative Log Likelihood: 40.3011\tSigma2 Prior: -506.3266\tRegularization: 0.0033\n",
            "Iter: 6790  \tTraining Loss: -508.3540    \n",
            "    Negative Log Likelihood: 33.1125\tSigma2 Prior: -541.4698\tRegularization: 0.0033\n",
            "Iter: 6800  \tTraining Loss: -496.9254    \n",
            "    Negative Log Likelihood: 39.8632\tSigma2 Prior: -536.7919\tRegularization: 0.0033\n",
            "Iter: 6810  \tTraining Loss: -519.0480    \n",
            "    Negative Log Likelihood: 32.2346\tSigma2 Prior: -551.2859\tRegularization: 0.0033\n",
            "Iter: 6820  \tTraining Loss: -494.0027    \n",
            "    Negative Log Likelihood: 30.3610\tSigma2 Prior: -524.3670\tRegularization: 0.0033\n",
            "Iter: 6830  \tTraining Loss: -501.1125    \n",
            "    Negative Log Likelihood: 38.5865\tSigma2 Prior: -539.7024\tRegularization: 0.0033\n",
            "Iter: 6840  \tTraining Loss: -488.7339    \n",
            "    Negative Log Likelihood: 31.9188\tSigma2 Prior: -520.6560\tRegularization: 0.0033\n",
            "Iter: 6850  \tTraining Loss: -481.5816    \n",
            "    Negative Log Likelihood: 33.6553\tSigma2 Prior: -515.2402\tRegularization: 0.0033\n",
            "Iter: 6860  \tTraining Loss: -522.4315    \n",
            "    Negative Log Likelihood: 33.1894\tSigma2 Prior: -555.6242\tRegularization: 0.0033\n",
            "Iter: 6870  \tTraining Loss: -512.3762    \n",
            "    Negative Log Likelihood: 34.0496\tSigma2 Prior: -546.4291\tRegularization: 0.0033\n",
            "Iter: 6880  \tTraining Loss: -416.5882    \n",
            "    Negative Log Likelihood: 33.6579\tSigma2 Prior: -450.2495\tRegularization: 0.0033\n",
            "Iter: 6890  \tTraining Loss: -470.3874    \n",
            "    Negative Log Likelihood: 34.8599\tSigma2 Prior: -505.2507\tRegularization: 0.0033\n",
            "Iter: 6900  \tTraining Loss: -504.9598    \n",
            "    Negative Log Likelihood: 31.0730\tSigma2 Prior: -536.0362\tRegularization: 0.0033\n",
            "Iter: 6910  \tTraining Loss: -510.0142    \n",
            "    Negative Log Likelihood: 36.5107\tSigma2 Prior: -546.5282\tRegularization: 0.0033\n",
            "Iter: 6920  \tTraining Loss: -494.5027    \n",
            "    Negative Log Likelihood: 31.3756\tSigma2 Prior: -525.8816\tRegularization: 0.0033\n",
            "Iter: 6930  \tTraining Loss: -488.8893    \n",
            "    Negative Log Likelihood: 32.0308\tSigma2 Prior: -520.9234\tRegularization: 0.0033\n",
            "Iter: 6940  \tTraining Loss: -508.0313    \n",
            "    Negative Log Likelihood: 36.4437\tSigma2 Prior: -544.4783\tRegularization: 0.0033\n",
            "Iter: 6950  \tTraining Loss: -505.4611    \n",
            "    Negative Log Likelihood: 33.1026\tSigma2 Prior: -538.5671\tRegularization: 0.0034\n",
            "Iter: 6960  \tTraining Loss: -487.4402    \n",
            "    Negative Log Likelihood: 34.5002\tSigma2 Prior: -521.9438\tRegularization: 0.0034\n",
            "Iter: 6970  \tTraining Loss: -499.1023    \n",
            "    Negative Log Likelihood: 31.4936\tSigma2 Prior: -530.5993\tRegularization: 0.0034\n",
            "Iter: 6980  \tTraining Loss: -473.5179    \n",
            "    Negative Log Likelihood: 39.9833\tSigma2 Prior: -513.5045\tRegularization: 0.0034\n",
            "Iter: 6990  \tTraining Loss: -478.2455    \n",
            "    Negative Log Likelihood: 35.4892\tSigma2 Prior: -513.7380\tRegularization: 0.0034\n",
            "Iter: 7000  \tTraining Loss: -513.1299    \n",
            "    Negative Log Likelihood: 34.4792\tSigma2 Prior: -547.6125\tRegularization: 0.0034\n",
            "Iter: 7010  \tTraining Loss: -435.5454    \n",
            "    Negative Log Likelihood: 34.4146\tSigma2 Prior: -469.9633\tRegularization: 0.0034\n",
            "Iter: 7020  \tTraining Loss: -487.2643    \n",
            "    Negative Log Likelihood: 27.5971\tSigma2 Prior: -514.8647\tRegularization: 0.0034\n",
            "Iter: 7030  \tTraining Loss: -490.4152    \n",
            "    Negative Log Likelihood: 35.0791\tSigma2 Prior: -525.4977\tRegularization: 0.0034\n",
            "Iter: 7040  \tTraining Loss: -495.5667    \n",
            "    Negative Log Likelihood: 35.4412\tSigma2 Prior: -531.0114\tRegularization: 0.0034\n",
            "Iter: 7050  \tTraining Loss: -509.9708    \n",
            "    Negative Log Likelihood: 34.0008\tSigma2 Prior: -543.9750\tRegularization: 0.0034\n",
            "Iter: 7060  \tTraining Loss: -461.1857    \n",
            "    Negative Log Likelihood: 37.1096\tSigma2 Prior: -498.2987\tRegularization: 0.0034\n",
            "Iter: 7070  \tTraining Loss: -505.0333    \n",
            "    Negative Log Likelihood: 28.7105\tSigma2 Prior: -533.7471\tRegularization: 0.0034\n",
            "Iter: 7080  \tTraining Loss: -479.7123    \n",
            "    Negative Log Likelihood: 35.8027\tSigma2 Prior: -515.5184\tRegularization: 0.0034\n",
            "Iter: 7090  \tTraining Loss: -484.8376    \n",
            "    Negative Log Likelihood: 30.8393\tSigma2 Prior: -515.6803\tRegularization: 0.0034\n",
            "Iter: 7100  \tTraining Loss: -499.6513    \n",
            "    Negative Log Likelihood: 33.4074\tSigma2 Prior: -533.0621\tRegularization: 0.0034\n",
            "Iter: 7110  \tTraining Loss: -477.4401    \n",
            "    Negative Log Likelihood: 34.7331\tSigma2 Prior: -512.1766\tRegularization: 0.0034\n",
            "Iter: 7120  \tTraining Loss: -515.0707    \n",
            "    Negative Log Likelihood: 32.9325\tSigma2 Prior: -548.0066\tRegularization: 0.0034\n",
            "Iter: 7130  \tTraining Loss: -478.8371    \n",
            "    Negative Log Likelihood: 34.3402\tSigma2 Prior: -513.1807\tRegularization: 0.0034\n",
            "Iter: 7140  \tTraining Loss: -477.3083    \n",
            "    Negative Log Likelihood: 34.9164\tSigma2 Prior: -512.2281\tRegularization: 0.0034\n",
            "Iter: 7150  \tTraining Loss: -517.0739    \n",
            "    Negative Log Likelihood: 32.5974\tSigma2 Prior: -549.6746\tRegularization: 0.0034\n",
            "Iter: 7160  \tTraining Loss: -519.4690    \n",
            "    Negative Log Likelihood: 38.3347\tSigma2 Prior: -557.8071\tRegularization: 0.0034\n",
            "Iter: 7170  \tTraining Loss: -496.3773    \n",
            "    Negative Log Likelihood: 27.9306\tSigma2 Prior: -524.3113\tRegularization: 0.0034\n",
            "Iter: 7180  \tTraining Loss: -505.0411    \n",
            "    Negative Log Likelihood: 33.6356\tSigma2 Prior: -538.6801\tRegularization: 0.0034\n",
            "Iter: 7190  \tTraining Loss: -497.6656    \n",
            "    Negative Log Likelihood: 32.3285\tSigma2 Prior: -529.9976\tRegularization: 0.0034\n",
            "Iter: 7200  \tTraining Loss: -516.3782    \n",
            "    Negative Log Likelihood: 38.1458\tSigma2 Prior: -554.5273\tRegularization: 0.0034\n",
            "Iter: 7210  \tTraining Loss: -511.4260    \n",
            "    Negative Log Likelihood: 28.6093\tSigma2 Prior: -540.0388\tRegularization: 0.0034\n",
            "Iter: 7220  \tTraining Loss: -514.9840    \n",
            "    Negative Log Likelihood: 36.1611\tSigma2 Prior: -551.1485\tRegularization: 0.0034\n",
            "Iter: 7230  \tTraining Loss: -502.7341    \n",
            "    Negative Log Likelihood: 31.1798\tSigma2 Prior: -533.9174\tRegularization: 0.0034\n",
            "Iter: 7240  \tTraining Loss: -485.5875    \n",
            "    Negative Log Likelihood: 29.6104\tSigma2 Prior: -515.2014\tRegularization: 0.0034\n",
            "Iter: 7250  \tTraining Loss: -479.9828    \n",
            "    Negative Log Likelihood: 35.1024\tSigma2 Prior: -515.0887\tRegularization: 0.0034\n",
            "Iter: 7260  \tTraining Loss: -516.2770    \n",
            "    Negative Log Likelihood: 33.3662\tSigma2 Prior: -549.6467\tRegularization: 0.0034\n",
            "Iter: 7270  \tTraining Loss: -494.5780    \n",
            "    Negative Log Likelihood: 38.5781\tSigma2 Prior: -533.1595\tRegularization: 0.0034\n",
            "Iter: 7280  \tTraining Loss: -503.8694    \n",
            "    Negative Log Likelihood: 28.3286\tSigma2 Prior: -532.2015\tRegularization: 0.0035\n",
            "Iter: 7290  \tTraining Loss: -487.8636    \n",
            "    Negative Log Likelihood: 31.8405\tSigma2 Prior: -519.7075\tRegularization: 0.0035\n",
            "Iter: 7300  \tTraining Loss: -510.5441    \n",
            "    Negative Log Likelihood: 34.3970\tSigma2 Prior: -544.9446\tRegularization: 0.0035\n",
            "Iter: 7310  \tTraining Loss: -460.0641    \n",
            "    Negative Log Likelihood: 40.4110\tSigma2 Prior: -500.4785\tRegularization: 0.0035\n",
            "Iter: 7320  \tTraining Loss: -499.2207    \n",
            "    Negative Log Likelihood: 32.6358\tSigma2 Prior: -531.8600\tRegularization: 0.0035\n",
            "Iter: 7330  \tTraining Loss: -480.8848    \n",
            "    Negative Log Likelihood: 34.2181\tSigma2 Prior: -515.1063\tRegularization: 0.0035\n",
            "Iter: 7340  \tTraining Loss: -505.9834    \n",
            "    Negative Log Likelihood: 35.0600\tSigma2 Prior: -541.0469\tRegularization: 0.0035\n",
            "Iter: 7350  \tTraining Loss: -497.1573    \n",
            "    Negative Log Likelihood: 32.4062\tSigma2 Prior: -529.5670\tRegularization: 0.0035\n",
            "Iter: 7360  \tTraining Loss: -506.2602    \n",
            "    Negative Log Likelihood: 30.7052\tSigma2 Prior: -536.9689\tRegularization: 0.0035\n",
            "Iter: 7370  \tTraining Loss: -511.6129    \n",
            "    Negative Log Likelihood: 34.2513\tSigma2 Prior: -545.8677\tRegularization: 0.0035\n",
            "Iter: 7380  \tTraining Loss: -485.4946    \n",
            "    Negative Log Likelihood: 33.0514\tSigma2 Prior: -518.5495\tRegularization: 0.0035\n",
            "Iter: 7390  \tTraining Loss: -520.0693    \n",
            "    Negative Log Likelihood: 33.5256\tSigma2 Prior: -553.5984\tRegularization: 0.0035\n",
            "Iter: 7400  \tTraining Loss: -507.8252    \n",
            "    Negative Log Likelihood: 32.1173\tSigma2 Prior: -539.9460\tRegularization: 0.0035\n",
            "Iter: 7410  \tTraining Loss: -486.8994    \n",
            "    Negative Log Likelihood: 32.8558\tSigma2 Prior: -519.7587\tRegularization: 0.0035\n",
            "Iter: 7420  \tTraining Loss: -499.2103    \n",
            "    Negative Log Likelihood: 34.3874\tSigma2 Prior: -533.6013\tRegularization: 0.0035\n",
            "Iter: 7430  \tTraining Loss: -488.7289    \n",
            "    Negative Log Likelihood: 32.3613\tSigma2 Prior: -521.0937\tRegularization: 0.0035\n",
            "Iter: 7440  \tTraining Loss: -487.4101    \n",
            "    Negative Log Likelihood: 34.6172\tSigma2 Prior: -522.0308\tRegularization: 0.0035\n",
            "Iter: 7450  \tTraining Loss: -499.5878    \n",
            "    Negative Log Likelihood: 33.1461\tSigma2 Prior: -532.7374\tRegularization: 0.0035\n",
            "Iter: 7460  \tTraining Loss: -505.2349    \n",
            "    Negative Log Likelihood: 32.0607\tSigma2 Prior: -537.2991\tRegularization: 0.0035\n",
            "Iter: 7470  \tTraining Loss: -470.9171    \n",
            "    Negative Log Likelihood: 33.8160\tSigma2 Prior: -504.7366\tRegularization: 0.0035\n",
            "Iter: 7480  \tTraining Loss: -492.4402    \n",
            "    Negative Log Likelihood: 30.3990\tSigma2 Prior: -522.8427\tRegularization: 0.0035\n",
            "Iter: 7490  \tTraining Loss: -523.8120    \n",
            "    Negative Log Likelihood: 34.7739\tSigma2 Prior: -558.5894\tRegularization: 0.0035\n",
            "Iter: 7500  \tTraining Loss: -514.8044    \n",
            "    Negative Log Likelihood: 29.2377\tSigma2 Prior: -544.0457\tRegularization: 0.0035\n",
            "Iter: 7510  \tTraining Loss: -514.0137    \n",
            "    Negative Log Likelihood: 34.5123\tSigma2 Prior: -548.5296\tRegularization: 0.0035\n",
            "Iter: 7520  \tTraining Loss: -482.8426    \n",
            "    Negative Log Likelihood: 32.1661\tSigma2 Prior: -515.0122\tRegularization: 0.0035\n",
            "Iter: 7530  \tTraining Loss: -501.1561    \n",
            "    Negative Log Likelihood: 34.1050\tSigma2 Prior: -535.2646\tRegularization: 0.0035\n",
            "Iter: 7540  \tTraining Loss: -439.7657    \n",
            "    Negative Log Likelihood: 38.1922\tSigma2 Prior: -477.9614\tRegularization: 0.0035\n",
            "Iter: 7550  \tTraining Loss: -507.4546    \n",
            "    Negative Log Likelihood: 28.0348\tSigma2 Prior: -535.4929\tRegularization: 0.0035\n",
            "Iter: 7560  \tTraining Loss: -501.1566    \n",
            "    Negative Log Likelihood: 38.6697\tSigma2 Prior: -539.8298\tRegularization: 0.0035\n",
            "Iter: 7570  \tTraining Loss: -485.8727    \n",
            "    Negative Log Likelihood: 30.3154\tSigma2 Prior: -516.1917\tRegularization: 0.0035\n",
            "Iter: 7580  \tTraining Loss: -511.7332    \n",
            "    Negative Log Likelihood: 32.3040\tSigma2 Prior: -544.0407\tRegularization: 0.0035\n",
            "Iter: 7590  \tTraining Loss: -489.8547    \n",
            "    Negative Log Likelihood: 40.9820\tSigma2 Prior: -530.8403\tRegularization: 0.0035\n",
            "Iter: 7600  \tTraining Loss: -491.9532    \n",
            "    Negative Log Likelihood: 33.3295\tSigma2 Prior: -525.2862\tRegularization: 0.0035\n",
            "Iter: 7610  \tTraining Loss: -516.3967    \n",
            "    Negative Log Likelihood: 33.9306\tSigma2 Prior: -550.3309\tRegularization: 0.0036\n",
            "Iter: 7620  \tTraining Loss: -484.9177    \n",
            "    Negative Log Likelihood: 35.8013\tSigma2 Prior: -520.7226\tRegularization: 0.0036\n",
            "Iter: 7630  \tTraining Loss: -486.5562    \n",
            "    Negative Log Likelihood: 33.3098\tSigma2 Prior: -519.8696\tRegularization: 0.0036\n",
            "Iter: 7640  \tTraining Loss: -464.5096    \n",
            "    Negative Log Likelihood: 34.1861\tSigma2 Prior: -498.6992\tRegularization: 0.0036\n",
            "Iter: 7650  \tTraining Loss: -486.9387    \n",
            "    Negative Log Likelihood: 37.3507\tSigma2 Prior: -524.2930\tRegularization: 0.0036\n",
            "Iter: 7660  \tTraining Loss: -502.8942    \n",
            "    Negative Log Likelihood: 33.0950\tSigma2 Prior: -535.9927\tRegularization: 0.0036\n",
            "Iter: 7670  \tTraining Loss: -486.4739    \n",
            "    Negative Log Likelihood: 33.5569\tSigma2 Prior: -520.0344\tRegularization: 0.0036\n",
            "Iter: 7680  \tTraining Loss: -481.0181    \n",
            "    Negative Log Likelihood: 34.0762\tSigma2 Prior: -515.0978\tRegularization: 0.0036\n",
            "Iter: 7690  \tTraining Loss: -500.7792    \n",
            "    Negative Log Likelihood: 33.9536\tSigma2 Prior: -534.7363\tRegularization: 0.0036\n",
            "Iter: 7700  \tTraining Loss: -498.5830    \n",
            "    Negative Log Likelihood: 33.0080\tSigma2 Prior: -531.5945\tRegularization: 0.0036\n",
            "Iter: 7710  \tTraining Loss: -428.5790    \n",
            "    Negative Log Likelihood: 35.7946\tSigma2 Prior: -464.3771\tRegularization: 0.0036\n",
            "Iter: 7720  \tTraining Loss: -488.5587    \n",
            "    Negative Log Likelihood: 28.9314\tSigma2 Prior: -517.4937\tRegularization: 0.0036\n",
            "Iter: 7730  \tTraining Loss: -463.0860    \n",
            "    Negative Log Likelihood: 34.7324\tSigma2 Prior: -497.8221\tRegularization: 0.0036\n",
            "Iter: 7740  \tTraining Loss: -493.2873    \n",
            "    Negative Log Likelihood: 34.3993\tSigma2 Prior: -527.6902\tRegularization: 0.0036\n",
            "Iter: 7750  \tTraining Loss: -463.9942    \n",
            "    Negative Log Likelihood: 35.2067\tSigma2 Prior: -499.2045\tRegularization: 0.0036\n",
            "Iter: 7760  \tTraining Loss: -486.1783    \n",
            "    Negative Log Likelihood: 31.8704\tSigma2 Prior: -518.0524\tRegularization: 0.0036\n",
            "Iter: 7770  \tTraining Loss: -503.4161    \n",
            "    Negative Log Likelihood: 39.6823\tSigma2 Prior: -543.1019\tRegularization: 0.0036\n",
            "Iter: 7780  \tTraining Loss: -528.5705    \n",
            "    Negative Log Likelihood: 32.9755\tSigma2 Prior: -561.5496\tRegularization: 0.0036\n",
            "Iter: 7790  \tTraining Loss: -483.3183    \n",
            "    Negative Log Likelihood: 37.4308\tSigma2 Prior: -520.7527\tRegularization: 0.0036\n",
            "Iter: 7800  \tTraining Loss: -499.1840    \n",
            "    Negative Log Likelihood: 28.4459\tSigma2 Prior: -527.6335\tRegularization: 0.0036\n",
            "Iter: 7810  \tTraining Loss: -473.6220    \n",
            "    Negative Log Likelihood: 30.3376\tSigma2 Prior: -503.9632\tRegularization: 0.0036\n",
            "Iter: 7820  \tTraining Loss: -466.6226    \n",
            "    Negative Log Likelihood: 36.6089\tSigma2 Prior: -503.2351\tRegularization: 0.0036\n",
            "Iter: 7830  \tTraining Loss: -501.2284    \n",
            "    Negative Log Likelihood: 31.0603\tSigma2 Prior: -532.2923\tRegularization: 0.0036\n",
            "Iter: 7840  \tTraining Loss: -491.1932    \n",
            "    Negative Log Likelihood: 30.4222\tSigma2 Prior: -521.6191\tRegularization: 0.0036\n",
            "Iter: 7850  \tTraining Loss: -490.4457    \n",
            "    Negative Log Likelihood: 31.5473\tSigma2 Prior: -521.9966\tRegularization: 0.0036\n",
            "Iter: 7860  \tTraining Loss: -453.5108    \n",
            "    Negative Log Likelihood: 36.7058\tSigma2 Prior: -490.2203\tRegularization: 0.0036\n",
            "Iter: 7870  \tTraining Loss: -508.0988    \n",
            "    Negative Log Likelihood: 35.0681\tSigma2 Prior: -543.1705\tRegularization: 0.0036\n",
            "Iter: 7880  \tTraining Loss: -477.5226    \n",
            "    Negative Log Likelihood: 31.2307\tSigma2 Prior: -508.7569\tRegularization: 0.0036\n",
            "Iter: 7890  \tTraining Loss: -474.0230    \n",
            "    Negative Log Likelihood: 32.5547\tSigma2 Prior: -506.5813\tRegularization: 0.0036\n",
            "Iter: 7900  \tTraining Loss: -499.1886    \n",
            "    Negative Log Likelihood: 29.7143\tSigma2 Prior: -528.9066\tRegularization: 0.0036\n",
            "Iter: 7910  \tTraining Loss: -500.3723    \n",
            "    Negative Log Likelihood: 37.1810\tSigma2 Prior: -537.5570\tRegularization: 0.0036\n",
            "Iter: 7920  \tTraining Loss: -477.1660    \n",
            "    Negative Log Likelihood: 32.0169\tSigma2 Prior: -509.1865\tRegularization: 0.0036\n",
            "Iter: 7930  \tTraining Loss: -478.0410    \n",
            "    Negative Log Likelihood: 35.5101\tSigma2 Prior: -513.5547\tRegularization: 0.0036\n",
            "Iter: 7940  \tTraining Loss: -495.3378    \n",
            "    Negative Log Likelihood: 33.8615\tSigma2 Prior: -529.2030\tRegularization: 0.0037\n",
            "Iter: 7950  \tTraining Loss: -498.5558    \n",
            "    Negative Log Likelihood: 32.1535\tSigma2 Prior: -530.7130\tRegularization: 0.0037\n",
            "Iter: 7960  \tTraining Loss: -512.1776    \n",
            "    Negative Log Likelihood: 30.6487\tSigma2 Prior: -542.8300\tRegularization: 0.0037\n",
            "Iter: 7970  \tTraining Loss: -527.1320    \n",
            "    Negative Log Likelihood: 30.7485\tSigma2 Prior: -557.8842\tRegularization: 0.0037\n",
            "Iter: 7980  \tTraining Loss: -490.9980    \n",
            "    Negative Log Likelihood: 38.2521\tSigma2 Prior: -529.2538\tRegularization: 0.0037\n",
            "Iter: 7990  \tTraining Loss: -500.4207    \n",
            "    Negative Log Likelihood: 26.1408\tSigma2 Prior: -526.5652\tRegularization: 0.0037\n",
            "Iter: 8000  \tTraining Loss: -496.4939    \n",
            "    Negative Log Likelihood: 33.4350\tSigma2 Prior: -529.9326\tRegularization: 0.0037\n",
            "Iter: 8010  \tTraining Loss: -483.4503    \n",
            "    Negative Log Likelihood: 32.0526\tSigma2 Prior: -515.5065\tRegularization: 0.0037\n",
            "Iter: 8020  \tTraining Loss: -493.4914    \n",
            "    Negative Log Likelihood: 30.3830\tSigma2 Prior: -523.8781\tRegularization: 0.0037\n",
            "Iter: 8030  \tTraining Loss: -516.1948    \n",
            "    Negative Log Likelihood: 36.3429\tSigma2 Prior: -552.5413\tRegularization: 0.0037\n",
            "Iter: 8040  \tTraining Loss: -507.9915    \n",
            "    Negative Log Likelihood: 32.4177\tSigma2 Prior: -540.4128\tRegularization: 0.0037\n",
            "Iter: 8050  \tTraining Loss: -491.7998    \n",
            "    Negative Log Likelihood: 32.6363\tSigma2 Prior: -524.4398\tRegularization: 0.0037\n",
            "Iter: 8060  \tTraining Loss: -471.6323    \n",
            "    Negative Log Likelihood: 38.3296\tSigma2 Prior: -509.9655\tRegularization: 0.0037\n",
            "Iter: 8070  \tTraining Loss: -518.4680    \n",
            "    Negative Log Likelihood: 32.6344\tSigma2 Prior: -551.1060\tRegularization: 0.0037\n",
            "Iter: 8080  \tTraining Loss: -490.7258    \n",
            "    Negative Log Likelihood: 35.7869\tSigma2 Prior: -526.5164\tRegularization: 0.0037\n",
            "Iter: 8090  \tTraining Loss: -499.2460    \n",
            "    Negative Log Likelihood: 28.9606\tSigma2 Prior: -528.2103\tRegularization: 0.0037\n",
            "Iter: 8100  \tTraining Loss: -490.4777    \n",
            "    Negative Log Likelihood: 33.4739\tSigma2 Prior: -523.9553\tRegularization: 0.0037\n",
            "Iter: 8110  \tTraining Loss: -482.9608    \n",
            "    Negative Log Likelihood: 31.7803\tSigma2 Prior: -514.7448\tRegularization: 0.0037\n",
            "Iter: 8120  \tTraining Loss: -479.3733    \n",
            "    Negative Log Likelihood: 32.5365\tSigma2 Prior: -511.9135\tRegularization: 0.0037\n",
            "Iter: 8130  \tTraining Loss: -503.1402    \n",
            "    Negative Log Likelihood: 35.7221\tSigma2 Prior: -538.8660\tRegularization: 0.0037\n",
            "Iter: 8140  \tTraining Loss: -506.8496    \n",
            "    Negative Log Likelihood: 31.3190\tSigma2 Prior: -538.1723\tRegularization: 0.0037\n",
            "Iter: 8150  \tTraining Loss: -504.0184    \n",
            "    Negative Log Likelihood: 39.5266\tSigma2 Prior: -543.5488\tRegularization: 0.0037\n",
            "Iter: 8160  \tTraining Loss: -471.2578    \n",
            "    Negative Log Likelihood: 32.7868\tSigma2 Prior: -504.0484\tRegularization: 0.0037\n",
            "Iter: 8170  \tTraining Loss: -482.2459    \n",
            "    Negative Log Likelihood: 33.4475\tSigma2 Prior: -515.6971\tRegularization: 0.0037\n",
            "Iter: 8180  \tTraining Loss: -477.1210    \n",
            "    Negative Log Likelihood: 36.4665\tSigma2 Prior: -513.5912\tRegularization: 0.0037\n",
            "Iter: 8190  \tTraining Loss: -505.4961    \n",
            "    Negative Log Likelihood: 32.3417\tSigma2 Prior: -537.8416\tRegularization: 0.0037\n",
            "Iter: 8200  \tTraining Loss: -471.1266    \n",
            "    Negative Log Likelihood: 36.5094\tSigma2 Prior: -507.6397\tRegularization: 0.0037\n",
            "Iter: 8210  \tTraining Loss: -461.7470    \n",
            "    Negative Log Likelihood: 38.5744\tSigma2 Prior: -500.3252\tRegularization: 0.0037\n",
            "Iter: 8220  \tTraining Loss: -512.6622    \n",
            "    Negative Log Likelihood: 35.4982\tSigma2 Prior: -548.1641\tRegularization: 0.0037\n",
            "Iter: 8230  \tTraining Loss: -478.2021    \n",
            "    Negative Log Likelihood: 37.7591\tSigma2 Prior: -515.9650\tRegularization: 0.0037\n",
            "Iter: 8240  \tTraining Loss: -479.3656    \n",
            "    Negative Log Likelihood: 31.9005\tSigma2 Prior: -511.2698\tRegularization: 0.0037\n",
            "Iter: 8250  \tTraining Loss: -486.8744    \n",
            "    Negative Log Likelihood: 36.1332\tSigma2 Prior: -523.0113\tRegularization: 0.0037\n",
            "Iter: 8260  \tTraining Loss: -490.2915    \n",
            "    Negative Log Likelihood: 32.3385\tSigma2 Prior: -522.6337\tRegularization: 0.0037\n",
            "Iter: 8270  \tTraining Loss: -489.5218    \n",
            "    Negative Log Likelihood: 36.4894\tSigma2 Prior: -526.0150\tRegularization: 0.0037\n",
            "Iter: 8280  \tTraining Loss: -482.6868    \n",
            "    Negative Log Likelihood: 33.2512\tSigma2 Prior: -515.9418\tRegularization: 0.0038\n",
            "Iter: 8290  \tTraining Loss: -502.6407    \n",
            "    Negative Log Likelihood: 35.0134\tSigma2 Prior: -537.6578\tRegularization: 0.0038\n",
            "Iter: 8300  \tTraining Loss: -505.3846    \n",
            "    Negative Log Likelihood: 34.1948\tSigma2 Prior: -539.5831\tRegularization: 0.0038\n",
            "Iter: 8310  \tTraining Loss: -491.7614    \n",
            "    Negative Log Likelihood: 34.4076\tSigma2 Prior: -526.1728\tRegularization: 0.0038\n",
            "Iter: 8320  \tTraining Loss: -474.5101    \n",
            "    Negative Log Likelihood: 27.7871\tSigma2 Prior: -502.3010\tRegularization: 0.0038\n",
            "Iter: 8330  \tTraining Loss: -521.1411    \n",
            "    Negative Log Likelihood: 35.7624\tSigma2 Prior: -556.9073\tRegularization: 0.0038\n",
            "Iter: 8340  \tTraining Loss: -503.3501    \n",
            "    Negative Log Likelihood: 31.8163\tSigma2 Prior: -535.1702\tRegularization: 0.0038\n",
            "Iter: 8350  \tTraining Loss: -513.9484    \n",
            "    Negative Log Likelihood: 29.3488\tSigma2 Prior: -543.3010\tRegularization: 0.0038\n",
            "Iter: 8360  \tTraining Loss: -511.2431    \n",
            "    Negative Log Likelihood: 39.3078\tSigma2 Prior: -550.5547\tRegularization: 0.0038\n",
            "Iter: 8370  \tTraining Loss: -508.3058    \n",
            "    Negative Log Likelihood: 32.4451\tSigma2 Prior: -540.7547\tRegularization: 0.0038\n",
            "Iter: 8380  \tTraining Loss: -511.1066    \n",
            "    Negative Log Likelihood: 33.8558\tSigma2 Prior: -544.9661\tRegularization: 0.0038\n",
            "Iter: 8390  \tTraining Loss: -504.3309    \n",
            "    Negative Log Likelihood: 32.5573\tSigma2 Prior: -536.8920\tRegularization: 0.0038\n",
            "Iter: 8400  \tTraining Loss: -507.5177    \n",
            "    Negative Log Likelihood: 33.5533\tSigma2 Prior: -541.0748\tRegularization: 0.0038\n",
            "Iter: 8410  \tTraining Loss: -499.2155    \n",
            "    Negative Log Likelihood: 35.1058\tSigma2 Prior: -534.3251\tRegularization: 0.0038\n",
            "Iter: 8420  \tTraining Loss: -480.6300    \n",
            "    Negative Log Likelihood: 31.3934\tSigma2 Prior: -512.0272\tRegularization: 0.0038\n",
            "Iter: 8430  \tTraining Loss: -463.7054    \n",
            "    Negative Log Likelihood: 29.0572\tSigma2 Prior: -492.7664\tRegularization: 0.0038\n",
            "Iter: 8440  \tTraining Loss: -458.1075    \n",
            "    Negative Log Likelihood: 36.6476\tSigma2 Prior: -494.7589\tRegularization: 0.0038\n",
            "Iter: 8450  \tTraining Loss: -469.5786    \n",
            "    Negative Log Likelihood: 33.0975\tSigma2 Prior: -502.6799\tRegularization: 0.0038\n",
            "Iter: 8460  \tTraining Loss: -471.4965    \n",
            "    Negative Log Likelihood: 27.9306\tSigma2 Prior: -499.4309\tRegularization: 0.0038\n",
            "Iter: 8470  \tTraining Loss: -508.4633    \n",
            "    Negative Log Likelihood: 37.8462\tSigma2 Prior: -546.3134\tRegularization: 0.0038\n",
            "Iter: 8480  \tTraining Loss: -508.8882    \n",
            "    Negative Log Likelihood: 37.6358\tSigma2 Prior: -546.5278\tRegularization: 0.0038\n",
            "Iter: 8490  \tTraining Loss: -422.2679    \n",
            "    Negative Log Likelihood: 27.9025\tSigma2 Prior: -450.1742\tRegularization: 0.0038\n",
            "Iter: 8500  \tTraining Loss: -499.7922    \n",
            "    Negative Log Likelihood: 27.3789\tSigma2 Prior: -527.1749\tRegularization: 0.0038\n",
            "Iter: 8510  \tTraining Loss: -510.9236    \n",
            "    Negative Log Likelihood: 37.9554\tSigma2 Prior: -548.8828\tRegularization: 0.0038\n",
            "Iter: 8520  \tTraining Loss: -524.9335    \n",
            "    Negative Log Likelihood: 30.2748\tSigma2 Prior: -555.2122\tRegularization: 0.0038\n",
            "Iter: 8530  \tTraining Loss: -528.2675    \n",
            "    Negative Log Likelihood: 38.7394\tSigma2 Prior: -567.0107\tRegularization: 0.0038\n",
            "Iter: 8540  \tTraining Loss: -491.5095    \n",
            "    Negative Log Likelihood: 38.0827\tSigma2 Prior: -529.5960\tRegularization: 0.0038\n",
            "Iter: 8550  \tTraining Loss: -497.0256    \n",
            "    Negative Log Likelihood: 40.7099\tSigma2 Prior: -537.7394\tRegularization: 0.0038\n",
            "Iter: 8560  \tTraining Loss: -495.5663    \n",
            "    Negative Log Likelihood: 32.8977\tSigma2 Prior: -528.4678\tRegularization: 0.0038\n",
            "Iter: 8570  \tTraining Loss: -492.5072    \n",
            "    Negative Log Likelihood: 30.6201\tSigma2 Prior: -523.1311\tRegularization: 0.0038\n",
            "Iter: 8580  \tTraining Loss: -496.4390    \n",
            "    Negative Log Likelihood: 32.3206\tSigma2 Prior: -528.7634\tRegularization: 0.0038\n",
            "Iter: 8590  \tTraining Loss: -499.7168    \n",
            "    Negative Log Likelihood: 34.0101\tSigma2 Prior: -533.7307\tRegularization: 0.0038\n",
            "Iter: 8600  \tTraining Loss: -424.7395    \n",
            "    Negative Log Likelihood: 43.7133\tSigma2 Prior: -468.4567\tRegularization: 0.0038\n",
            "Iter: 8610  \tTraining Loss: -475.6613    \n",
            "    Negative Log Likelihood: 33.3602\tSigma2 Prior: -509.0253\tRegularization: 0.0038\n",
            "Iter: 8620  \tTraining Loss: -514.3545    \n",
            "    Negative Log Likelihood: 33.4219\tSigma2 Prior: -547.7803\tRegularization: 0.0039\n",
            "Iter: 8630  \tTraining Loss: -492.5550    \n",
            "    Negative Log Likelihood: 31.8984\tSigma2 Prior: -524.4572\tRegularization: 0.0039\n",
            "Iter: 8640  \tTraining Loss: -510.0831    \n",
            "    Negative Log Likelihood: 36.2784\tSigma2 Prior: -546.3654\tRegularization: 0.0039\n",
            "Iter: 8650  \tTraining Loss: -468.0393    \n",
            "    Negative Log Likelihood: 33.9268\tSigma2 Prior: -501.9700\tRegularization: 0.0039\n",
            "Iter: 8660  \tTraining Loss: -459.4381    \n",
            "    Negative Log Likelihood: 29.3100\tSigma2 Prior: -488.7519\tRegularization: 0.0039\n",
            "Iter: 8670  \tTraining Loss: -493.4087    \n",
            "    Negative Log Likelihood: 37.9937\tSigma2 Prior: -531.4062\tRegularization: 0.0039\n",
            "Iter: 8680  \tTraining Loss: -499.7170    \n",
            "    Negative Log Likelihood: 26.9617\tSigma2 Prior: -526.6826\tRegularization: 0.0039\n",
            "Iter: 8690  \tTraining Loss: -484.0525    \n",
            "    Negative Log Likelihood: 30.0126\tSigma2 Prior: -514.0690\tRegularization: 0.0039\n",
            "Iter: 8700  \tTraining Loss: -489.6231    \n",
            "    Negative Log Likelihood: 36.7070\tSigma2 Prior: -526.3340\tRegularization: 0.0039\n",
            "Iter: 8710  \tTraining Loss: -517.2332    \n",
            "    Negative Log Likelihood: 33.9233\tSigma2 Prior: -551.1603\tRegularization: 0.0039\n",
            "Iter: 8720  \tTraining Loss: -502.0181    \n",
            "    Negative Log Likelihood: 30.3610\tSigma2 Prior: -532.3831\tRegularization: 0.0039\n",
            "Iter: 8730  \tTraining Loss: -522.5845    \n",
            "    Negative Log Likelihood: 33.7141\tSigma2 Prior: -556.3025\tRegularization: 0.0039\n",
            "Iter: 8740  \tTraining Loss: -499.0580    \n",
            "    Negative Log Likelihood: 40.4894\tSigma2 Prior: -539.5513\tRegularization: 0.0039\n",
            "Iter: 8750  \tTraining Loss: -487.7985    \n",
            "    Negative Log Likelihood: 32.1050\tSigma2 Prior: -519.9073\tRegularization: 0.0039\n",
            "Iter: 8760  \tTraining Loss: -473.2318    \n",
            "    Negative Log Likelihood: 32.4742\tSigma2 Prior: -505.7100\tRegularization: 0.0039\n",
            "Iter: 8770  \tTraining Loss: -525.3824    \n",
            "    Negative Log Likelihood: 36.7045\tSigma2 Prior: -562.0908\tRegularization: 0.0039\n",
            "Iter: 8780  \tTraining Loss: -479.9416    \n",
            "    Negative Log Likelihood: 34.1868\tSigma2 Prior: -514.1324\tRegularization: 0.0039\n",
            "Iter: 8790  \tTraining Loss: -465.7135    \n",
            "    Negative Log Likelihood: 31.9148\tSigma2 Prior: -497.6323\tRegularization: 0.0039\n",
            "Iter: 8800  \tTraining Loss: -472.0443    \n",
            "    Negative Log Likelihood: 37.5238\tSigma2 Prior: -509.5720\tRegularization: 0.0039\n",
            "Iter: 8810  \tTraining Loss: -496.0593    \n",
            "    Negative Log Likelihood: 29.0821\tSigma2 Prior: -525.1453\tRegularization: 0.0039\n",
            "Iter: 8820  \tTraining Loss: -493.5000    \n",
            "    Negative Log Likelihood: 34.0207\tSigma2 Prior: -527.5246\tRegularization: 0.0039\n",
            "Iter: 8830  \tTraining Loss: -526.1906    \n",
            "    Negative Log Likelihood: 34.9194\tSigma2 Prior: -561.1140\tRegularization: 0.0039\n",
            "Iter: 8840  \tTraining Loss: -507.2071    \n",
            "    Negative Log Likelihood: 35.2929\tSigma2 Prior: -542.5039\tRegularization: 0.0039\n",
            "Iter: 8850  \tTraining Loss: -477.6709    \n",
            "    Negative Log Likelihood: 36.8497\tSigma2 Prior: -514.5245\tRegularization: 0.0039\n",
            "Iter: 8860  \tTraining Loss: -499.9429    \n",
            "    Negative Log Likelihood: 31.0158\tSigma2 Prior: -530.9626\tRegularization: 0.0039\n",
            "Iter: 8870  \tTraining Loss: -520.9586    \n",
            "    Negative Log Likelihood: 31.2953\tSigma2 Prior: -552.2579\tRegularization: 0.0039\n",
            "Iter: 8880  \tTraining Loss: -505.7077    \n",
            "    Negative Log Likelihood: 32.8619\tSigma2 Prior: -538.5735\tRegularization: 0.0039\n",
            "Iter: 8890  \tTraining Loss: -495.4229    \n",
            "    Negative Log Likelihood: 32.9375\tSigma2 Prior: -528.3644\tRegularization: 0.0039\n",
            "Iter: 8900  \tTraining Loss: -498.7953    \n",
            "    Negative Log Likelihood: 31.3444\tSigma2 Prior: -530.1437\tRegularization: 0.0039\n",
            "Iter: 8910  \tTraining Loss: -518.5540    \n",
            "    Negative Log Likelihood: 32.7787\tSigma2 Prior: -551.3365\tRegularization: 0.0039\n",
            "Iter: 8920  \tTraining Loss: -484.4041    \n",
            "    Negative Log Likelihood: 32.6769\tSigma2 Prior: -517.0850\tRegularization: 0.0039\n",
            "Iter: 8930  \tTraining Loss: -498.3271    \n",
            "    Negative Log Likelihood: 37.5231\tSigma2 Prior: -535.8542\tRegularization: 0.0039\n",
            "Iter: 8940  \tTraining Loss: -491.2770    \n",
            "    Negative Log Likelihood: 35.7552\tSigma2 Prior: -527.0361\tRegularization: 0.0039\n",
            "Iter: 8950  \tTraining Loss: -482.0698    \n",
            "    Negative Log Likelihood: 32.3060\tSigma2 Prior: -514.3797\tRegularization: 0.0039\n",
            "Iter: 8960  \tTraining Loss: -491.6301    \n",
            "    Negative Log Likelihood: 33.5385\tSigma2 Prior: -525.1726\tRegularization: 0.0039\n",
            "Iter: 8970  \tTraining Loss: -462.2062    \n",
            "    Negative Log Likelihood: 32.0137\tSigma2 Prior: -494.2239\tRegularization: 0.0040\n",
            "Iter: 8980  \tTraining Loss: -502.2687    \n",
            "    Negative Log Likelihood: 38.4239\tSigma2 Prior: -540.6965\tRegularization: 0.0040\n",
            "Iter: 8990  \tTraining Loss: -504.6721    \n",
            "    Negative Log Likelihood: 32.9160\tSigma2 Prior: -537.5920\tRegularization: 0.0040\n",
            "Iter: 9000  \tTraining Loss: -483.7321    \n",
            "    Negative Log Likelihood: 35.6866\tSigma2 Prior: -519.4227\tRegularization: 0.0040\n",
            "Iter: 9010  \tTraining Loss: -495.1841    \n",
            "    Negative Log Likelihood: 31.9290\tSigma2 Prior: -527.1171\tRegularization: 0.0040\n",
            "Iter: 9020  \tTraining Loss: -440.1395    \n",
            "    Negative Log Likelihood: 40.3967\tSigma2 Prior: -480.5402\tRegularization: 0.0040\n",
            "Iter: 9030  \tTraining Loss: -472.2880    \n",
            "    Negative Log Likelihood: 36.0093\tSigma2 Prior: -508.3013\tRegularization: 0.0040\n",
            "Iter: 9040  \tTraining Loss: -462.4217    \n",
            "    Negative Log Likelihood: 33.4830\tSigma2 Prior: -495.9087\tRegularization: 0.0040\n",
            "Iter: 9050  \tTraining Loss: -485.3234    \n",
            "    Negative Log Likelihood: 33.3186\tSigma2 Prior: -518.6460\tRegularization: 0.0040\n",
            "Iter: 9060  \tTraining Loss: -481.3822    \n",
            "    Negative Log Likelihood: 34.0695\tSigma2 Prior: -515.4557\tRegularization: 0.0040\n",
            "Iter: 9070  \tTraining Loss: -483.2364    \n",
            "    Negative Log Likelihood: 33.2378\tSigma2 Prior: -516.4781\tRegularization: 0.0040\n",
            "Iter: 9080  \tTraining Loss: -438.5148    \n",
            "    Negative Log Likelihood: 31.6087\tSigma2 Prior: -470.1275\tRegularization: 0.0040\n",
            "Iter: 9090  \tTraining Loss: -507.2003    \n",
            "    Negative Log Likelihood: 35.8373\tSigma2 Prior: -543.0417\tRegularization: 0.0040\n",
            "Iter: 9100  \tTraining Loss: -418.1154    \n",
            "    Negative Log Likelihood: 34.3779\tSigma2 Prior: -452.4973\tRegularization: 0.0040\n",
            "Iter: 9110  \tTraining Loss: -483.8079    \n",
            "    Negative Log Likelihood: 30.2931\tSigma2 Prior: -514.1050\tRegularization: 0.0040\n",
            "Iter: 9120  \tTraining Loss: -493.1377    \n",
            "    Negative Log Likelihood: 34.0873\tSigma2 Prior: -527.2290\tRegularization: 0.0040\n",
            "Iter: 9130  \tTraining Loss: -493.8249    \n",
            "    Negative Log Likelihood: 32.2963\tSigma2 Prior: -526.1252\tRegularization: 0.0040\n",
            "Iter: 9140  \tTraining Loss: -517.9113    \n",
            "    Negative Log Likelihood: 36.0292\tSigma2 Prior: -553.9445\tRegularization: 0.0040\n",
            "Iter: 9150  \tTraining Loss: -506.6619    \n",
            "    Negative Log Likelihood: 37.6128\tSigma2 Prior: -544.2787\tRegularization: 0.0040\n",
            "Iter: 9160  \tTraining Loss: -504.0619    \n",
            "    Negative Log Likelihood: 32.0780\tSigma2 Prior: -536.1439\tRegularization: 0.0040\n",
            "Iter: 9170  \tTraining Loss: -512.6480    \n",
            "    Negative Log Likelihood: 36.1126\tSigma2 Prior: -548.7646\tRegularization: 0.0040\n",
            "Iter: 9180  \tTraining Loss: -512.5090    \n",
            "    Negative Log Likelihood: 36.9025\tSigma2 Prior: -549.4155\tRegularization: 0.0040\n",
            "Iter: 9190  \tTraining Loss: -519.0868    \n",
            "    Negative Log Likelihood: 34.9624\tSigma2 Prior: -554.0532\tRegularization: 0.0040\n",
            "Iter: 9200  \tTraining Loss: -476.5781    \n",
            "    Negative Log Likelihood: 29.0332\tSigma2 Prior: -505.6154\tRegularization: 0.0040\n",
            "Iter: 9210  \tTraining Loss: -500.3840    \n",
            "    Negative Log Likelihood: 27.9664\tSigma2 Prior: -528.3545\tRegularization: 0.0040\n",
            "Iter: 9220  \tTraining Loss: -444.6909    \n",
            "    Negative Log Likelihood: 45.7733\tSigma2 Prior: -490.4683\tRegularization: 0.0040\n",
            "Iter: 9230  \tTraining Loss: -482.5377    \n",
            "    Negative Log Likelihood: 27.6250\tSigma2 Prior: -510.1667\tRegularization: 0.0040\n",
            "Iter: 9240  \tTraining Loss: -453.7611    \n",
            "    Negative Log Likelihood: 32.4487\tSigma2 Prior: -486.2137\tRegularization: 0.0040\n",
            "Iter: 9250  \tTraining Loss: -470.0343    \n",
            "    Negative Log Likelihood: 37.7591\tSigma2 Prior: -507.7974\tRegularization: 0.0040\n",
            "Iter: 9260  \tTraining Loss: -503.0947    \n",
            "    Negative Log Likelihood: 32.9859\tSigma2 Prior: -536.0847\tRegularization: 0.0040\n",
            "Iter: 9270  \tTraining Loss: -440.6371    \n",
            "    Negative Log Likelihood: 34.5365\tSigma2 Prior: -475.1777\tRegularization: 0.0040\n",
            "Iter: 9280  \tTraining Loss: -507.3764    \n",
            "    Negative Log Likelihood: 29.9143\tSigma2 Prior: -537.2948\tRegularization: 0.0040\n",
            "Iter: 9290  \tTraining Loss: -498.5882    \n",
            "    Negative Log Likelihood: 33.3935\tSigma2 Prior: -531.9857\tRegularization: 0.0040\n",
            "Iter: 9300  \tTraining Loss: -459.4962    \n",
            "    Negative Log Likelihood: 32.8501\tSigma2 Prior: -492.3504\tRegularization: 0.0040\n",
            "Iter: 9310  \tTraining Loss: -496.3117    \n",
            "    Negative Log Likelihood: 34.9002\tSigma2 Prior: -531.2160\tRegularization: 0.0040\n",
            "Iter: 9320  \tTraining Loss: -473.5127    \n",
            "    Negative Log Likelihood: 38.2937\tSigma2 Prior: -511.8105\tRegularization: 0.0041\n",
            "Iter: 9330  \tTraining Loss: -501.1566    \n",
            "    Negative Log Likelihood: 27.3652\tSigma2 Prior: -528.5258\tRegularization: 0.0041\n",
            "Iter: 9340  \tTraining Loss: -469.9716    \n",
            "    Negative Log Likelihood: 37.3758\tSigma2 Prior: -507.3514\tRegularization: 0.0041\n",
            "Iter: 9350  \tTraining Loss: -504.2437    \n",
            "    Negative Log Likelihood: 29.2120\tSigma2 Prior: -533.4598\tRegularization: 0.0041\n",
            "Iter: 9360  \tTraining Loss: -494.2050    \n",
            "    Negative Log Likelihood: 36.9268\tSigma2 Prior: -531.1359\tRegularization: 0.0041\n",
            "Iter: 9370  \tTraining Loss: -532.1314    \n",
            "    Negative Log Likelihood: 30.5070\tSigma2 Prior: -562.6425\tRegularization: 0.0041\n",
            "Iter: 9380  \tTraining Loss: -501.4325    \n",
            "    Negative Log Likelihood: 32.6726\tSigma2 Prior: -534.1092\tRegularization: 0.0041\n",
            "Iter: 9390  \tTraining Loss: -519.2011    \n",
            "    Negative Log Likelihood: 29.9212\tSigma2 Prior: -549.1264\tRegularization: 0.0041\n",
            "Iter: 9400  \tTraining Loss: -493.6293    \n",
            "    Negative Log Likelihood: 28.5162\tSigma2 Prior: -522.1497\tRegularization: 0.0041\n",
            "Iter: 9410  \tTraining Loss: -501.0691    \n",
            "    Negative Log Likelihood: 37.9369\tSigma2 Prior: -539.0100\tRegularization: 0.0041\n",
            "Iter: 9420  \tTraining Loss: -492.1092    \n",
            "    Negative Log Likelihood: 29.3687\tSigma2 Prior: -521.4821\tRegularization: 0.0041\n",
            "Iter: 9430  \tTraining Loss: -493.7672    \n",
            "    Negative Log Likelihood: 31.3212\tSigma2 Prior: -525.0925\tRegularization: 0.0041\n",
            "Iter: 9440  \tTraining Loss: -504.5896    \n",
            "    Negative Log Likelihood: 39.6092\tSigma2 Prior: -544.2029\tRegularization: 0.0041\n",
            "Iter: 9450  \tTraining Loss: -480.3680    \n",
            "    Negative Log Likelihood: 28.7218\tSigma2 Prior: -509.0939\tRegularization: 0.0041\n",
            "Iter: 9460  \tTraining Loss: -433.8291    \n",
            "    Negative Log Likelihood: 34.6843\tSigma2 Prior: -468.5175\tRegularization: 0.0041\n",
            "Iter: 9470  \tTraining Loss: -483.9251    \n",
            "    Negative Log Likelihood: 33.1165\tSigma2 Prior: -517.0458\tRegularization: 0.0041\n",
            "Iter: 9480  \tTraining Loss: -524.0825    \n",
            "    Negative Log Likelihood: 32.6343\tSigma2 Prior: -556.7208\tRegularization: 0.0041\n",
            "Iter: 9490  \tTraining Loss: -475.5260    \n",
            "    Negative Log Likelihood: 40.5843\tSigma2 Prior: -516.1143\tRegularization: 0.0041\n",
            "Iter: 9500  \tTraining Loss: -486.2243    \n",
            "    Negative Log Likelihood: 29.8942\tSigma2 Prior: -516.1227\tRegularization: 0.0041\n",
            "Iter: 9510  \tTraining Loss: -504.3202    \n",
            "    Negative Log Likelihood: 31.2289\tSigma2 Prior: -535.5532\tRegularization: 0.0041\n",
            "Iter: 9520  \tTraining Loss: -486.6395    \n",
            "    Negative Log Likelihood: 38.8472\tSigma2 Prior: -525.4908\tRegularization: 0.0041\n",
            "Iter: 9530  \tTraining Loss: -495.8794    \n",
            "    Negative Log Likelihood: 28.1012\tSigma2 Prior: -523.9847\tRegularization: 0.0041\n",
            "Iter: 9540  \tTraining Loss: -456.5309    \n",
            "    Negative Log Likelihood: 38.0516\tSigma2 Prior: -494.5866\tRegularization: 0.0041\n",
            "Iter: 9550  \tTraining Loss: -422.2388    \n",
            "    Negative Log Likelihood: 34.0740\tSigma2 Prior: -456.3170\tRegularization: 0.0041\n",
            "Iter: 9560  \tTraining Loss: -504.1086    \n",
            "    Negative Log Likelihood: 29.4978\tSigma2 Prior: -533.6105\tRegularization: 0.0041\n",
            "Iter: 9570  \tTraining Loss: -463.5168    \n",
            "    Negative Log Likelihood: 36.3088\tSigma2 Prior: -499.8298\tRegularization: 0.0041\n",
            "Iter: 9580  \tTraining Loss: -421.8399    \n",
            "    Negative Log Likelihood: 35.8725\tSigma2 Prior: -457.7165\tRegularization: 0.0041\n",
            "Iter: 9590  \tTraining Loss: -478.1503    \n",
            "    Negative Log Likelihood: 29.3071\tSigma2 Prior: -507.4616\tRegularization: 0.0041\n",
            "Iter: 9600  \tTraining Loss: -522.2695    \n",
            "    Negative Log Likelihood: 34.6626\tSigma2 Prior: -556.9362\tRegularization: 0.0041\n",
            "Iter: 9610  \tTraining Loss: -501.6542    \n",
            "    Negative Log Likelihood: 32.8143\tSigma2 Prior: -534.4726\tRegularization: 0.0041\n",
            "Iter: 9620  \tTraining Loss: -503.2392    \n",
            "    Negative Log Likelihood: 32.8905\tSigma2 Prior: -536.1338\tRegularization: 0.0041\n",
            "Iter: 9630  \tTraining Loss: -523.8976    \n",
            "    Negative Log Likelihood: 32.5311\tSigma2 Prior: -556.4329\tRegularization: 0.0041\n",
            "Iter: 9640  \tTraining Loss: -517.8584    \n",
            "    Negative Log Likelihood: 32.5075\tSigma2 Prior: -550.3700\tRegularization: 0.0041\n",
            "Iter: 9650  \tTraining Loss: -458.8271    \n",
            "    Negative Log Likelihood: 38.6817\tSigma2 Prior: -497.5130\tRegularization: 0.0041\n",
            "Iter: 9660  \tTraining Loss: -469.6096    \n",
            "    Negative Log Likelihood: 33.9651\tSigma2 Prior: -503.5789\tRegularization: 0.0041\n",
            "Iter: 9670  \tTraining Loss: -506.6893    \n",
            "    Negative Log Likelihood: 32.6120\tSigma2 Prior: -539.3055\tRegularization: 0.0042\n",
            "Iter: 9680  \tTraining Loss: -494.5565    \n",
            "    Negative Log Likelihood: 38.8920\tSigma2 Prior: -533.4527\tRegularization: 0.0042\n",
            "Iter: 9690  \tTraining Loss: -524.5930    \n",
            "    Negative Log Likelihood: 31.4490\tSigma2 Prior: -556.0461\tRegularization: 0.0042\n",
            "Iter: 9700  \tTraining Loss: -486.9161    \n",
            "    Negative Log Likelihood: 32.7588\tSigma2 Prior: -519.6791\tRegularization: 0.0042\n",
            "Iter: 9710  \tTraining Loss: -508.8624    \n",
            "    Negative Log Likelihood: 32.5107\tSigma2 Prior: -541.3773\tRegularization: 0.0042\n",
            "Iter: 9720  \tTraining Loss: -505.3989    \n",
            "    Negative Log Likelihood: 33.4629\tSigma2 Prior: -538.8659\tRegularization: 0.0042\n",
            "Iter: 9730  \tTraining Loss: -515.6088    \n",
            "    Negative Log Likelihood: 32.6718\tSigma2 Prior: -548.2848\tRegularization: 0.0042\n",
            "Iter: 9740  \tTraining Loss: -474.3430    \n",
            "    Negative Log Likelihood: 36.1644\tSigma2 Prior: -510.5116\tRegularization: 0.0042\n",
            "Iter: 9750  \tTraining Loss: -484.5689    \n",
            "    Negative Log Likelihood: 32.1295\tSigma2 Prior: -516.7026\tRegularization: 0.0042\n",
            "Iter: 9760  \tTraining Loss: -518.8847    \n",
            "    Negative Log Likelihood: 30.2749\tSigma2 Prior: -549.1638\tRegularization: 0.0042\n",
            "Iter: 9770  \tTraining Loss: -515.8395    \n",
            "    Negative Log Likelihood: 31.3055\tSigma2 Prior: -547.1492\tRegularization: 0.0042\n",
            "Iter: 9780  \tTraining Loss: -444.9104    \n",
            "    Negative Log Likelihood: 36.7799\tSigma2 Prior: -481.6945\tRegularization: 0.0042\n",
            "Iter: 9790  \tTraining Loss: -518.5864    \n",
            "    Negative Log Likelihood: 37.9117\tSigma2 Prior: -556.5023\tRegularization: 0.0042\n",
            "Iter: 9800  \tTraining Loss: -460.6147    \n",
            "    Negative Log Likelihood: 30.2367\tSigma2 Prior: -490.8555\tRegularization: 0.0042\n",
            "Iter: 9810  \tTraining Loss: -514.1949    \n",
            "    Negative Log Likelihood: 29.9270\tSigma2 Prior: -544.1261\tRegularization: 0.0042\n",
            "Iter: 9820  \tTraining Loss: -528.4656    \n",
            "    Negative Log Likelihood: 39.5156\tSigma2 Prior: -567.9855\tRegularization: 0.0042\n",
            "Iter: 9830  \tTraining Loss: -483.7458    \n",
            "    Negative Log Likelihood: 28.0333\tSigma2 Prior: -511.7833\tRegularization: 0.0042\n",
            "Iter: 9840  \tTraining Loss: -504.6455    \n",
            "    Negative Log Likelihood: 31.4533\tSigma2 Prior: -536.1031\tRegularization: 0.0042\n",
            "Iter: 9850  \tTraining Loss: -499.0939    \n",
            "    Negative Log Likelihood: 29.0655\tSigma2 Prior: -528.1636\tRegularization: 0.0042\n",
            "Iter: 9860  \tTraining Loss: -501.2567    \n",
            "    Negative Log Likelihood: 32.5336\tSigma2 Prior: -533.7945\tRegularization: 0.0042\n",
            "Iter: 9870  \tTraining Loss: -513.0191    \n",
            "    Negative Log Likelihood: 36.4196\tSigma2 Prior: -549.4429\tRegularization: 0.0042\n",
            "Iter: 9880  \tTraining Loss: -505.2975    \n",
            "    Negative Log Likelihood: 36.5286\tSigma2 Prior: -541.8303\tRegularization: 0.0042\n",
            "Iter: 9890  \tTraining Loss: -498.4248    \n",
            "    Negative Log Likelihood: 34.7945\tSigma2 Prior: -533.2235\tRegularization: 0.0042\n",
            "Iter: 9900  \tTraining Loss: -475.7958    \n",
            "    Negative Log Likelihood: 29.9863\tSigma2 Prior: -505.7864\tRegularization: 0.0042\n",
            "Iter: 9910  \tTraining Loss: -514.7771    \n",
            "    Negative Log Likelihood: 35.7908\tSigma2 Prior: -550.5721\tRegularization: 0.0042\n",
            "Iter: 9920  \tTraining Loss: -479.1512    \n",
            "    Negative Log Likelihood: 34.9313\tSigma2 Prior: -514.0868\tRegularization: 0.0042\n",
            "Iter: 9930  \tTraining Loss: -490.7903    \n",
            "    Negative Log Likelihood: 31.3833\tSigma2 Prior: -522.1778\tRegularization: 0.0042\n",
            "Iter: 9940  \tTraining Loss: -458.9824    \n",
            "    Negative Log Likelihood: 33.5008\tSigma2 Prior: -492.4874\tRegularization: 0.0042\n",
            "Iter: 9950  \tTraining Loss: -488.5769    \n",
            "    Negative Log Likelihood: 30.4860\tSigma2 Prior: -519.0671\tRegularization: 0.0042\n",
            "Iter: 9960  \tTraining Loss: -468.0071    \n",
            "    Negative Log Likelihood: 34.1622\tSigma2 Prior: -502.1735\tRegularization: 0.0042\n",
            "Iter: 9970  \tTraining Loss: -482.5034    \n",
            "    Negative Log Likelihood: 32.3234\tSigma2 Prior: -514.8310\tRegularization: 0.0042\n",
            "Iter: 9980  \tTraining Loss: -504.3846    \n",
            "    Negative Log Likelihood: 31.4733\tSigma2 Prior: -535.8621\tRegularization: 0.0042\n",
            "Iter: 9990  \tTraining Loss: -483.9328    \n",
            "    Negative Log Likelihood: 41.3815\tSigma2 Prior: -525.3186\tRegularization: 0.0042\n",
            "Iter: 10000  \tTraining Loss: -517.9980    \n",
            "    Negative Log Likelihood: 30.4137\tSigma2 Prior: -548.4160\tRegularization: 0.0042\n",
            "Iter: 10010  \tTraining Loss: -467.3661    \n",
            "    Negative Log Likelihood: 38.6171\tSigma2 Prior: -505.9874\tRegularization: 0.0042\n",
            "Iter: 10020  \tTraining Loss: -500.5913    \n",
            "    Negative Log Likelihood: 33.0041\tSigma2 Prior: -533.5997\tRegularization: 0.0042\n",
            "Iter: 10030  \tTraining Loss: -515.3655    \n",
            "    Negative Log Likelihood: 35.7462\tSigma2 Prior: -551.1159\tRegularization: 0.0043\n",
            "Iter: 10040  \tTraining Loss: -499.6965    \n",
            "    Negative Log Likelihood: 28.8908\tSigma2 Prior: -528.5916\tRegularization: 0.0043\n",
            "Iter: 10050  \tTraining Loss: -508.0270    \n",
            "    Negative Log Likelihood: 35.5858\tSigma2 Prior: -543.6171\tRegularization: 0.0043\n",
            "Iter: 10060  \tTraining Loss: -481.0385    \n",
            "    Negative Log Likelihood: 33.9401\tSigma2 Prior: -514.9829\tRegularization: 0.0043\n",
            "Iter: 10070  \tTraining Loss: -492.0583    \n",
            "    Negative Log Likelihood: 30.4860\tSigma2 Prior: -522.5486\tRegularization: 0.0043\n",
            "Iter: 10080  \tTraining Loss: -509.8518    \n",
            "    Negative Log Likelihood: 31.9624\tSigma2 Prior: -541.8185\tRegularization: 0.0043\n",
            "Iter: 10090  \tTraining Loss: -497.9071    \n",
            "    Negative Log Likelihood: 40.2179\tSigma2 Prior: -538.1293\tRegularization: 0.0043\n",
            "Iter: 10100  \tTraining Loss: -491.3461    \n",
            "    Negative Log Likelihood: 33.1594\tSigma2 Prior: -524.5098\tRegularization: 0.0043\n",
            "Iter: 10110  \tTraining Loss: -446.6462    \n",
            "    Negative Log Likelihood: 33.6185\tSigma2 Prior: -480.2690\tRegularization: 0.0043\n",
            "Iter: 10120  \tTraining Loss: -488.7754    \n",
            "    Negative Log Likelihood: 29.1139\tSigma2 Prior: -517.8936\tRegularization: 0.0043\n",
            "Iter: 10130  \tTraining Loss: -472.5028    \n",
            "    Negative Log Likelihood: 34.2175\tSigma2 Prior: -506.7246\tRegularization: 0.0043\n",
            "Iter: 10140  \tTraining Loss: -474.4346    \n",
            "    Negative Log Likelihood: 35.2522\tSigma2 Prior: -509.6911\tRegularization: 0.0043\n",
            "Iter: 10150  \tTraining Loss: -485.8222    \n",
            "    Negative Log Likelihood: 32.4371\tSigma2 Prior: -518.2636\tRegularization: 0.0043\n",
            "Iter: 10160  \tTraining Loss: -500.3229    \n",
            "    Negative Log Likelihood: 34.8919\tSigma2 Prior: -535.2191\tRegularization: 0.0043\n",
            "Iter: 10170  \tTraining Loss: -489.2004    \n",
            "    Negative Log Likelihood: 34.5229\tSigma2 Prior: -523.7276\tRegularization: 0.0043\n",
            "Iter: 10180  \tTraining Loss: -479.3975    \n",
            "    Negative Log Likelihood: 35.3672\tSigma2 Prior: -514.7690\tRegularization: 0.0043\n",
            "Iter: 10190  \tTraining Loss: -506.0332    \n",
            "    Negative Log Likelihood: 35.7501\tSigma2 Prior: -541.7877\tRegularization: 0.0043\n",
            "Iter: 10200  \tTraining Loss: -500.6697    \n",
            "    Negative Log Likelihood: 23.6424\tSigma2 Prior: -524.3165\tRegularization: 0.0043\n",
            "Iter: 10210  \tTraining Loss: -491.0673    \n",
            "    Negative Log Likelihood: 35.9349\tSigma2 Prior: -527.0065\tRegularization: 0.0043\n",
            "Iter: 10220  \tTraining Loss: -502.1498    \n",
            "    Negative Log Likelihood: 30.2161\tSigma2 Prior: -532.3702\tRegularization: 0.0043\n",
            "Iter: 10230  \tTraining Loss: -491.6728    \n",
            "    Negative Log Likelihood: 30.1159\tSigma2 Prior: -521.7930\tRegularization: 0.0043\n",
            "Iter: 10240  \tTraining Loss: -447.3088    \n",
            "    Negative Log Likelihood: 34.9860\tSigma2 Prior: -482.2991\tRegularization: 0.0043\n",
            "Iter: 10250  \tTraining Loss: -507.3722    \n",
            "    Negative Log Likelihood: 33.1131\tSigma2 Prior: -540.4896\tRegularization: 0.0043\n",
            "Iter: 10260  \tTraining Loss: -487.3668    \n",
            "    Negative Log Likelihood: 35.9566\tSigma2 Prior: -523.3276\tRegularization: 0.0043\n",
            "Iter: 10270  \tTraining Loss: -525.8870    \n",
            "    Negative Log Likelihood: 35.4530\tSigma2 Prior: -561.3443\tRegularization: 0.0043\n",
            "Iter: 10280  \tTraining Loss: -480.5727    \n",
            "    Negative Log Likelihood: 31.6007\tSigma2 Prior: -512.1777\tRegularization: 0.0043\n",
            "Iter: 10290  \tTraining Loss: -442.7156    \n",
            "    Negative Log Likelihood: 37.2651\tSigma2 Prior: -479.9850\tRegularization: 0.0043\n",
            "Iter: 10300  \tTraining Loss: -521.0870    \n",
            "    Negative Log Likelihood: 37.3338\tSigma2 Prior: -558.4252\tRegularization: 0.0043\n",
            "Iter: 10310  \tTraining Loss: -488.6150    \n",
            "    Negative Log Likelihood: 26.8819\tSigma2 Prior: -515.5012\tRegularization: 0.0043\n",
            "Iter: 10320  \tTraining Loss: -504.6250    \n",
            "    Negative Log Likelihood: 31.4245\tSigma2 Prior: -536.0538\tRegularization: 0.0043\n",
            "Iter: 10330  \tTraining Loss: -462.9435    \n",
            "    Negative Log Likelihood: 40.3628\tSigma2 Prior: -503.3106\tRegularization: 0.0043\n",
            "Iter: 10340  \tTraining Loss: -475.9143    \n",
            "    Negative Log Likelihood: 30.0366\tSigma2 Prior: -505.9553\tRegularization: 0.0043\n",
            "Iter: 10350  \tTraining Loss: -469.4458    \n",
            "    Negative Log Likelihood: 37.4764\tSigma2 Prior: -506.9265\tRegularization: 0.0043\n",
            "Iter: 10360  \tTraining Loss: -480.1413    \n",
            "    Negative Log Likelihood: 25.8037\tSigma2 Prior: -505.9494\tRegularization: 0.0043\n",
            "Iter: 10370  \tTraining Loss: -494.0640    \n",
            "    Negative Log Likelihood: 33.2565\tSigma2 Prior: -527.3248\tRegularization: 0.0043\n",
            "Iter: 10380  \tTraining Loss: -470.7002    \n",
            "    Negative Log Likelihood: 37.5764\tSigma2 Prior: -508.2810\tRegularization: 0.0043\n",
            "Iter: 10390  \tTraining Loss: -481.4269    \n",
            "    Negative Log Likelihood: 31.2230\tSigma2 Prior: -512.6543\tRegularization: 0.0044\n",
            "Iter: 10400  \tTraining Loss: -467.5486    \n",
            "    Negative Log Likelihood: 36.6052\tSigma2 Prior: -504.1582\tRegularization: 0.0044\n",
            "Iter: 10410  \tTraining Loss: -507.7377    \n",
            "    Negative Log Likelihood: 33.1199\tSigma2 Prior: -540.8619\tRegularization: 0.0044\n",
            "Iter: 10420  \tTraining Loss: -442.7516    \n",
            "    Negative Log Likelihood: 36.8481\tSigma2 Prior: -479.6041\tRegularization: 0.0044\n",
            "Iter: 10430  \tTraining Loss: -498.6361    \n",
            "    Negative Log Likelihood: 33.6826\tSigma2 Prior: -532.3231\tRegularization: 0.0044\n",
            "Iter: 10440  \tTraining Loss: -484.8396    \n",
            "    Negative Log Likelihood: 34.2732\tSigma2 Prior: -519.1172\tRegularization: 0.0044\n",
            "Iter: 10450  \tTraining Loss: -515.2116    \n",
            "    Negative Log Likelihood: 36.4146\tSigma2 Prior: -551.6306\tRegularization: 0.0044\n",
            "Iter: 10460  \tTraining Loss: -511.0736    \n",
            "    Negative Log Likelihood: 33.2772\tSigma2 Prior: -544.3552\tRegularization: 0.0044\n",
            "Iter: 10470  \tTraining Loss: -496.4012    \n",
            "    Negative Log Likelihood: 30.7800\tSigma2 Prior: -527.1855\tRegularization: 0.0044\n",
            "Iter: 10480  \tTraining Loss: -531.5668    \n",
            "    Negative Log Likelihood: 34.4164\tSigma2 Prior: -565.9876\tRegularization: 0.0044\n",
            "Iter: 10490  \tTraining Loss: -468.1817    \n",
            "    Negative Log Likelihood: 31.9271\tSigma2 Prior: -500.1132\tRegularization: 0.0044\n",
            "Iter: 10500  \tTraining Loss: -496.3938    \n",
            "    Negative Log Likelihood: 29.9892\tSigma2 Prior: -526.3875\tRegularization: 0.0044\n",
            "Iter: 10510  \tTraining Loss: -528.2044    \n",
            "    Negative Log Likelihood: 38.3816\tSigma2 Prior: -566.5904\tRegularization: 0.0044\n",
            "Iter: 10520  \tTraining Loss: -511.5736    \n",
            "    Negative Log Likelihood: 30.8711\tSigma2 Prior: -542.4492\tRegularization: 0.0044\n",
            "Iter: 10530  \tTraining Loss: -478.9001    \n",
            "    Negative Log Likelihood: 33.5350\tSigma2 Prior: -512.4395\tRegularization: 0.0044\n",
            "Iter: 10540  \tTraining Loss: -508.9738    \n",
            "    Negative Log Likelihood: 34.9939\tSigma2 Prior: -543.9721\tRegularization: 0.0044\n",
            "Iter: 10550  \tTraining Loss: -429.1056    \n",
            "    Negative Log Likelihood: 39.9156\tSigma2 Prior: -469.0255\tRegularization: 0.0044\n",
            "Iter: 10560  \tTraining Loss: -481.1207    \n",
            "    Negative Log Likelihood: 27.2723\tSigma2 Prior: -508.3974\tRegularization: 0.0044\n",
            "Iter: 10570  \tTraining Loss: -467.9774    \n",
            "    Negative Log Likelihood: 29.5295\tSigma2 Prior: -497.5113\tRegularization: 0.0044\n",
            "Iter: 10580  \tTraining Loss: -524.0236    \n",
            "    Negative Log Likelihood: 33.1637\tSigma2 Prior: -557.1917\tRegularization: 0.0044\n",
            "Iter: 10590  \tTraining Loss: -457.9009    \n",
            "    Negative Log Likelihood: 33.0530\tSigma2 Prior: -490.9583\tRegularization: 0.0044\n",
            "Iter: 10600  \tTraining Loss: -489.4845    \n",
            "    Negative Log Likelihood: 28.7418\tSigma2 Prior: -518.2307\tRegularization: 0.0044\n",
            "Iter: 10610  \tTraining Loss: -475.0614    \n",
            "    Negative Log Likelihood: 32.2725\tSigma2 Prior: -507.3383\tRegularization: 0.0044\n",
            "Iter: 10620  \tTraining Loss: -522.7546    \n",
            "    Negative Log Likelihood: 35.8670\tSigma2 Prior: -558.6260\tRegularization: 0.0044\n",
            "Iter: 10630  \tTraining Loss: -485.2930    \n",
            "    Negative Log Likelihood: 26.4674\tSigma2 Prior: -511.7649\tRegularization: 0.0044\n",
            "Iter: 10640  \tTraining Loss: -509.0835    \n",
            "    Negative Log Likelihood: 34.6324\tSigma2 Prior: -543.7203\tRegularization: 0.0044\n",
            "Iter: 10650  \tTraining Loss: -500.4488    \n",
            "    Negative Log Likelihood: 32.8800\tSigma2 Prior: -533.3333\tRegularization: 0.0044\n",
            "Iter: 10660  \tTraining Loss: -495.8734    \n",
            "    Negative Log Likelihood: 30.7055\tSigma2 Prior: -526.5833\tRegularization: 0.0044\n",
            "Iter: 10670  \tTraining Loss: -462.9417    \n",
            "    Negative Log Likelihood: 32.1677\tSigma2 Prior: -495.1138\tRegularization: 0.0044\n",
            "Iter: 10680  \tTraining Loss: -468.4550    \n",
            "    Negative Log Likelihood: 31.7305\tSigma2 Prior: -500.1900\tRegularization: 0.0044\n",
            "Iter: 10690  \tTraining Loss: -527.6273    \n",
            "    Negative Log Likelihood: 39.1749\tSigma2 Prior: -566.8066\tRegularization: 0.0044\n",
            "Iter: 10700  \tTraining Loss: -514.1127    \n",
            "    Negative Log Likelihood: 31.6336\tSigma2 Prior: -545.7507\tRegularization: 0.0044\n",
            "Iter: 10710  \tTraining Loss: -514.1072    \n",
            "    Negative Log Likelihood: 32.6725\tSigma2 Prior: -546.7842\tRegularization: 0.0044\n",
            "Iter: 10720  \tTraining Loss: -484.0026    \n",
            "    Negative Log Likelihood: 32.2993\tSigma2 Prior: -516.3064\tRegularization: 0.0044\n",
            "Iter: 10730  \tTraining Loss: -506.1342    \n",
            "    Negative Log Likelihood: 33.9262\tSigma2 Prior: -540.0649\tRegularization: 0.0044\n",
            "Iter: 10740  \tTraining Loss: -481.4407    \n",
            "    Negative Log Likelihood: 33.7884\tSigma2 Prior: -515.2335\tRegularization: 0.0044\n",
            "Iter: 10750  \tTraining Loss: -450.5966    \n",
            "    Negative Log Likelihood: 33.1795\tSigma2 Prior: -483.7805\tRegularization: 0.0044\n",
            "Iter: 10760  \tTraining Loss: -479.3442    \n",
            "    Negative Log Likelihood: 36.0952\tSigma2 Prior: -515.4439\tRegularization: 0.0044\n",
            "Iter: 10770  \tTraining Loss: -508.3801    \n",
            "    Negative Log Likelihood: 28.7692\tSigma2 Prior: -537.1537\tRegularization: 0.0045\n",
            "Iter: 10780  \tTraining Loss: -485.2624    \n",
            "    Negative Log Likelihood: 35.0660\tSigma2 Prior: -520.3328\tRegularization: 0.0045\n",
            "Iter: 10790  \tTraining Loss: -491.6579    \n",
            "    Negative Log Likelihood: 32.2033\tSigma2 Prior: -523.8657\tRegularization: 0.0045\n",
            "Iter: 10800  \tTraining Loss: -511.6487    \n",
            "    Negative Log Likelihood: 37.3801\tSigma2 Prior: -549.0333\tRegularization: 0.0045\n",
            "Iter: 10810  \tTraining Loss: -447.9785    \n",
            "    Negative Log Likelihood: 33.4321\tSigma2 Prior: -481.4151\tRegularization: 0.0045\n",
            "Iter: 10820  \tTraining Loss: -509.3173    \n",
            "    Negative Log Likelihood: 32.8556\tSigma2 Prior: -542.1774\tRegularization: 0.0045\n",
            "Iter: 10830  \tTraining Loss: -489.5088    \n",
            "    Negative Log Likelihood: 38.6216\tSigma2 Prior: -528.1348\tRegularization: 0.0045\n",
            "Iter: 10840  \tTraining Loss: -507.1892    \n",
            "    Negative Log Likelihood: 33.4554\tSigma2 Prior: -540.6490\tRegularization: 0.0045\n",
            "Iter: 10850  \tTraining Loss: -468.2463    \n",
            "    Negative Log Likelihood: 28.5764\tSigma2 Prior: -496.8272\tRegularization: 0.0045\n",
            "Iter: 10860  \tTraining Loss: -481.8938    \n",
            "    Negative Log Likelihood: 33.7634\tSigma2 Prior: -515.6617\tRegularization: 0.0045\n",
            "Iter: 10870  \tTraining Loss: -518.4629    \n",
            "    Negative Log Likelihood: 30.9831\tSigma2 Prior: -549.4505\tRegularization: 0.0045\n",
            "Iter: 10880  \tTraining Loss: -497.6223    \n",
            "    Negative Log Likelihood: 37.3083\tSigma2 Prior: -534.9351\tRegularization: 0.0045\n",
            "Iter: 10890  \tTraining Loss: -461.8328    \n",
            "    Negative Log Likelihood: 30.4955\tSigma2 Prior: -492.3328\tRegularization: 0.0045\n",
            "Iter: 10900  \tTraining Loss: -464.4102    \n",
            "    Negative Log Likelihood: 30.8331\tSigma2 Prior: -495.2478\tRegularization: 0.0045\n",
            "Iter: 10910  \tTraining Loss: -491.5721    \n",
            "    Negative Log Likelihood: 33.5332\tSigma2 Prior: -525.1099\tRegularization: 0.0045\n",
            "Iter: 10920  \tTraining Loss: -515.2377    \n",
            "    Negative Log Likelihood: 31.6185\tSigma2 Prior: -546.8608\tRegularization: 0.0045\n",
            "Iter: 10930  \tTraining Loss: -501.6217    \n",
            "    Negative Log Likelihood: 33.2996\tSigma2 Prior: -534.9258\tRegularization: 0.0045\n",
            "Iter: 10940  \tTraining Loss: -492.4630    \n",
            "    Negative Log Likelihood: 29.0298\tSigma2 Prior: -521.4973\tRegularization: 0.0045\n",
            "Iter: 10950  \tTraining Loss: -512.7888    \n",
            "    Negative Log Likelihood: 36.7268\tSigma2 Prior: -549.5201\tRegularization: 0.0045\n",
            "Iter: 10960  \tTraining Loss: -474.1461    \n",
            "    Negative Log Likelihood: 33.3227\tSigma2 Prior: -507.4732\tRegularization: 0.0045\n",
            "Iter: 10970  \tTraining Loss: -500.6784    \n",
            "    Negative Log Likelihood: 27.3414\tSigma2 Prior: -528.0243\tRegularization: 0.0045\n",
            "Iter: 10980  \tTraining Loss: -523.9852    \n",
            "    Negative Log Likelihood: 36.1807\tSigma2 Prior: -560.1704\tRegularization: 0.0045\n",
            "Iter: 10990  \tTraining Loss: -492.2453    \n",
            "    Negative Log Likelihood: 32.4782\tSigma2 Prior: -524.7280\tRegularization: 0.0045\n",
            "Iter: 11000  \tTraining Loss: -501.5818    \n",
            "    Negative Log Likelihood: 29.7825\tSigma2 Prior: -531.3689\tRegularization: 0.0045\n",
            "Iter: 11010  \tTraining Loss: -521.3726    \n",
            "    Negative Log Likelihood: 33.7482\tSigma2 Prior: -555.1253\tRegularization: 0.0045\n",
            "Iter: 11020  \tTraining Loss: -464.8477    \n",
            "    Negative Log Likelihood: 37.3439\tSigma2 Prior: -502.1962\tRegularization: 0.0045\n",
            "Iter: 11030  \tTraining Loss: -505.1862    \n",
            "    Negative Log Likelihood: 25.7115\tSigma2 Prior: -530.9023\tRegularization: 0.0045\n",
            "Iter: 11040  \tTraining Loss: -502.4289    \n",
            "    Negative Log Likelihood: 31.3871\tSigma2 Prior: -533.8205\tRegularization: 0.0045\n",
            "Iter: 11050  \tTraining Loss: -471.5873    \n",
            "    Negative Log Likelihood: 34.9879\tSigma2 Prior: -506.5797\tRegularization: 0.0045\n",
            "Iter: 11060  \tTraining Loss: -492.3727    \n",
            "    Negative Log Likelihood: 30.6149\tSigma2 Prior: -522.9921\tRegularization: 0.0045\n",
            "Iter: 11070  \tTraining Loss: -502.5784    \n",
            "    Negative Log Likelihood: 35.0110\tSigma2 Prior: -537.5939\tRegularization: 0.0045\n",
            "Iter: 11080  \tTraining Loss: -509.9600    \n",
            "    Negative Log Likelihood: 31.9681\tSigma2 Prior: -541.9326\tRegularization: 0.0045\n",
            "Iter: 11090  \tTraining Loss: -522.2140    \n",
            "    Negative Log Likelihood: 38.8184\tSigma2 Prior: -561.0369\tRegularization: 0.0045\n",
            "Iter: 11100  \tTraining Loss: -507.0132    \n",
            "    Negative Log Likelihood: 34.4663\tSigma2 Prior: -541.4840\tRegularization: 0.0045\n",
            "Iter: 11110  \tTraining Loss: -493.4383    \n",
            "    Negative Log Likelihood: 35.7751\tSigma2 Prior: -529.2180\tRegularization: 0.0045\n",
            "Iter: 11120  \tTraining Loss: -498.3021    \n",
            "    Negative Log Likelihood: 32.6100\tSigma2 Prior: -530.9167\tRegularization: 0.0045\n",
            "Iter: 11130  \tTraining Loss: -511.2947    \n",
            "    Negative Log Likelihood: 34.4175\tSigma2 Prior: -545.7167\tRegularization: 0.0045\n",
            "Iter: 11140  \tTraining Loss: -496.6962    \n",
            "    Negative Log Likelihood: 35.1641\tSigma2 Prior: -531.8649\tRegularization: 0.0045\n",
            "Iter: 11150  \tTraining Loss: -473.1754    \n",
            "    Negative Log Likelihood: 30.8399\tSigma2 Prior: -504.0199\tRegularization: 0.0046\n",
            "Iter: 11160  \tTraining Loss: -453.4353    \n",
            "    Negative Log Likelihood: 34.1666\tSigma2 Prior: -487.6065\tRegularization: 0.0046\n",
            "Iter: 11170  \tTraining Loss: -496.5319    \n",
            "    Negative Log Likelihood: 28.6811\tSigma2 Prior: -525.2175\tRegularization: 0.0046\n",
            "Iter: 11180  \tTraining Loss: -476.3629    \n",
            "    Negative Log Likelihood: 33.7113\tSigma2 Prior: -510.0788\tRegularization: 0.0046\n",
            "Iter: 11190  \tTraining Loss: -485.0014    \n",
            "    Negative Log Likelihood: 35.7497\tSigma2 Prior: -520.7556\tRegularization: 0.0046\n",
            "Iter: 11200  \tTraining Loss: -495.9725    \n",
            "    Negative Log Likelihood: 31.6996\tSigma2 Prior: -527.6766\tRegularization: 0.0046\n",
            "Iter: 11210  \tTraining Loss: -486.7628    \n",
            "    Negative Log Likelihood: 34.8705\tSigma2 Prior: -521.6379\tRegularization: 0.0046\n",
            "Iter: 11220  \tTraining Loss: -475.2199    \n",
            "    Negative Log Likelihood: 31.7843\tSigma2 Prior: -507.0088\tRegularization: 0.0046\n",
            "Iter: 11230  \tTraining Loss: -494.1593    \n",
            "    Negative Log Likelihood: 32.4708\tSigma2 Prior: -526.6346\tRegularization: 0.0046\n",
            "Iter: 11240  \tTraining Loss: -498.5866    \n",
            "    Negative Log Likelihood: 33.0903\tSigma2 Prior: -531.6815\tRegularization: 0.0046\n",
            "Iter: 11250  \tTraining Loss: -476.1417    \n",
            "    Negative Log Likelihood: 38.8907\tSigma2 Prior: -515.0370\tRegularization: 0.0046\n",
            "Iter: 11260  \tTraining Loss: -491.4992    \n",
            "    Negative Log Likelihood: 27.4558\tSigma2 Prior: -518.9596\tRegularization: 0.0046\n",
            "Iter: 11270  \tTraining Loss: -521.8949    \n",
            "    Negative Log Likelihood: 36.5337\tSigma2 Prior: -558.4332\tRegularization: 0.0046\n",
            "Iter: 11280  \tTraining Loss: -486.9850    \n",
            "    Negative Log Likelihood: 33.2278\tSigma2 Prior: -520.2174\tRegularization: 0.0046\n",
            "Iter: 11290  \tTraining Loss: -495.9446    \n",
            "    Negative Log Likelihood: 36.2335\tSigma2 Prior: -532.1827\tRegularization: 0.0046\n",
            "Iter: 11300  \tTraining Loss: -495.0653    \n",
            "    Negative Log Likelihood: 35.0781\tSigma2 Prior: -530.1481\tRegularization: 0.0046\n",
            "Iter: 11310  \tTraining Loss: -522.1763    \n",
            "    Negative Log Likelihood: 30.7183\tSigma2 Prior: -552.8992\tRegularization: 0.0046\n",
            "Iter: 11320  \tTraining Loss: -490.0584    \n",
            "    Negative Log Likelihood: 30.8171\tSigma2 Prior: -520.8802\tRegularization: 0.0046\n",
            "Iter: 11330  \tTraining Loss: -468.3971    \n",
            "    Negative Log Likelihood: 36.0531\tSigma2 Prior: -504.4549\tRegularization: 0.0046\n",
            "Iter: 11340  \tTraining Loss: -470.9167    \n",
            "    Negative Log Likelihood: 35.9390\tSigma2 Prior: -506.8603\tRegularization: 0.0046\n",
            "Iter: 11350  \tTraining Loss: -526.1185    \n",
            "    Negative Log Likelihood: 27.0519\tSigma2 Prior: -553.1749\tRegularization: 0.0046\n",
            "Iter: 11360  \tTraining Loss: -480.7871    \n",
            "    Negative Log Likelihood: 42.7262\tSigma2 Prior: -523.5179\tRegularization: 0.0046\n",
            "Iter: 11370  \tTraining Loss: -520.3541    \n",
            "    Negative Log Likelihood: 28.6288\tSigma2 Prior: -548.9875\tRegularization: 0.0046\n",
            "Iter: 11380  \tTraining Loss: -481.9596    \n",
            "    Negative Log Likelihood: 31.1666\tSigma2 Prior: -513.1307\tRegularization: 0.0046\n",
            "Iter: 11390  \tTraining Loss: -514.6288    \n",
            "    Negative Log Likelihood: 35.4738\tSigma2 Prior: -550.1073\tRegularization: 0.0046\n",
            "Iter: 11400  \tTraining Loss: -509.7965    \n",
            "    Negative Log Likelihood: 32.5639\tSigma2 Prior: -542.3650\tRegularization: 0.0046\n",
            "Iter: 11410  \tTraining Loss: -464.0390    \n",
            "    Negative Log Likelihood: 33.2710\tSigma2 Prior: -497.3146\tRegularization: 0.0046\n",
            "Iter: 11420  \tTraining Loss: -499.1971    \n",
            "    Negative Log Likelihood: 33.0067\tSigma2 Prior: -532.2083\tRegularization: 0.0046\n",
            "Iter: 11430  \tTraining Loss: -425.1661    \n",
            "    Negative Log Likelihood: 32.1500\tSigma2 Prior: -457.3207\tRegularization: 0.0046\n",
            "Iter: 11440  \tTraining Loss: -498.3554    \n",
            "    Negative Log Likelihood: 28.0471\tSigma2 Prior: -526.4071\tRegularization: 0.0046\n",
            "Iter: 11450  \tTraining Loss: -488.8822    \n",
            "    Negative Log Likelihood: 37.3391\tSigma2 Prior: -526.2260\tRegularization: 0.0046\n",
            "Iter: 11460  \tTraining Loss: -520.0322    \n",
            "    Negative Log Likelihood: 33.1322\tSigma2 Prior: -553.1691\tRegularization: 0.0046\n",
            "Iter: 11470  \tTraining Loss: -454.3362    \n",
            "    Negative Log Likelihood: 34.4357\tSigma2 Prior: -488.7766\tRegularization: 0.0046\n",
            "Iter: 11480  \tTraining Loss: -489.0394    \n",
            "    Negative Log Likelihood: 27.8596\tSigma2 Prior: -516.9036\tRegularization: 0.0046\n",
            "Iter: 11490  \tTraining Loss: -508.2418    \n",
            "    Negative Log Likelihood: 37.6475\tSigma2 Prior: -545.8940\tRegularization: 0.0046\n",
            "Iter: 11500  \tTraining Loss: -480.2424    \n",
            "    Negative Log Likelihood: 35.9680\tSigma2 Prior: -516.2150\tRegularization: 0.0046\n",
            "Iter: 11510  \tTraining Loss: -493.9931    \n",
            "    Negative Log Likelihood: 30.9753\tSigma2 Prior: -524.9730\tRegularization: 0.0046\n",
            "Iter: 11520  \tTraining Loss: -495.2211    \n",
            "    Negative Log Likelihood: 32.3733\tSigma2 Prior: -527.5991\tRegularization: 0.0046\n",
            "Iter: 11530  \tTraining Loss: -496.6140    \n",
            "    Negative Log Likelihood: 38.4393\tSigma2 Prior: -535.0579\tRegularization: 0.0047\n",
            "Iter: 11540  \tTraining Loss: -489.9397    \n",
            "    Negative Log Likelihood: 29.3270\tSigma2 Prior: -519.2714\tRegularization: 0.0047\n",
            "Iter: 11550  \tTraining Loss: -490.0267    \n",
            "    Negative Log Likelihood: 35.0816\tSigma2 Prior: -525.1130\tRegularization: 0.0047\n",
            "Iter: 11560  \tTraining Loss: -504.8938    \n",
            "    Negative Log Likelihood: 30.1591\tSigma2 Prior: -535.0576\tRegularization: 0.0047\n",
            "Iter: 11570  \tTraining Loss: -497.6029    \n",
            "    Negative Log Likelihood: 33.4044\tSigma2 Prior: -531.0120\tRegularization: 0.0047\n",
            "Iter: 11580  \tTraining Loss: -429.4309    \n",
            "    Negative Log Likelihood: 36.3320\tSigma2 Prior: -465.7676\tRegularization: 0.0047\n",
            "Iter: 11590  \tTraining Loss: -502.3559    \n",
            "    Negative Log Likelihood: 31.0834\tSigma2 Prior: -533.4440\tRegularization: 0.0047\n",
            "Iter: 11600  \tTraining Loss: -514.3561    \n",
            "    Negative Log Likelihood: 33.5225\tSigma2 Prior: -547.8832\tRegularization: 0.0047\n",
            "Iter: 11610  \tTraining Loss: -510.1844    \n",
            "    Negative Log Likelihood: 31.0254\tSigma2 Prior: -541.2145\tRegularization: 0.0047\n",
            "Iter: 11620  \tTraining Loss: -505.3743    \n",
            "    Negative Log Likelihood: 31.2714\tSigma2 Prior: -536.6504\tRegularization: 0.0047\n",
            "Iter: 11630  \tTraining Loss: -485.3163    \n",
            "    Negative Log Likelihood: 37.5651\tSigma2 Prior: -522.8861\tRegularization: 0.0047\n",
            "Iter: 11640  \tTraining Loss: -505.8907    \n",
            "    Negative Log Likelihood: 33.4538\tSigma2 Prior: -539.3491\tRegularization: 0.0047\n",
            "Iter: 11650  \tTraining Loss: -478.1404    \n",
            "    Negative Log Likelihood: 42.9523\tSigma2 Prior: -521.0974\tRegularization: 0.0047\n",
            "Iter: 11660  \tTraining Loss: -494.9251    \n",
            "    Negative Log Likelihood: 29.1302\tSigma2 Prior: -524.0600\tRegularization: 0.0047\n",
            "Iter: 11670  \tTraining Loss: -506.7961    \n",
            "    Negative Log Likelihood: 34.0746\tSigma2 Prior: -540.8754\tRegularization: 0.0047\n",
            "Iter: 11680  \tTraining Loss: -522.1038    \n",
            "    Negative Log Likelihood: 33.4643\tSigma2 Prior: -555.5728\tRegularization: 0.0047\n",
            "Iter: 11690  \tTraining Loss: -508.5131    \n",
            "    Negative Log Likelihood: 27.4875\tSigma2 Prior: -536.0053\tRegularization: 0.0047\n",
            "Iter: 11700  \tTraining Loss: -495.9665    \n",
            "    Negative Log Likelihood: 30.5029\tSigma2 Prior: -526.4741\tRegularization: 0.0047\n",
            "Iter: 11710  \tTraining Loss: -494.7762    \n",
            "    Negative Log Likelihood: 34.0274\tSigma2 Prior: -528.8083\tRegularization: 0.0047\n",
            "Iter: 11720  \tTraining Loss: -468.1686    \n",
            "    Negative Log Likelihood: 34.0296\tSigma2 Prior: -502.2029\tRegularization: 0.0047\n",
            "Iter: 11730  \tTraining Loss: -516.1410    \n",
            "    Negative Log Likelihood: 33.0751\tSigma2 Prior: -549.2208\tRegularization: 0.0047\n",
            "Iter: 11740  \tTraining Loss: -488.5107    \n",
            "    Negative Log Likelihood: 32.0699\tSigma2 Prior: -520.5853\tRegularization: 0.0047\n",
            "Iter: 11750  \tTraining Loss: -506.9086    \n",
            "    Negative Log Likelihood: 31.8166\tSigma2 Prior: -538.7300\tRegularization: 0.0047\n",
            "Iter: 11760  \tTraining Loss: -506.9672    \n",
            "    Negative Log Likelihood: 26.6344\tSigma2 Prior: -533.6063\tRegularization: 0.0047\n",
            "Iter: 11770  \tTraining Loss: -515.8177    \n",
            "    Negative Log Likelihood: 34.7049\tSigma2 Prior: -550.5273\tRegularization: 0.0047\n",
            "Iter: 11780  \tTraining Loss: -515.2222    \n",
            "    Negative Log Likelihood: 32.5369\tSigma2 Prior: -547.7637\tRegularization: 0.0047\n",
            "Iter: 11790  \tTraining Loss: -505.2499    \n",
            "    Negative Log Likelihood: 28.0792\tSigma2 Prior: -533.3339\tRegularization: 0.0047\n",
            "Iter: 11800  \tTraining Loss: -488.8286    \n",
            "    Negative Log Likelihood: 37.5862\tSigma2 Prior: -526.4196\tRegularization: 0.0047\n",
            "Iter: 11810  \tTraining Loss: -482.5768    \n",
            "    Negative Log Likelihood: 31.1887\tSigma2 Prior: -513.7701\tRegularization: 0.0047\n",
            "Iter: 11820  \tTraining Loss: -461.9832    \n",
            "    Negative Log Likelihood: 32.4216\tSigma2 Prior: -494.4095\tRegularization: 0.0047\n",
            "Iter: 11830  \tTraining Loss: -504.0819    \n",
            "    Negative Log Likelihood: 31.7729\tSigma2 Prior: -535.8596\tRegularization: 0.0047\n",
            "Iter: 11840  \tTraining Loss: -471.3971    \n",
            "    Negative Log Likelihood: 45.2456\tSigma2 Prior: -516.6473\tRegularization: 0.0047\n",
            "Iter: 11850  \tTraining Loss: -500.5918    \n",
            "    Negative Log Likelihood: 25.7100\tSigma2 Prior: -526.3065\tRegularization: 0.0047\n",
            "Iter: 11860  \tTraining Loss: -451.0616    \n",
            "    Negative Log Likelihood: 37.2319\tSigma2 Prior: -488.2982\tRegularization: 0.0047\n",
            "Iter: 11870  \tTraining Loss: -511.3942    \n",
            "    Negative Log Likelihood: 34.0566\tSigma2 Prior: -545.4555\tRegularization: 0.0047\n",
            "Iter: 11880  \tTraining Loss: -496.2595    \n",
            "    Negative Log Likelihood: 35.8561\tSigma2 Prior: -532.1204\tRegularization: 0.0047\n",
            "Iter: 11890  \tTraining Loss: -473.1239    \n",
            "    Negative Log Likelihood: 33.9782\tSigma2 Prior: -507.1069\tRegularization: 0.0047\n",
            "Iter: 11900  \tTraining Loss: -486.0361    \n",
            "    Negative Log Likelihood: 33.0586\tSigma2 Prior: -519.0994\tRegularization: 0.0047\n",
            "Iter: 11910  \tTraining Loss: -483.2836    \n",
            "    Negative Log Likelihood: 35.0650\tSigma2 Prior: -518.3534\tRegularization: 0.0047\n",
            "Iter: 11920  \tTraining Loss: -484.9164    \n",
            "    Negative Log Likelihood: 34.4816\tSigma2 Prior: -519.4027\tRegularization: 0.0047\n",
            "Iter: 11930  \tTraining Loss: -464.8788    \n",
            "    Negative Log Likelihood: 35.5181\tSigma2 Prior: -500.4017\tRegularization: 0.0048\n",
            "Iter: 11940  \tTraining Loss: -463.9867    \n",
            "    Negative Log Likelihood: 26.2402\tSigma2 Prior: -490.2317\tRegularization: 0.0048\n",
            "Iter: 11950  \tTraining Loss: -485.4064    \n",
            "    Negative Log Likelihood: 36.8776\tSigma2 Prior: -522.2888\tRegularization: 0.0048\n",
            "Iter: 11960  \tTraining Loss: -487.3015    \n",
            "    Negative Log Likelihood: 34.1051\tSigma2 Prior: -521.4113\tRegularization: 0.0048\n",
            "Iter: 11970  \tTraining Loss: -479.3224    \n",
            "    Negative Log Likelihood: 26.6295\tSigma2 Prior: -505.9566\tRegularization: 0.0048\n",
            "Iter: 11980  \tTraining Loss: -489.2094    \n",
            "    Negative Log Likelihood: 36.0069\tSigma2 Prior: -525.2211\tRegularization: 0.0048\n",
            "Iter: 11990  \tTraining Loss: -498.9163    \n",
            "    Negative Log Likelihood: 30.8924\tSigma2 Prior: -529.8134\tRegularization: 0.0048\n",
            "Iter: 12000  \tTraining Loss: -501.3114    \n",
            "    Negative Log Likelihood: 34.3484\tSigma2 Prior: -535.6646\tRegularization: 0.0048\n",
            "Iter: 12010  \tTraining Loss: -499.1022    \n",
            "    Negative Log Likelihood: 36.9479\tSigma2 Prior: -536.0548\tRegularization: 0.0048\n",
            "Iter: 12020  \tTraining Loss: -478.4966    \n",
            "    Negative Log Likelihood: 25.6274\tSigma2 Prior: -504.1288\tRegularization: 0.0048\n",
            "Iter: 12030  \tTraining Loss: -508.0562    \n",
            "    Negative Log Likelihood: 30.7256\tSigma2 Prior: -538.7866\tRegularization: 0.0048\n",
            "Iter: 12040  \tTraining Loss: -496.4773    \n",
            "    Negative Log Likelihood: 32.9784\tSigma2 Prior: -529.4605\tRegularization: 0.0048\n",
            "Iter: 12050  \tTraining Loss: -490.2005    \n",
            "    Negative Log Likelihood: 32.0561\tSigma2 Prior: -522.2614\tRegularization: 0.0048\n",
            "Iter: 12060  \tTraining Loss: -495.5939    \n",
            "    Negative Log Likelihood: 33.1705\tSigma2 Prior: -528.7692\tRegularization: 0.0048\n",
            "Iter: 12070  \tTraining Loss: -504.2734    \n",
            "    Negative Log Likelihood: 30.6828\tSigma2 Prior: -534.9610\tRegularization: 0.0048\n",
            "Iter: 12080  \tTraining Loss: -485.3958    \n",
            "    Negative Log Likelihood: 35.7203\tSigma2 Prior: -521.1209\tRegularization: 0.0048\n",
            "Iter: 12090  \tTraining Loss: -482.5161    \n",
            "    Negative Log Likelihood: 29.4068\tSigma2 Prior: -511.9277\tRegularization: 0.0048\n",
            "Iter: 12100  \tTraining Loss: -522.5801    \n",
            "    Negative Log Likelihood: 30.2007\tSigma2 Prior: -552.7856\tRegularization: 0.0048\n",
            "Iter: 12110  \tTraining Loss: -504.2083    \n",
            "    Negative Log Likelihood: 36.3188\tSigma2 Prior: -540.5319\tRegularization: 0.0048\n",
            "Iter: 12120  \tTraining Loss: -476.0067    \n",
            "    Negative Log Likelihood: 36.3858\tSigma2 Prior: -512.3972\tRegularization: 0.0048\n",
            "Iter: 12130  \tTraining Loss: -490.5494    \n",
            "    Negative Log Likelihood: 30.8151\tSigma2 Prior: -521.3693\tRegularization: 0.0048\n",
            "Iter: 12140  \tTraining Loss: -485.1736    \n",
            "    Negative Log Likelihood: 27.1272\tSigma2 Prior: -512.3056\tRegularization: 0.0048\n",
            "Iter: 12150  \tTraining Loss: -484.0268    \n",
            "    Negative Log Likelihood: 37.6940\tSigma2 Prior: -521.7256\tRegularization: 0.0048\n",
            "Iter: 12160  \tTraining Loss: -525.6695    \n",
            "    Negative Log Likelihood: 31.9064\tSigma2 Prior: -557.5807\tRegularization: 0.0048\n",
            "Iter: 12170  \tTraining Loss: -509.2347    \n",
            "    Negative Log Likelihood: 29.7867\tSigma2 Prior: -539.0262\tRegularization: 0.0048\n",
            "Iter: 12180  \tTraining Loss: -511.1464    \n",
            "    Negative Log Likelihood: 35.4597\tSigma2 Prior: -546.6109\tRegularization: 0.0048\n",
            "Iter: 12190  \tTraining Loss: -481.6329    \n",
            "    Negative Log Likelihood: 29.9722\tSigma2 Prior: -511.6099\tRegularization: 0.0048\n",
            "Iter: 12200  \tTraining Loss: -513.3478    \n",
            "    Negative Log Likelihood: 31.7356\tSigma2 Prior: -545.0883\tRegularization: 0.0048\n",
            "Iter: 12210  \tTraining Loss: -486.3021    \n",
            "    Negative Log Likelihood: 32.8990\tSigma2 Prior: -519.2059\tRegularization: 0.0048\n",
            "Iter: 12220  \tTraining Loss: -468.1997    \n",
            "    Negative Log Likelihood: 30.5042\tSigma2 Prior: -498.7087\tRegularization: 0.0048\n",
            "Iter: 12230  \tTraining Loss: -489.1294    \n",
            "    Negative Log Likelihood: 35.2384\tSigma2 Prior: -524.3727\tRegularization: 0.0048\n",
            "Iter: 12240  \tTraining Loss: -453.9930    \n",
            "    Negative Log Likelihood: 33.6698\tSigma2 Prior: -487.6677\tRegularization: 0.0048\n",
            "Iter: 12250  \tTraining Loss: -495.3350    \n",
            "    Negative Log Likelihood: 31.7795\tSigma2 Prior: -527.1193\tRegularization: 0.0048\n",
            "Iter: 12260  \tTraining Loss: -449.9985    \n",
            "    Negative Log Likelihood: 35.0858\tSigma2 Prior: -485.0892\tRegularization: 0.0048\n",
            "Iter: 12270  \tTraining Loss: -501.8018    \n",
            "    Negative Log Likelihood: 32.2267\tSigma2 Prior: -534.0333\tRegularization: 0.0048\n",
            "Iter: 12280  \tTraining Loss: -478.1762    \n",
            "    Negative Log Likelihood: 29.7123\tSigma2 Prior: -507.8933\tRegularization: 0.0048\n",
            "Iter: 12290  \tTraining Loss: -485.6342    \n",
            "    Negative Log Likelihood: 28.8556\tSigma2 Prior: -514.4946\tRegularization: 0.0048\n",
            "Iter: 12300  \tTraining Loss: -484.8790    \n",
            "    Negative Log Likelihood: 39.1102\tSigma2 Prior: -523.9940\tRegularization: 0.0048\n",
            "Iter: 12310  \tTraining Loss: -487.1698    \n",
            "    Negative Log Likelihood: 34.4561\tSigma2 Prior: -521.6307\tRegularization: 0.0048\n",
            "Iter: 12320  \tTraining Loss: -507.6369    \n",
            "    Negative Log Likelihood: 32.5838\tSigma2 Prior: -540.2255\tRegularization: 0.0048\n",
            "Iter: 12330  \tTraining Loss: -451.5568    \n",
            "    Negative Log Likelihood: 34.4702\tSigma2 Prior: -486.0319\tRegularization: 0.0049\n",
            "Iter: 12340  \tTraining Loss: -507.1824    \n",
            "    Negative Log Likelihood: 29.3610\tSigma2 Prior: -536.5483\tRegularization: 0.0049\n",
            "Iter: 12350  \tTraining Loss: -494.0704    \n",
            "    Negative Log Likelihood: 36.6000\tSigma2 Prior: -530.6753\tRegularization: 0.0049\n",
            "Iter: 12360  \tTraining Loss: -466.9660    \n",
            "    Negative Log Likelihood: 35.7037\tSigma2 Prior: -502.6746\tRegularization: 0.0049\n",
            "Iter: 12370  \tTraining Loss: -482.1450    \n",
            "    Negative Log Likelihood: 31.4645\tSigma2 Prior: -513.6144\tRegularization: 0.0049\n",
            "Iter: 12380  \tTraining Loss: -508.5543    \n",
            "    Negative Log Likelihood: 33.3682\tSigma2 Prior: -541.9273\tRegularization: 0.0049\n",
            "Iter: 12390  \tTraining Loss: -497.7355    \n",
            "    Negative Log Likelihood: 33.6158\tSigma2 Prior: -531.3562\tRegularization: 0.0049\n",
            "Iter: 12400  \tTraining Loss: -500.1400    \n",
            "    Negative Log Likelihood: 37.0185\tSigma2 Prior: -537.1635\tRegularization: 0.0049\n",
            "Iter: 12410  \tTraining Loss: -497.1618    \n",
            "    Negative Log Likelihood: 27.7427\tSigma2 Prior: -524.9094\tRegularization: 0.0049\n",
            "Iter: 12420  \tTraining Loss: -500.0159    \n",
            "    Negative Log Likelihood: 33.2383\tSigma2 Prior: -533.2591\tRegularization: 0.0049\n",
            "Iter: 12430  \tTraining Loss: -491.2826    \n",
            "    Negative Log Likelihood: 37.3760\tSigma2 Prior: -528.6635\tRegularization: 0.0049\n",
            "Iter: 12440  \tTraining Loss: -506.4718    \n",
            "    Negative Log Likelihood: 28.1567\tSigma2 Prior: -534.6334\tRegularization: 0.0049\n",
            "Iter: 12450  \tTraining Loss: -523.6101    \n",
            "    Negative Log Likelihood: 37.9838\tSigma2 Prior: -561.5988\tRegularization: 0.0049\n",
            "Iter: 12460  \tTraining Loss: -494.6634    \n",
            "    Negative Log Likelihood: 30.8195\tSigma2 Prior: -525.4877\tRegularization: 0.0049\n",
            "Iter: 12470  \tTraining Loss: -502.9835    \n",
            "    Negative Log Likelihood: 32.6467\tSigma2 Prior: -535.6350\tRegularization: 0.0049\n",
            "Iter: 12480  \tTraining Loss: -489.2188    \n",
            "    Negative Log Likelihood: 36.4115\tSigma2 Prior: -525.6352\tRegularization: 0.0049\n",
            "Iter: 12490  \tTraining Loss: -482.5891    \n",
            "    Negative Log Likelihood: 30.4481\tSigma2 Prior: -513.0421\tRegularization: 0.0049\n",
            "Iter: 12500  \tTraining Loss: -490.8379    \n",
            "    Negative Log Likelihood: 33.1550\tSigma2 Prior: -523.9977\tRegularization: 0.0049\n",
            "Iter: 12510  \tTraining Loss: -473.7397    \n",
            "    Negative Log Likelihood: 33.5089\tSigma2 Prior: -507.2535\tRegularization: 0.0049\n",
            "Iter: 12520  \tTraining Loss: -488.6601    \n",
            "    Negative Log Likelihood: 34.8796\tSigma2 Prior: -523.5446\tRegularization: 0.0049\n",
            "Iter: 12530  \tTraining Loss: -474.9370    \n",
            "    Negative Log Likelihood: 32.2246\tSigma2 Prior: -507.1666\tRegularization: 0.0049\n",
            "Iter: 12540  \tTraining Loss: -511.1058    \n",
            "    Negative Log Likelihood: 30.0948\tSigma2 Prior: -541.2056\tRegularization: 0.0049\n",
            "Iter: 12550  \tTraining Loss: -487.6393    \n",
            "    Negative Log Likelihood: 34.2291\tSigma2 Prior: -521.8733\tRegularization: 0.0049\n",
            "Iter: 12560  \tTraining Loss: -466.8963    \n",
            "    Negative Log Likelihood: 34.2424\tSigma2 Prior: -501.1436\tRegularization: 0.0049\n",
            "Iter: 12570  \tTraining Loss: -487.7874    \n",
            "    Negative Log Likelihood: 38.8656\tSigma2 Prior: -526.6580\tRegularization: 0.0049\n",
            "Iter: 12580  \tTraining Loss: -507.1585    \n",
            "    Negative Log Likelihood: 32.0766\tSigma2 Prior: -539.2401\tRegularization: 0.0049\n",
            "Iter: 12590  \tTraining Loss: -532.6520    \n",
            "    Negative Log Likelihood: 32.9128\tSigma2 Prior: -565.5696\tRegularization: 0.0049\n",
            "Iter: 12600  \tTraining Loss: -484.2517    \n",
            "    Negative Log Likelihood: 32.8150\tSigma2 Prior: -517.0717\tRegularization: 0.0049\n",
            "Iter: 12610  \tTraining Loss: -466.7612    \n",
            "    Negative Log Likelihood: 32.2732\tSigma2 Prior: -499.0393\tRegularization: 0.0049\n",
            "Iter: 12620  \tTraining Loss: -473.4603    \n",
            "    Negative Log Likelihood: 35.4491\tSigma2 Prior: -508.9144\tRegularization: 0.0049\n",
            "Iter: 12630  \tTraining Loss: -511.7630    \n",
            "    Negative Log Likelihood: 33.9281\tSigma2 Prior: -545.6960\tRegularization: 0.0049\n",
            "Iter: 12640  \tTraining Loss: -507.3713    \n",
            "    Negative Log Likelihood: 33.0916\tSigma2 Prior: -540.4679\tRegularization: 0.0049\n",
            "Iter: 12650  \tTraining Loss: -514.6682    \n",
            "    Negative Log Likelihood: 32.4042\tSigma2 Prior: -547.0773\tRegularization: 0.0049\n",
            "Iter: 12660  \tTraining Loss: -501.0038    \n",
            "    Negative Log Likelihood: 32.3727\tSigma2 Prior: -533.3815\tRegularization: 0.0049\n",
            "Iter: 12670  \tTraining Loss: -502.1105    \n",
            "    Negative Log Likelihood: 33.8923\tSigma2 Prior: -536.0078\tRegularization: 0.0049\n",
            "Iter: 12680  \tTraining Loss: -505.5336    \n",
            "    Negative Log Likelihood: 30.1897\tSigma2 Prior: -535.7283\tRegularization: 0.0049\n",
            "Iter: 12690  \tTraining Loss: -471.0306    \n",
            "    Negative Log Likelihood: 31.1431\tSigma2 Prior: -502.1786\tRegularization: 0.0049\n",
            "Iter: 12700  \tTraining Loss: -513.3776    \n",
            "    Negative Log Likelihood: 34.2734\tSigma2 Prior: -547.6560\tRegularization: 0.0049\n",
            "Iter: 12710  \tTraining Loss: -431.1618    \n",
            "    Negative Log Likelihood: 30.5859\tSigma2 Prior: -461.7527\tRegularization: 0.0049\n",
            "Iter: 12720  \tTraining Loss: -520.4604    \n",
            "    Negative Log Likelihood: 31.7985\tSigma2 Prior: -552.2638\tRegularization: 0.0049\n",
            "Iter: 12730  \tTraining Loss: -508.7471    \n",
            "    Negative Log Likelihood: 35.6550\tSigma2 Prior: -544.4070\tRegularization: 0.0049\n",
            "Iter: 12740  \tTraining Loss: -441.7612    \n",
            "    Negative Log Likelihood: 35.1609\tSigma2 Prior: -476.9270\tRegularization: 0.0049\n",
            "Iter: 12750  \tTraining Loss: -486.5697    \n",
            "    Negative Log Likelihood: 28.4102\tSigma2 Prior: -514.9849\tRegularization: 0.0050\n",
            "Iter: 12760  \tTraining Loss: -518.2708    \n",
            "    Negative Log Likelihood: 34.3888\tSigma2 Prior: -552.6645\tRegularization: 0.0050\n",
            "Iter: 12770  \tTraining Loss: -510.1055    \n",
            "    Negative Log Likelihood: 29.5913\tSigma2 Prior: -539.7017\tRegularization: 0.0050\n",
            "Iter: 12780  \tTraining Loss: -499.0928    \n",
            "    Negative Log Likelihood: 33.1517\tSigma2 Prior: -532.2495\tRegularization: 0.0050\n",
            "Iter: 12790  \tTraining Loss: -507.0739    \n",
            "    Negative Log Likelihood: 33.0860\tSigma2 Prior: -540.1649\tRegularization: 0.0050\n",
            "Iter: 12800  \tTraining Loss: -498.8141    \n",
            "    Negative Log Likelihood: 33.5133\tSigma2 Prior: -532.3323\tRegularization: 0.0050\n",
            "Iter: 12810  \tTraining Loss: -487.2072    \n",
            "    Negative Log Likelihood: 29.9431\tSigma2 Prior: -517.1553\tRegularization: 0.0050\n",
            "Iter: 12820  \tTraining Loss: -519.5623    \n",
            "    Negative Log Likelihood: 32.2628\tSigma2 Prior: -551.8300\tRegularization: 0.0050\n",
            "Iter: 12830  \tTraining Loss: -491.6412    \n",
            "    Negative Log Likelihood: 34.9892\tSigma2 Prior: -526.6354\tRegularization: 0.0050\n",
            "Iter: 12840  \tTraining Loss: -508.0788    \n",
            "    Negative Log Likelihood: 29.8552\tSigma2 Prior: -537.9389\tRegularization: 0.0050\n",
            "Iter: 12850  \tTraining Loss: -521.1898    \n",
            "    Negative Log Likelihood: 32.2337\tSigma2 Prior: -553.4285\tRegularization: 0.0050\n",
            "Iter: 12860  \tTraining Loss: -482.2392    \n",
            "    Negative Log Likelihood: 35.7489\tSigma2 Prior: -517.9930\tRegularization: 0.0050\n",
            "Iter: 12870  \tTraining Loss: -492.6206    \n",
            "    Negative Log Likelihood: 33.5703\tSigma2 Prior: -526.1959\tRegularization: 0.0050\n",
            "Iter: 12880  \tTraining Loss: -519.1250    \n",
            "    Negative Log Likelihood: 35.4857\tSigma2 Prior: -554.6158\tRegularization: 0.0050\n",
            "Iter: 12890  \tTraining Loss: -462.0362    \n",
            "    Negative Log Likelihood: 29.6294\tSigma2 Prior: -491.6705\tRegularization: 0.0050\n",
            "Iter: 12900  \tTraining Loss: -514.4170    \n",
            "    Negative Log Likelihood: 36.2655\tSigma2 Prior: -550.6874\tRegularization: 0.0050\n",
            "Iter: 12910  \tTraining Loss: -487.6429    \n",
            "    Negative Log Likelihood: 30.6562\tSigma2 Prior: -518.3041\tRegularization: 0.0050\n",
            "Iter: 12920  \tTraining Loss: -458.6674    \n",
            "    Negative Log Likelihood: 32.1970\tSigma2 Prior: -490.8694\tRegularization: 0.0050\n",
            "Iter: 12930  \tTraining Loss: -499.9041    \n",
            "    Negative Log Likelihood: 32.1525\tSigma2 Prior: -532.0616\tRegularization: 0.0050\n",
            "Iter: 12940  \tTraining Loss: -495.4850    \n",
            "    Negative Log Likelihood: 35.0725\tSigma2 Prior: -530.5626\tRegularization: 0.0050\n",
            "Iter: 12950  \tTraining Loss: -450.6618    \n",
            "    Negative Log Likelihood: 33.1747\tSigma2 Prior: -483.8415\tRegularization: 0.0050\n",
            "Iter: 12960  \tTraining Loss: -499.0757    \n",
            "    Negative Log Likelihood: 28.5806\tSigma2 Prior: -527.6614\tRegularization: 0.0050\n",
            "Iter: 12970  \tTraining Loss: -476.6838    \n",
            "    Negative Log Likelihood: 32.0119\tSigma2 Prior: -508.7007\tRegularization: 0.0050\n",
            "Iter: 12980  \tTraining Loss: -487.0558    \n",
            "    Negative Log Likelihood: 36.3112\tSigma2 Prior: -523.3719\tRegularization: 0.0050\n",
            "Iter: 12990  \tTraining Loss: -497.3170    \n",
            "    Negative Log Likelihood: 30.2484\tSigma2 Prior: -527.5704\tRegularization: 0.0050\n",
            "Iter: 13000  \tTraining Loss: -485.0119    \n",
            "    Negative Log Likelihood: 36.8460\tSigma2 Prior: -521.8629\tRegularization: 0.0050\n",
            "Iter: 13010  \tTraining Loss: -502.7511    \n",
            "    Negative Log Likelihood: 32.1121\tSigma2 Prior: -534.8682\tRegularization: 0.0050\n",
            "Iter: 13020  \tTraining Loss: -494.0507    \n",
            "    Negative Log Likelihood: 33.1096\tSigma2 Prior: -527.1653\tRegularization: 0.0050\n",
            "Iter: 13030  \tTraining Loss: -511.7228    \n",
            "    Negative Log Likelihood: 30.9599\tSigma2 Prior: -542.6877\tRegularization: 0.0050\n",
            "Iter: 13040  \tTraining Loss: -484.3809    \n",
            "    Negative Log Likelihood: 33.7811\tSigma2 Prior: -518.1670\tRegularization: 0.0050\n",
            "Iter: 13050  \tTraining Loss: -499.0009    \n",
            "    Negative Log Likelihood: 33.7729\tSigma2 Prior: -532.7787\tRegularization: 0.0050\n",
            "Iter: 13060  \tTraining Loss: -508.2813    \n",
            "    Negative Log Likelihood: 35.0733\tSigma2 Prior: -543.3596\tRegularization: 0.0050\n",
            "Iter: 13070  \tTraining Loss: -494.8660    \n",
            "    Negative Log Likelihood: 32.3763\tSigma2 Prior: -527.2473\tRegularization: 0.0050\n",
            "Iter: 13080  \tTraining Loss: -473.1506    \n",
            "    Negative Log Likelihood: 34.7690\tSigma2 Prior: -507.9247\tRegularization: 0.0050\n",
            "Iter: 13090  \tTraining Loss: -494.2751    \n",
            "    Negative Log Likelihood: 28.5988\tSigma2 Prior: -522.8789\tRegularization: 0.0050\n",
            "Iter: 13100  \tTraining Loss: -488.2786    \n",
            "    Negative Log Likelihood: 36.5371\tSigma2 Prior: -524.8207\tRegularization: 0.0050\n",
            "Iter: 13110  \tTraining Loss: -456.7991    \n",
            "    Negative Log Likelihood: 31.0179\tSigma2 Prior: -487.8221\tRegularization: 0.0050\n",
            "Iter: 13120  \tTraining Loss: -516.7929    \n",
            "    Negative Log Likelihood: 28.5493\tSigma2 Prior: -545.3473\tRegularization: 0.0050\n",
            "Iter: 13130  \tTraining Loss: -467.1263    \n",
            "    Negative Log Likelihood: 35.6361\tSigma2 Prior: -502.7674\tRegularization: 0.0050\n",
            "Iter: 13140  \tTraining Loss: -495.9013    \n",
            "    Negative Log Likelihood: 26.3554\tSigma2 Prior: -522.2618\tRegularization: 0.0050\n",
            "Iter: 13150  \tTraining Loss: -460.7425    \n",
            "    Negative Log Likelihood: 37.0380\tSigma2 Prior: -497.7856\tRegularization: 0.0050\n",
            "Iter: 13160  \tTraining Loss: -459.2375    \n",
            "    Negative Log Likelihood: 37.0948\tSigma2 Prior: -496.3373\tRegularization: 0.0050\n",
            "Iter: 13170  \tTraining Loss: -496.6164    \n",
            "    Negative Log Likelihood: 28.6514\tSigma2 Prior: -525.2728\tRegularization: 0.0051\n",
            "Iter: 13180  \tTraining Loss: -481.4066    \n",
            "    Negative Log Likelihood: 30.9618\tSigma2 Prior: -512.3734\tRegularization: 0.0051\n",
            "Iter: 13190  \tTraining Loss: -500.3648    \n",
            "    Negative Log Likelihood: 34.7214\tSigma2 Prior: -535.0912\tRegularization: 0.0051\n",
            "Iter: 13200  \tTraining Loss: -500.8355    \n",
            "    Negative Log Likelihood: 31.1238\tSigma2 Prior: -531.9644\tRegularization: 0.0051\n",
            "Iter: 13210  \tTraining Loss: -451.9369    \n",
            "    Negative Log Likelihood: 29.8965\tSigma2 Prior: -481.8385\tRegularization: 0.0051\n",
            "Iter: 13220  \tTraining Loss: -480.6011    \n",
            "    Negative Log Likelihood: 36.0690\tSigma2 Prior: -516.6752\tRegularization: 0.0051\n",
            "Iter: 13230  \tTraining Loss: -519.3166    \n",
            "    Negative Log Likelihood: 33.5199\tSigma2 Prior: -552.8416\tRegularization: 0.0051\n",
            "Iter: 13240  \tTraining Loss: -517.6594    \n",
            "    Negative Log Likelihood: 29.2782\tSigma2 Prior: -546.9427\tRegularization: 0.0051\n",
            "Iter: 13250  \tTraining Loss: -497.3748    \n",
            "    Negative Log Likelihood: 35.2900\tSigma2 Prior: -532.6699\tRegularization: 0.0051\n",
            "Iter: 13260  \tTraining Loss: -515.7724    \n",
            "    Negative Log Likelihood: 33.3912\tSigma2 Prior: -549.1686\tRegularization: 0.0051\n",
            "Iter: 13270  \tTraining Loss: -450.7418    \n",
            "    Negative Log Likelihood: 35.8641\tSigma2 Prior: -486.6109\tRegularization: 0.0051\n",
            "Iter: 13280  \tTraining Loss: -495.1292    \n",
            "    Negative Log Likelihood: 32.3787\tSigma2 Prior: -527.5129\tRegularization: 0.0051\n",
            "Iter: 13290  \tTraining Loss: -480.2721    \n",
            "    Negative Log Likelihood: 36.5975\tSigma2 Prior: -516.8747\tRegularization: 0.0051\n",
            "Iter: 13300  \tTraining Loss: -514.1126    \n",
            "    Negative Log Likelihood: 30.8169\tSigma2 Prior: -544.9346\tRegularization: 0.0051\n",
            "Iter: 13310  \tTraining Loss: -497.4974    \n",
            "    Negative Log Likelihood: 34.3364\tSigma2 Prior: -531.8389\tRegularization: 0.0051\n",
            "Iter: 13320  \tTraining Loss: -496.9576    \n",
            "    Negative Log Likelihood: 32.3784\tSigma2 Prior: -529.3411\tRegularization: 0.0051\n",
            "Iter: 13330  \tTraining Loss: -519.6578    \n",
            "    Negative Log Likelihood: 34.4681\tSigma2 Prior: -554.1310\tRegularization: 0.0051\n",
            "Iter: 13340  \tTraining Loss: -520.7060    \n",
            "    Negative Log Likelihood: 29.9105\tSigma2 Prior: -550.6216\tRegularization: 0.0051\n",
            "Iter: 13350  \tTraining Loss: -499.9262    \n",
            "    Negative Log Likelihood: 33.6258\tSigma2 Prior: -533.5571\tRegularization: 0.0051\n",
            "Iter: 13360  \tTraining Loss: -475.5974    \n",
            "    Negative Log Likelihood: 36.8534\tSigma2 Prior: -512.4558\tRegularization: 0.0051\n",
            "Iter: 13370  \tTraining Loss: -462.8911    \n",
            "    Negative Log Likelihood: 30.7449\tSigma2 Prior: -493.6410\tRegularization: 0.0051\n",
            "Iter: 13380  \tTraining Loss: -518.1462    \n",
            "    Negative Log Likelihood: 30.3744\tSigma2 Prior: -548.5258\tRegularization: 0.0051\n",
            "Iter: 13390  \tTraining Loss: -528.5746    \n",
            "    Negative Log Likelihood: 39.6862\tSigma2 Prior: -568.2659\tRegularization: 0.0051\n",
            "Iter: 13400  \tTraining Loss: -500.9158    \n",
            "    Negative Log Likelihood: 31.1124\tSigma2 Prior: -532.0333\tRegularization: 0.0051\n",
            "Iter: 13410  \tTraining Loss: -474.1149    \n",
            "    Negative Log Likelihood: 36.2788\tSigma2 Prior: -510.3987\tRegularization: 0.0051\n",
            "Iter: 13420  \tTraining Loss: -502.1649    \n",
            "    Negative Log Likelihood: 31.0490\tSigma2 Prior: -533.2189\tRegularization: 0.0051\n",
            "Iter: 13430  \tTraining Loss: -496.7046    \n",
            "    Negative Log Likelihood: 31.1788\tSigma2 Prior: -527.8885\tRegularization: 0.0051\n",
            "Iter: 13440  \tTraining Loss: -491.4935    \n",
            "    Negative Log Likelihood: 34.1780\tSigma2 Prior: -525.6766\tRegularization: 0.0051\n",
            "Iter: 13450  \tTraining Loss: -499.7906    \n",
            "    Negative Log Likelihood: 34.7183\tSigma2 Prior: -534.5141\tRegularization: 0.0051\n",
            "Iter: 13460  \tTraining Loss: -469.0531    \n",
            "    Negative Log Likelihood: 31.8587\tSigma2 Prior: -500.9169\tRegularization: 0.0051\n",
            "Iter: 13470  \tTraining Loss: -486.7474    \n",
            "    Negative Log Likelihood: 31.2783\tSigma2 Prior: -518.0309\tRegularization: 0.0051\n",
            "Iter: 13480  \tTraining Loss: -504.0721    \n",
            "    Negative Log Likelihood: 32.3697\tSigma2 Prior: -536.4469\tRegularization: 0.0051\n",
            "Iter: 13490  \tTraining Loss: -510.9537    \n",
            "    Negative Log Likelihood: 32.8927\tSigma2 Prior: -543.8516\tRegularization: 0.0051\n",
            "Iter: 13500  \tTraining Loss: -508.8007    \n",
            "    Negative Log Likelihood: 28.7303\tSigma2 Prior: -537.5361\tRegularization: 0.0051\n",
            "Iter: 13510  \tTraining Loss: -477.9774    \n",
            "    Negative Log Likelihood: 34.8897\tSigma2 Prior: -512.8723\tRegularization: 0.0051\n",
            "Iter: 13520  \tTraining Loss: -451.7617    \n",
            "    Negative Log Likelihood: 33.6879\tSigma2 Prior: -485.4547\tRegularization: 0.0051\n",
            "Iter: 13530  \tTraining Loss: -509.2234    \n",
            "    Negative Log Likelihood: 30.8089\tSigma2 Prior: -540.0374\tRegularization: 0.0051\n",
            "Iter: 13540  \tTraining Loss: -484.8598    \n",
            "    Negative Log Likelihood: 35.2878\tSigma2 Prior: -520.1527\tRegularization: 0.0051\n",
            "Iter: 13550  \tTraining Loss: -508.0982    \n",
            "    Negative Log Likelihood: 29.1386\tSigma2 Prior: -537.2420\tRegularization: 0.0051\n",
            "Iter: 13560  \tTraining Loss: -504.7741    \n",
            "    Negative Log Likelihood: 32.2825\tSigma2 Prior: -537.0618\tRegularization: 0.0051\n",
            "Iter: 13570  \tTraining Loss: -494.7064    \n",
            "    Negative Log Likelihood: 32.9665\tSigma2 Prior: -527.6780\tRegularization: 0.0051\n",
            "Iter: 13580  \tTraining Loss: -491.4869    \n",
            "    Negative Log Likelihood: 35.6539\tSigma2 Prior: -527.1460\tRegularization: 0.0051\n",
            "Iter: 13590  \tTraining Loss: -484.1877    \n",
            "    Negative Log Likelihood: 33.5563\tSigma2 Prior: -517.7492\tRegularization: 0.0051\n",
            "Iter: 13600  \tTraining Loss: -519.8441    \n",
            "    Negative Log Likelihood: 32.4115\tSigma2 Prior: -552.2608\tRegularization: 0.0052\n",
            "Iter: 13610  \tTraining Loss: -504.1379    \n",
            "    Negative Log Likelihood: 37.7498\tSigma2 Prior: -541.8929\tRegularization: 0.0052\n",
            "Iter: 13620  \tTraining Loss: -476.3421    \n",
            "    Negative Log Likelihood: 31.6021\tSigma2 Prior: -507.9493\tRegularization: 0.0052\n",
            "Iter: 13630  \tTraining Loss: -502.4220    \n",
            "    Negative Log Likelihood: 29.7779\tSigma2 Prior: -532.2051\tRegularization: 0.0052\n",
            "Iter: 13640  \tTraining Loss: -507.6272    \n",
            "    Negative Log Likelihood: 27.3787\tSigma2 Prior: -535.0110\tRegularization: 0.0052\n",
            "Iter: 13650  \tTraining Loss: -488.7565    \n",
            "    Negative Log Likelihood: 34.6052\tSigma2 Prior: -523.3669\tRegularization: 0.0052\n",
            "Iter: 13660  \tTraining Loss: -485.5431    \n",
            "    Negative Log Likelihood: 30.4229\tSigma2 Prior: -515.9711\tRegularization: 0.0052\n",
            "Iter: 13670  \tTraining Loss: -476.6350    \n",
            "    Negative Log Likelihood: 26.0701\tSigma2 Prior: -502.7103\tRegularization: 0.0052\n",
            "Iter: 13680  \tTraining Loss: -531.1160    \n",
            "    Negative Log Likelihood: 33.5565\tSigma2 Prior: -564.6776\tRegularization: 0.0052\n",
            "Iter: 13690  \tTraining Loss: -477.7597    \n",
            "    Negative Log Likelihood: 34.3393\tSigma2 Prior: -512.1041\tRegularization: 0.0052\n",
            "Iter: 13700  \tTraining Loss: -510.7766    \n",
            "    Negative Log Likelihood: 29.1076\tSigma2 Prior: -539.8893\tRegularization: 0.0052\n",
            "Iter: 13710  \tTraining Loss: -480.8830    \n",
            "    Negative Log Likelihood: 31.5055\tSigma2 Prior: -512.3937\tRegularization: 0.0052\n",
            "Iter: 13720  \tTraining Loss: -482.1592    \n",
            "    Negative Log Likelihood: 32.5641\tSigma2 Prior: -514.7285\tRegularization: 0.0052\n",
            "Iter: 13730  \tTraining Loss: -505.9649    \n",
            "    Negative Log Likelihood: 36.2289\tSigma2 Prior: -542.1990\tRegularization: 0.0052\n",
            "Iter: 13740  \tTraining Loss: -489.9869    \n",
            "    Negative Log Likelihood: 31.6448\tSigma2 Prior: -521.6369\tRegularization: 0.0052\n",
            "Iter: 13750  \tTraining Loss: -505.0127    \n",
            "    Negative Log Likelihood: 36.0460\tSigma2 Prior: -541.0638\tRegularization: 0.0052\n",
            "Iter: 13760  \tTraining Loss: -501.3535    \n",
            "    Negative Log Likelihood: 32.1702\tSigma2 Prior: -533.5289\tRegularization: 0.0052\n",
            "Iter: 13770  \tTraining Loss: -483.0500    \n",
            "    Negative Log Likelihood: 34.8566\tSigma2 Prior: -517.9117\tRegularization: 0.0052\n",
            "Iter: 13780  \tTraining Loss: -491.6040    \n",
            "    Negative Log Likelihood: 36.3149\tSigma2 Prior: -527.9241\tRegularization: 0.0052\n",
            "Iter: 13790  \tTraining Loss: -507.8148    \n",
            "    Negative Log Likelihood: 34.6525\tSigma2 Prior: -542.4724\tRegularization: 0.0052\n",
            "Iter: 13800  \tTraining Loss: -506.5151    \n",
            "    Negative Log Likelihood: 30.7122\tSigma2 Prior: -537.2325\tRegularization: 0.0052\n",
            "Iter: 13810  \tTraining Loss: -497.7881    \n",
            "    Negative Log Likelihood: 36.5590\tSigma2 Prior: -534.3522\tRegularization: 0.0052\n",
            "Iter: 13820  \tTraining Loss: -501.0025    \n",
            "    Negative Log Likelihood: 27.0276\tSigma2 Prior: -528.0353\tRegularization: 0.0052\n",
            "Iter: 13830  \tTraining Loss: -499.6097    \n",
            "    Negative Log Likelihood: 27.2822\tSigma2 Prior: -526.8971\tRegularization: 0.0052\n",
            "Iter: 13840  \tTraining Loss: -503.5413    \n",
            "    Negative Log Likelihood: 41.2599\tSigma2 Prior: -544.8064\tRegularization: 0.0052\n",
            "Iter: 13850  \tTraining Loss: -502.9326    \n",
            "    Negative Log Likelihood: 30.0209\tSigma2 Prior: -532.9587\tRegularization: 0.0052\n",
            "Iter: 13860  \tTraining Loss: -497.5155    \n",
            "    Negative Log Likelihood: 36.1659\tSigma2 Prior: -533.6866\tRegularization: 0.0052\n",
            "Iter: 13870  \tTraining Loss: -509.6594    \n",
            "    Negative Log Likelihood: 33.8606\tSigma2 Prior: -543.5252\tRegularization: 0.0052\n",
            "Iter: 13880  \tTraining Loss: -483.4590    \n",
            "    Negative Log Likelihood: 29.5106\tSigma2 Prior: -512.9748\tRegularization: 0.0052\n",
            "Iter: 13890  \tTraining Loss: -524.3290    \n",
            "    Negative Log Likelihood: 34.9333\tSigma2 Prior: -559.2675\tRegularization: 0.0052\n",
            "Iter: 13900  \tTraining Loss: -477.1165    \n",
            "    Negative Log Likelihood: 29.3351\tSigma2 Prior: -506.4568\tRegularization: 0.0052\n",
            "Iter: 13910  \tTraining Loss: -494.8742    \n",
            "    Negative Log Likelihood: 22.2538\tSigma2 Prior: -517.1332\tRegularization: 0.0052\n",
            "Iter: 13920  \tTraining Loss: -485.2577    \n",
            "    Negative Log Likelihood: 37.7762\tSigma2 Prior: -523.0391\tRegularization: 0.0052\n",
            "Iter: 13930  \tTraining Loss: -512.1105    \n",
            "    Negative Log Likelihood: 30.6507\tSigma2 Prior: -542.7665\tRegularization: 0.0052\n",
            "Iter: 13940  \tTraining Loss: -512.3281    \n",
            "    Negative Log Likelihood: 34.2801\tSigma2 Prior: -546.6134\tRegularization: 0.0052\n",
            "Iter: 13950  \tTraining Loss: -489.1313    \n",
            "    Negative Log Likelihood: 35.3023\tSigma2 Prior: -524.4388\tRegularization: 0.0052\n",
            "Iter: 13960  \tTraining Loss: -470.0395    \n",
            "    Negative Log Likelihood: 33.5123\tSigma2 Prior: -503.5570\tRegularization: 0.0052\n",
            "Iter: 13970  \tTraining Loss: -516.0748    \n",
            "    Negative Log Likelihood: 39.6540\tSigma2 Prior: -555.7340\tRegularization: 0.0052\n",
            "Iter: 13980  \tTraining Loss: -498.8495    \n",
            "    Negative Log Likelihood: 28.9614\tSigma2 Prior: -527.8161\tRegularization: 0.0052\n",
            "Iter: 13990  \tTraining Loss: -500.9159    \n",
            "    Negative Log Likelihood: 33.7636\tSigma2 Prior: -534.6848\tRegularization: 0.0052\n",
            "Iter: 14000  \tTraining Loss: -491.1036    \n",
            "    Negative Log Likelihood: 31.5724\tSigma2 Prior: -522.6813\tRegularization: 0.0052\n",
            "Iter: 14010  \tTraining Loss: -480.5767    \n",
            "    Negative Log Likelihood: 32.4284\tSigma2 Prior: -513.0104\tRegularization: 0.0052\n",
            "Iter: 14020  \tTraining Loss: -488.1791    \n",
            "    Negative Log Likelihood: 32.9981\tSigma2 Prior: -521.1824\tRegularization: 0.0052\n",
            "Iter: 14030  \tTraining Loss: -487.9618    \n",
            "    Negative Log Likelihood: 33.8281\tSigma2 Prior: -521.7952\tRegularization: 0.0052\n",
            "Iter: 14040  \tTraining Loss: -477.9865    \n",
            "    Negative Log Likelihood: 30.8872\tSigma2 Prior: -508.8788\tRegularization: 0.0053\n",
            "Iter: 14050  \tTraining Loss: -521.1358    \n",
            "    Negative Log Likelihood: 36.0163\tSigma2 Prior: -557.1573\tRegularization: 0.0053\n",
            "Iter: 14060  \tTraining Loss: -508.6041    \n",
            "    Negative Log Likelihood: 28.0143\tSigma2 Prior: -536.6237\tRegularization: 0.0053\n",
            "Iter: 14070  \tTraining Loss: -458.5896    \n",
            "    Negative Log Likelihood: 33.3736\tSigma2 Prior: -491.9684\tRegularization: 0.0053\n",
            "Iter: 14080  \tTraining Loss: -447.4660    \n",
            "    Negative Log Likelihood: 32.8707\tSigma2 Prior: -480.3419\tRegularization: 0.0053\n",
            "Iter: 14090  \tTraining Loss: -480.9749    \n",
            "    Negative Log Likelihood: 31.8480\tSigma2 Prior: -512.8281\tRegularization: 0.0053\n",
            "Iter: 14100  \tTraining Loss: -489.8699    \n",
            "    Negative Log Likelihood: 37.7255\tSigma2 Prior: -527.6006\tRegularization: 0.0053\n",
            "Iter: 14110  \tTraining Loss: -483.3929    \n",
            "    Negative Log Likelihood: 29.7573\tSigma2 Prior: -513.1555\tRegularization: 0.0053\n",
            "Iter: 14120  \tTraining Loss: -454.9985    \n",
            "    Negative Log Likelihood: 33.6946\tSigma2 Prior: -488.6984\tRegularization: 0.0053\n",
            "Iter: 14130  \tTraining Loss: -424.2133    \n",
            "    Negative Log Likelihood: 35.0676\tSigma2 Prior: -459.2862\tRegularization: 0.0053\n",
            "Iter: 14140  \tTraining Loss: -506.0637    \n",
            "    Negative Log Likelihood: 33.6388\tSigma2 Prior: -539.7078\tRegularization: 0.0053\n",
            "Iter: 14150  \tTraining Loss: -476.7678    \n",
            "    Negative Log Likelihood: 30.1413\tSigma2 Prior: -506.9143\tRegularization: 0.0053\n",
            "Iter: 14160  \tTraining Loss: -485.4013    \n",
            "    Negative Log Likelihood: 31.4488\tSigma2 Prior: -516.8554\tRegularization: 0.0053\n",
            "Iter: 14170  \tTraining Loss: -504.9989    \n",
            "    Negative Log Likelihood: 37.6968\tSigma2 Prior: -542.7010\tRegularization: 0.0053\n",
            "Iter: 14180  \tTraining Loss: -494.3176    \n",
            "    Negative Log Likelihood: 27.0482\tSigma2 Prior: -521.3711\tRegularization: 0.0053\n",
            "Iter: 14190  \tTraining Loss: -498.1189    \n",
            "    Negative Log Likelihood: 34.1092\tSigma2 Prior: -532.2333\tRegularization: 0.0053\n",
            "Iter: 14200  \tTraining Loss: -505.4935    \n",
            "    Negative Log Likelihood: 27.5137\tSigma2 Prior: -533.0125\tRegularization: 0.0053\n",
            "Iter: 14210  \tTraining Loss: -495.7567    \n",
            "    Negative Log Likelihood: 29.8753\tSigma2 Prior: -525.6373\tRegularization: 0.0053\n",
            "Iter: 14220  \tTraining Loss: -500.6883    \n",
            "    Negative Log Likelihood: 36.3655\tSigma2 Prior: -537.0590\tRegularization: 0.0053\n",
            "Iter: 14230  \tTraining Loss: -464.5447    \n",
            "    Negative Log Likelihood: 26.3045\tSigma2 Prior: -490.8544\tRegularization: 0.0053\n",
            "Iter: 14240  \tTraining Loss: -475.2563    \n",
            "    Negative Log Likelihood: 34.0114\tSigma2 Prior: -509.2730\tRegularization: 0.0053\n",
            "Iter: 14250  \tTraining Loss: -515.3718    \n",
            "    Negative Log Likelihood: 31.4482\tSigma2 Prior: -546.8253\tRegularization: 0.0053\n",
            "Iter: 14260  \tTraining Loss: -494.7635    \n",
            "    Negative Log Likelihood: 32.9209\tSigma2 Prior: -527.6897\tRegularization: 0.0053\n",
            "Iter: 14270  \tTraining Loss: -455.0016    \n",
            "    Negative Log Likelihood: 36.4034\tSigma2 Prior: -491.4103\tRegularization: 0.0053\n",
            "Iter: 14280  \tTraining Loss: -494.8729    \n",
            "    Negative Log Likelihood: 30.1848\tSigma2 Prior: -525.0630\tRegularization: 0.0053\n",
            "Iter: 14290  \tTraining Loss: -525.9795    \n",
            "    Negative Log Likelihood: 35.1182\tSigma2 Prior: -561.1030\tRegularization: 0.0053\n",
            "Iter: 14300  \tTraining Loss: -512.8171    \n",
            "    Negative Log Likelihood: 29.9162\tSigma2 Prior: -542.7386\tRegularization: 0.0053\n",
            "Iter: 14310  \tTraining Loss: -487.0994    \n",
            "    Negative Log Likelihood: 31.3290\tSigma2 Prior: -518.4337\tRegularization: 0.0053\n",
            "Iter: 14320  \tTraining Loss: -495.3459    \n",
            "    Negative Log Likelihood: 33.9281\tSigma2 Prior: -529.2794\tRegularization: 0.0053\n",
            "Iter: 14330  \tTraining Loss: -506.3129    \n",
            "    Negative Log Likelihood: 34.5439\tSigma2 Prior: -540.8622\tRegularization: 0.0053\n",
            "Iter: 14340  \tTraining Loss: -489.1119    \n",
            "    Negative Log Likelihood: 31.7513\tSigma2 Prior: -520.8685\tRegularization: 0.0053\n",
            "Iter: 14350  \tTraining Loss: -530.2792    \n",
            "    Negative Log Likelihood: 31.7793\tSigma2 Prior: -562.0638\tRegularization: 0.0053\n",
            "Iter: 14360  \tTraining Loss: -478.9559    \n",
            "    Negative Log Likelihood: 35.5301\tSigma2 Prior: -514.4913\tRegularization: 0.0053\n",
            "Iter: 14370  \tTraining Loss: -495.0547    \n",
            "    Negative Log Likelihood: 24.5734\tSigma2 Prior: -519.6334\tRegularization: 0.0053\n",
            "Iter: 14380  \tTraining Loss: -496.9663    \n",
            "    Negative Log Likelihood: 34.3597\tSigma2 Prior: -531.3314\tRegularization: 0.0053\n",
            "Iter: 14390  \tTraining Loss: -442.8665    \n",
            "    Negative Log Likelihood: 32.2209\tSigma2 Prior: -475.0927\tRegularization: 0.0053\n",
            "Iter: 14400  \tTraining Loss: -514.0707    \n",
            "    Negative Log Likelihood: 27.3637\tSigma2 Prior: -541.4396\tRegularization: 0.0053\n",
            "Iter: 14410  \tTraining Loss: -518.2114    \n",
            "    Negative Log Likelihood: 35.4492\tSigma2 Prior: -553.6660\tRegularization: 0.0053\n",
            "Iter: 14420  \tTraining Loss: -503.4615    \n",
            "    Negative Log Likelihood: 33.2445\tSigma2 Prior: -536.7114\tRegularization: 0.0053\n",
            "Iter: 14430  \tTraining Loss: -499.7693    \n",
            "    Negative Log Likelihood: 26.6004\tSigma2 Prior: -526.3750\tRegularization: 0.0053\n",
            "Iter: 14440  \tTraining Loss: -498.6130    \n",
            "    Negative Log Likelihood: 33.9387\tSigma2 Prior: -532.5571\tRegularization: 0.0053\n",
            "Iter: 14450  \tTraining Loss: -481.2900    \n",
            "    Negative Log Likelihood: 31.6649\tSigma2 Prior: -512.9602\tRegularization: 0.0053\n",
            "Iter: 14460  \tTraining Loss: -507.9202    \n",
            "    Negative Log Likelihood: 28.8428\tSigma2 Prior: -536.7683\tRegularization: 0.0053\n",
            "Iter: 14470  \tTraining Loss: -473.1223    \n",
            "    Negative Log Likelihood: 37.3164\tSigma2 Prior: -510.4440\tRegularization: 0.0053\n",
            "Iter: 14480  \tTraining Loss: -514.4774    \n",
            "    Negative Log Likelihood: 31.2222\tSigma2 Prior: -545.7049\tRegularization: 0.0053\n",
            "Iter: 14490  \tTraining Loss: -444.0214    \n",
            "    Negative Log Likelihood: 39.4516\tSigma2 Prior: -483.4783\tRegularization: 0.0053\n",
            "Iter: 14500  \tTraining Loss: -446.8493    \n",
            "    Negative Log Likelihood: 25.3829\tSigma2 Prior: -472.2376\tRegularization: 0.0054\n",
            "Iter: 14510  \tTraining Loss: -472.7536    \n",
            "    Negative Log Likelihood: 31.4293\tSigma2 Prior: -504.1883\tRegularization: 0.0054\n",
            "Iter: 14520  \tTraining Loss: -483.1873    \n",
            "    Negative Log Likelihood: 35.6875\tSigma2 Prior: -518.8802\tRegularization: 0.0054\n",
            "Iter: 14530  \tTraining Loss: -488.4651    \n",
            "    Negative Log Likelihood: 35.7821\tSigma2 Prior: -524.2526\tRegularization: 0.0054\n",
            "Iter: 14540  \tTraining Loss: -495.5008    \n",
            "    Negative Log Likelihood: 25.8801\tSigma2 Prior: -521.3863\tRegularization: 0.0054\n",
            "Iter: 14550  \tTraining Loss: -488.4331    \n",
            "    Negative Log Likelihood: 30.0999\tSigma2 Prior: -518.5383\tRegularization: 0.0054\n",
            "Iter: 14560  \tTraining Loss: -483.8846    \n",
            "    Negative Log Likelihood: 32.5915\tSigma2 Prior: -516.4814\tRegularization: 0.0054\n",
            "Iter: 14570  \tTraining Loss: -472.8475    \n",
            "    Negative Log Likelihood: 26.8152\tSigma2 Prior: -499.6680\tRegularization: 0.0054\n",
            "Iter: 14580  \tTraining Loss: -499.5990    \n",
            "    Negative Log Likelihood: 34.3270\tSigma2 Prior: -533.9314\tRegularization: 0.0054\n",
            "Iter: 14590  \tTraining Loss: -505.4830    \n",
            "    Negative Log Likelihood: 32.0044\tSigma2 Prior: -537.4928\tRegularization: 0.0054\n",
            "Iter: 14600  \tTraining Loss: -471.9993    \n",
            "    Negative Log Likelihood: 27.2463\tSigma2 Prior: -499.2509\tRegularization: 0.0054\n",
            "Iter: 14610  \tTraining Loss: -483.7719    \n",
            "    Negative Log Likelihood: 38.0904\tSigma2 Prior: -521.8677\tRegularization: 0.0054\n",
            "Iter: 14620  \tTraining Loss: -505.8207    \n",
            "    Negative Log Likelihood: 25.3934\tSigma2 Prior: -531.2195\tRegularization: 0.0054\n",
            "Iter: 14630  \tTraining Loss: -489.1520    \n",
            "    Negative Log Likelihood: 28.2126\tSigma2 Prior: -517.3699\tRegularization: 0.0054\n",
            "Iter: 14640  \tTraining Loss: -508.8052    \n",
            "    Negative Log Likelihood: 34.1233\tSigma2 Prior: -542.9338\tRegularization: 0.0054\n",
            "Iter: 14650  \tTraining Loss: -453.9544    \n",
            "    Negative Log Likelihood: 33.1592\tSigma2 Prior: -487.1190\tRegularization: 0.0054\n",
            "Iter: 14660  \tTraining Loss: -497.5210    \n",
            "    Negative Log Likelihood: 32.7366\tSigma2 Prior: -530.2630\tRegularization: 0.0054\n",
            "Iter: 14670  \tTraining Loss: -482.6682    \n",
            "    Negative Log Likelihood: 34.9517\tSigma2 Prior: -517.6253\tRegularization: 0.0054\n",
            "Iter: 14680  \tTraining Loss: -502.5648    \n",
            "    Negative Log Likelihood: 30.1211\tSigma2 Prior: -532.6913\tRegularization: 0.0054\n",
            "Iter: 14690  \tTraining Loss: -433.9933    \n",
            "    Negative Log Likelihood: 35.6262\tSigma2 Prior: -469.6248\tRegularization: 0.0054\n",
            "Iter: 14700  \tTraining Loss: -478.4131    \n",
            "    Negative Log Likelihood: 30.8141\tSigma2 Prior: -509.2326\tRegularization: 0.0054\n",
            "Iter: 14710  \tTraining Loss: -497.7841    \n",
            "    Negative Log Likelihood: 31.2447\tSigma2 Prior: -529.0342\tRegularization: 0.0054\n",
            "Iter: 14720  \tTraining Loss: -469.1615    \n",
            "    Negative Log Likelihood: 34.4393\tSigma2 Prior: -503.6062\tRegularization: 0.0054\n",
            "Iter: 14730  \tTraining Loss: -481.1444    \n",
            "    Negative Log Likelihood: 29.2214\tSigma2 Prior: -510.3712\tRegularization: 0.0054\n",
            "Iter: 14740  \tTraining Loss: -489.2484    \n",
            "    Negative Log Likelihood: 34.9593\tSigma2 Prior: -524.2131\tRegularization: 0.0054\n",
            "Iter: 14750  \tTraining Loss: -497.8970    \n",
            "    Negative Log Likelihood: 32.9696\tSigma2 Prior: -530.8720\tRegularization: 0.0054\n",
            "Iter: 14760  \tTraining Loss: -495.2716    \n",
            "    Negative Log Likelihood: 34.6346\tSigma2 Prior: -529.9116\tRegularization: 0.0054\n",
            "Iter: 14770  \tTraining Loss: -484.6502    \n",
            "    Negative Log Likelihood: 31.7902\tSigma2 Prior: -516.4458\tRegularization: 0.0054\n",
            "Iter: 14780  \tTraining Loss: -462.4334    \n",
            "    Negative Log Likelihood: 28.8890\tSigma2 Prior: -491.3278\tRegularization: 0.0054\n",
            "Iter: 14790  \tTraining Loss: -503.3408    \n",
            "    Negative Log Likelihood: 34.7535\tSigma2 Prior: -538.0996\tRegularization: 0.0054\n",
            "Iter: 14800  \tTraining Loss: -482.2664    \n",
            "    Negative Log Likelihood: 33.1227\tSigma2 Prior: -515.3945\tRegularization: 0.0054\n",
            "Iter: 14810  \tTraining Loss: -512.9881    \n",
            "    Negative Log Likelihood: 29.2648\tSigma2 Prior: -542.2583\tRegularization: 0.0054\n",
            "Iter: 14820  \tTraining Loss: -449.3959    \n",
            "    Negative Log Likelihood: 34.7334\tSigma2 Prior: -484.1347\tRegularization: 0.0054\n",
            "Iter: 14830  \tTraining Loss: -494.0506    \n",
            "    Negative Log Likelihood: 29.8474\tSigma2 Prior: -523.9034\tRegularization: 0.0054\n",
            "Iter: 14840  \tTraining Loss: -520.0084    \n",
            "    Negative Log Likelihood: 34.6564\tSigma2 Prior: -554.6703\tRegularization: 0.0054\n",
            "Iter: 14850  \tTraining Loss: -492.8497    \n",
            "    Negative Log Likelihood: 32.3155\tSigma2 Prior: -525.1707\tRegularization: 0.0054\n",
            "Iter: 14860  \tTraining Loss: -466.1639    \n",
            "    Negative Log Likelihood: 32.5152\tSigma2 Prior: -498.6845\tRegularization: 0.0054\n",
            "Iter: 14870  \tTraining Loss: -488.9606    \n",
            "    Negative Log Likelihood: 30.6183\tSigma2 Prior: -519.5843\tRegularization: 0.0054\n",
            "Iter: 14880  \tTraining Loss: -503.8361    \n",
            "    Negative Log Likelihood: 34.2792\tSigma2 Prior: -538.1207\tRegularization: 0.0054\n",
            "Iter: 14890  \tTraining Loss: -486.4402    \n",
            "    Negative Log Likelihood: 31.9353\tSigma2 Prior: -518.3809\tRegularization: 0.0054\n",
            "Iter: 14900  \tTraining Loss: -494.9160    \n",
            "    Negative Log Likelihood: 34.2904\tSigma2 Prior: -529.2118\tRegularization: 0.0054\n",
            "Iter: 14910  \tTraining Loss: -524.5490    \n",
            "    Negative Log Likelihood: 33.8223\tSigma2 Prior: -558.3767\tRegularization: 0.0054\n",
            "Iter: 14920  \tTraining Loss: -519.5935    \n",
            "    Negative Log Likelihood: 35.7892\tSigma2 Prior: -555.3881\tRegularization: 0.0054\n",
            "Iter: 14930  \tTraining Loss: -490.0626    \n",
            "    Negative Log Likelihood: 32.0686\tSigma2 Prior: -522.1366\tRegularization: 0.0054\n",
            "Iter: 14940  \tTraining Loss: -455.9551    \n",
            "    Negative Log Likelihood: 30.2590\tSigma2 Prior: -486.2195\tRegularization: 0.0054\n",
            "Iter: 14950  \tTraining Loss: -511.9126    \n",
            "    Negative Log Likelihood: 36.3630\tSigma2 Prior: -548.2809\tRegularization: 0.0054\n",
            "Iter: 14960  \tTraining Loss: -456.7723    \n",
            "    Negative Log Likelihood: 28.5274\tSigma2 Prior: -485.3052\tRegularization: 0.0054\n",
            "Iter: 14970  \tTraining Loss: -501.8922    \n",
            "    Negative Log Likelihood: 31.2895\tSigma2 Prior: -533.1871\tRegularization: 0.0055\n",
            "Iter: 14980  \tTraining Loss: -475.9106    \n",
            "    Negative Log Likelihood: 36.7560\tSigma2 Prior: -512.6721\tRegularization: 0.0055\n",
            "Iter: 14990  \tTraining Loss: -492.4698    \n",
            "    Negative Log Likelihood: 28.1812\tSigma2 Prior: -520.6564\tRegularization: 0.0055\n",
            "Iter: 15000  \tTraining Loss: -509.0613    \n",
            "    Negative Log Likelihood: 32.8233\tSigma2 Prior: -541.8901\tRegularization: 0.0055\n",
            "Iter: 15010  \tTraining Loss: -468.1752    \n",
            "    Negative Log Likelihood: 34.0328\tSigma2 Prior: -502.2135\tRegularization: 0.0055\n",
            "Iter: 15020  \tTraining Loss: -443.3474    \n",
            "    Negative Log Likelihood: 37.9559\tSigma2 Prior: -481.3087\tRegularization: 0.0055\n",
            "Iter: 15030  \tTraining Loss: -486.6577    \n",
            "    Negative Log Likelihood: 29.8231\tSigma2 Prior: -516.4862\tRegularization: 0.0055\n",
            "Iter: 15040  \tTraining Loss: -519.7678    \n",
            "    Negative Log Likelihood: 30.2780\tSigma2 Prior: -550.0513\tRegularization: 0.0055\n",
            "Iter: 15050  \tTraining Loss: -503.8654    \n",
            "    Negative Log Likelihood: 32.6368\tSigma2 Prior: -536.5076\tRegularization: 0.0055\n",
            "Iter: 15060  \tTraining Loss: -500.5519    \n",
            "    Negative Log Likelihood: 30.6636\tSigma2 Prior: -531.2209\tRegularization: 0.0055\n",
            "Iter: 15070  \tTraining Loss: -505.9342    \n",
            "    Negative Log Likelihood: 36.2436\tSigma2 Prior: -542.1833\tRegularization: 0.0055\n",
            "Iter: 15080  \tTraining Loss: -505.8354    \n",
            "    Negative Log Likelihood: 37.8087\tSigma2 Prior: -543.6495\tRegularization: 0.0055\n",
            "Iter: 15090  \tTraining Loss: -520.6656    \n",
            "    Negative Log Likelihood: 32.1060\tSigma2 Prior: -552.7771\tRegularization: 0.0055\n",
            "Iter: 15100  \tTraining Loss: -518.2354    \n",
            "    Negative Log Likelihood: 32.2421\tSigma2 Prior: -550.4830\tRegularization: 0.0055\n",
            "Iter: 15110  \tTraining Loss: -510.3059    \n",
            "    Negative Log Likelihood: 33.9564\tSigma2 Prior: -544.2678\tRegularization: 0.0055\n",
            "Iter: 15120  \tTraining Loss: -478.4414    \n",
            "    Negative Log Likelihood: 28.0948\tSigma2 Prior: -506.5418\tRegularization: 0.0055\n",
            "Iter: 15130  \tTraining Loss: -508.8312    \n",
            "    Negative Log Likelihood: 29.0917\tSigma2 Prior: -537.9285\tRegularization: 0.0055\n",
            "Iter: 15140  \tTraining Loss: -506.4496    \n",
            "    Negative Log Likelihood: 36.8313\tSigma2 Prior: -543.2863\tRegularization: 0.0055\n",
            "Iter: 15150  \tTraining Loss: -496.2512    \n",
            "    Negative Log Likelihood: 36.5524\tSigma2 Prior: -532.8090\tRegularization: 0.0055\n",
            "Iter: 15160  \tTraining Loss: -525.4490    \n",
            "    Negative Log Likelihood: 31.9458\tSigma2 Prior: -557.4003\tRegularization: 0.0055\n",
            "Iter: 15170  \tTraining Loss: -454.4816    \n",
            "    Negative Log Likelihood: 32.7815\tSigma2 Prior: -487.2686\tRegularization: 0.0055\n",
            "Iter: 15180  \tTraining Loss: -488.4561    \n",
            "    Negative Log Likelihood: 29.4062\tSigma2 Prior: -517.8679\tRegularization: 0.0055\n",
            "Iter: 15190  \tTraining Loss: -491.0681    \n",
            "    Negative Log Likelihood: 32.9969\tSigma2 Prior: -524.0705\tRegularization: 0.0055\n",
            "Iter: 15200  \tTraining Loss: -516.6796    \n",
            "    Negative Log Likelihood: 32.7527\tSigma2 Prior: -549.4378\tRegularization: 0.0055\n",
            "Iter: 15210  \tTraining Loss: -516.8391    \n",
            "    Negative Log Likelihood: 29.1914\tSigma2 Prior: -546.0360\tRegularization: 0.0055\n",
            "Iter: 15220  \tTraining Loss: -505.4184    \n",
            "    Negative Log Likelihood: 37.5034\tSigma2 Prior: -542.9273\tRegularization: 0.0055\n",
            "Iter: 15230  \tTraining Loss: -402.9836    \n",
            "    Negative Log Likelihood: 29.1585\tSigma2 Prior: -432.1476\tRegularization: 0.0055\n",
            "Iter: 15240  \tTraining Loss: -490.6994    \n",
            "    Negative Log Likelihood: 24.9184\tSigma2 Prior: -515.6232\tRegularization: 0.0055\n",
            "Iter: 15250  \tTraining Loss: -503.7421    \n",
            "    Negative Log Likelihood: 39.9453\tSigma2 Prior: -543.6929\tRegularization: 0.0055\n",
            "Iter: 15260  \tTraining Loss: -494.3302    \n",
            "    Negative Log Likelihood: 25.4974\tSigma2 Prior: -519.8332\tRegularization: 0.0055\n",
            "Iter: 15270  \tTraining Loss: -501.7637    \n",
            "    Negative Log Likelihood: 28.8496\tSigma2 Prior: -530.6188\tRegularization: 0.0055\n",
            "Iter: 15280  \tTraining Loss: -461.9608    \n",
            "    Negative Log Likelihood: 34.5418\tSigma2 Prior: -496.5082\tRegularization: 0.0055\n",
            "Iter: 15290  \tTraining Loss: -495.4240    \n",
            "    Negative Log Likelihood: 26.2382\tSigma2 Prior: -521.6677\tRegularization: 0.0055\n",
            "Iter: 15300  \tTraining Loss: -511.0431    \n",
            "    Negative Log Likelihood: 37.8855\tSigma2 Prior: -548.9341\tRegularization: 0.0055\n",
            "Iter: 15310  \tTraining Loss: -494.6727    \n",
            "    Negative Log Likelihood: 29.4196\tSigma2 Prior: -524.0978\tRegularization: 0.0055\n",
            "Iter: 15320  \tTraining Loss: -513.2256    \n",
            "    Negative Log Likelihood: 31.5886\tSigma2 Prior: -544.8198\tRegularization: 0.0055\n",
            "Iter: 15330  \tTraining Loss: -472.0854    \n",
            "    Negative Log Likelihood: 36.6142\tSigma2 Prior: -508.7052\tRegularization: 0.0055\n",
            "Iter: 15340  \tTraining Loss: -492.2992    \n",
            "    Negative Log Likelihood: 27.9152\tSigma2 Prior: -520.2199\tRegularization: 0.0055\n",
            "Iter: 15350  \tTraining Loss: -507.1806    \n",
            "    Negative Log Likelihood: 30.2106\tSigma2 Prior: -537.3967\tRegularization: 0.0055\n",
            "Iter: 15360  \tTraining Loss: -492.2195    \n",
            "    Negative Log Likelihood: 37.5917\tSigma2 Prior: -529.8167\tRegularization: 0.0055\n",
            "Iter: 15370  \tTraining Loss: -511.9632    \n",
            "    Negative Log Likelihood: 33.5891\tSigma2 Prior: -545.5577\tRegularization: 0.0055\n",
            "Iter: 15380  \tTraining Loss: -472.7497    \n",
            "    Negative Log Likelihood: 39.8231\tSigma2 Prior: -512.5782\tRegularization: 0.0055\n",
            "Iter: 15390  \tTraining Loss: -499.5860    \n",
            "    Negative Log Likelihood: 27.2188\tSigma2 Prior: -526.8103\tRegularization: 0.0055\n",
            "Iter: 15400  \tTraining Loss: -506.5004    \n",
            "    Negative Log Likelihood: 34.8302\tSigma2 Prior: -541.3361\tRegularization: 0.0055\n",
            "Iter: 15410  \tTraining Loss: -506.8254    \n",
            "    Negative Log Likelihood: 32.1161\tSigma2 Prior: -538.9471\tRegularization: 0.0055\n",
            "Iter: 15420  \tTraining Loss: -509.0724    \n",
            "    Negative Log Likelihood: 35.8719\tSigma2 Prior: -544.9498\tRegularization: 0.0055\n",
            "Iter: 15430  \tTraining Loss: -495.4328    \n",
            "    Negative Log Likelihood: 35.5734\tSigma2 Prior: -531.0118\tRegularization: 0.0055\n",
            "Iter: 15440  \tTraining Loss: -521.9332    \n",
            "    Negative Log Likelihood: 29.4956\tSigma2 Prior: -551.4344\tRegularization: 0.0055\n",
            "Iter: 15450  \tTraining Loss: -484.8157    \n",
            "    Negative Log Likelihood: 32.0546\tSigma2 Prior: -516.8758\tRegularization: 0.0055\n",
            "Iter: 15460  \tTraining Loss: -460.8336    \n",
            "    Negative Log Likelihood: 31.5884\tSigma2 Prior: -492.4276\tRegularization: 0.0055\n",
            "Iter: 15470  \tTraining Loss: -495.7290    \n",
            "    Negative Log Likelihood: 37.5559\tSigma2 Prior: -533.2904\tRegularization: 0.0056\n",
            "Iter: 15480  \tTraining Loss: -518.6451    \n",
            "    Negative Log Likelihood: 33.8484\tSigma2 Prior: -552.4991\tRegularization: 0.0056\n",
            "Iter: 15490  \tTraining Loss: -490.8773    \n",
            "    Negative Log Likelihood: 33.2468\tSigma2 Prior: -524.1296\tRegularization: 0.0056\n",
            "Iter: 15500  \tTraining Loss: -500.1660    \n",
            "    Negative Log Likelihood: 30.3733\tSigma2 Prior: -530.5449\tRegularization: 0.0056\n",
            "Iter: 15510  \tTraining Loss: -487.3683    \n",
            "    Negative Log Likelihood: 38.4393\tSigma2 Prior: -525.8131\tRegularization: 0.0056\n",
            "Iter: 15520  \tTraining Loss: -490.6909    \n",
            "    Negative Log Likelihood: 35.3646\tSigma2 Prior: -526.0610\tRegularization: 0.0056\n",
            "Iter: 15530  \tTraining Loss: -470.0363    \n",
            "    Negative Log Likelihood: 31.7557\tSigma2 Prior: -501.7975\tRegularization: 0.0056\n",
            "Iter: 15540  \tTraining Loss: -492.7393    \n",
            "    Negative Log Likelihood: 34.8415\tSigma2 Prior: -527.5864\tRegularization: 0.0056\n",
            "Iter: 15550  \tTraining Loss: -490.3302    \n",
            "    Negative Log Likelihood: 30.8817\tSigma2 Prior: -521.2175\tRegularization: 0.0056\n",
            "Iter: 15560  \tTraining Loss: -497.6236    \n",
            "    Negative Log Likelihood: 33.6562\tSigma2 Prior: -531.2854\tRegularization: 0.0056\n",
            "Iter: 15570  \tTraining Loss: -527.3917    \n",
            "    Negative Log Likelihood: 33.3734\tSigma2 Prior: -560.7706\tRegularization: 0.0056\n",
            "Iter: 15580  \tTraining Loss: -486.3102    \n",
            "    Negative Log Likelihood: 30.5221\tSigma2 Prior: -516.8379\tRegularization: 0.0056\n",
            "Iter: 15590  \tTraining Loss: -506.5233    \n",
            "    Negative Log Likelihood: 23.8331\tSigma2 Prior: -530.3620\tRegularization: 0.0056\n",
            "Iter: 15600  \tTraining Loss: -479.1028    \n",
            "    Negative Log Likelihood: 33.5370\tSigma2 Prior: -512.6454\tRegularization: 0.0056\n",
            "Iter: 15610  \tTraining Loss: -493.8587    \n",
            "    Negative Log Likelihood: 32.1566\tSigma2 Prior: -526.0208\tRegularization: 0.0056\n",
            "Iter: 15620  \tTraining Loss: -486.2062    \n",
            "    Negative Log Likelihood: 28.1560\tSigma2 Prior: -514.3678\tRegularization: 0.0056\n",
            "Iter: 15630  \tTraining Loss: -518.8930    \n",
            "    Negative Log Likelihood: 30.0470\tSigma2 Prior: -548.9456\tRegularization: 0.0056\n",
            "Iter: 15640  \tTraining Loss: -488.4976    \n",
            "    Negative Log Likelihood: 33.9704\tSigma2 Prior: -522.4736\tRegularization: 0.0056\n",
            "Iter: 15650  \tTraining Loss: -492.4694    \n",
            "    Negative Log Likelihood: 31.6761\tSigma2 Prior: -524.1511\tRegularization: 0.0056\n",
            "Iter: 15660  \tTraining Loss: -492.6902    \n",
            "    Negative Log Likelihood: 34.6010\tSigma2 Prior: -527.2968\tRegularization: 0.0056\n",
            "Iter: 15670  \tTraining Loss: -450.8954    \n",
            "    Negative Log Likelihood: 35.3277\tSigma2 Prior: -486.2286\tRegularization: 0.0056\n",
            "Iter: 15680  \tTraining Loss: -494.0890    \n",
            "    Negative Log Likelihood: 32.6150\tSigma2 Prior: -526.7096\tRegularization: 0.0056\n",
            "Iter: 15690  \tTraining Loss: -506.6413    \n",
            "    Negative Log Likelihood: 29.6413\tSigma2 Prior: -536.2882\tRegularization: 0.0056\n",
            "Iter: 15700  \tTraining Loss: -500.1740    \n",
            "    Negative Log Likelihood: 35.5378\tSigma2 Prior: -535.7173\tRegularization: 0.0056\n",
            "Iter: 15710  \tTraining Loss: -471.3461    \n",
            "    Negative Log Likelihood: 32.1899\tSigma2 Prior: -503.5416\tRegularization: 0.0056\n",
            "Iter: 15720  \tTraining Loss: -482.2965    \n",
            "    Negative Log Likelihood: 27.9501\tSigma2 Prior: -510.2523\tRegularization: 0.0056\n",
            "Iter: 15730  \tTraining Loss: -498.7734    \n",
            "    Negative Log Likelihood: 32.8290\tSigma2 Prior: -531.6080\tRegularization: 0.0056\n",
            "Iter: 15740  \tTraining Loss: -517.3277    \n",
            "    Negative Log Likelihood: 30.7252\tSigma2 Prior: -548.0585\tRegularization: 0.0056\n",
            "Iter: 15750  \tTraining Loss: -498.3700    \n",
            "    Negative Log Likelihood: 26.2442\tSigma2 Prior: -524.6198\tRegularization: 0.0056\n",
            "Iter: 15760  \tTraining Loss: -522.9562    \n",
            "    Negative Log Likelihood: 36.1560\tSigma2 Prior: -559.1179\tRegularization: 0.0056\n",
            "Iter: 15770  \tTraining Loss: -487.0690    \n",
            "    Negative Log Likelihood: 29.2628\tSigma2 Prior: -516.3375\tRegularization: 0.0056\n",
            "Iter: 15780  \tTraining Loss: -518.2003    \n",
            "    Negative Log Likelihood: 34.5513\tSigma2 Prior: -552.7572\tRegularization: 0.0056\n",
            "Iter: 15790  \tTraining Loss: -507.9926    \n",
            "    Negative Log Likelihood: 31.3215\tSigma2 Prior: -539.3198\tRegularization: 0.0056\n",
            "Iter: 15800  \tTraining Loss: -477.8434    \n",
            "    Negative Log Likelihood: 35.6357\tSigma2 Prior: -513.4847\tRegularization: 0.0056\n",
            "Iter: 15810  \tTraining Loss: -497.8655    \n",
            "    Negative Log Likelihood: 31.4840\tSigma2 Prior: -529.3551\tRegularization: 0.0056\n",
            "Iter: 15820  \tTraining Loss: -454.4119    \n",
            "    Negative Log Likelihood: 30.1776\tSigma2 Prior: -484.5952\tRegularization: 0.0056\n",
            "Iter: 15830  \tTraining Loss: -504.6636    \n",
            "    Negative Log Likelihood: 36.7052\tSigma2 Prior: -541.3744\tRegularization: 0.0056\n",
            "Iter: 15840  \tTraining Loss: -485.3978    \n",
            "    Negative Log Likelihood: 29.2417\tSigma2 Prior: -514.6451\tRegularization: 0.0056\n",
            "Iter: 15850  \tTraining Loss: -508.8888    \n",
            "    Negative Log Likelihood: 27.9910\tSigma2 Prior: -536.8854\tRegularization: 0.0056\n",
            "Iter: 15860  \tTraining Loss: -494.9589    \n",
            "    Negative Log Likelihood: 39.0526\tSigma2 Prior: -534.0171\tRegularization: 0.0056\n",
            "Iter: 15870  \tTraining Loss: -491.5641    \n",
            "    Negative Log Likelihood: 32.9407\tSigma2 Prior: -524.5104\tRegularization: 0.0056\n",
            "Iter: 15880  \tTraining Loss: -467.2770    \n",
            "    Negative Log Likelihood: 30.4698\tSigma2 Prior: -497.7525\tRegularization: 0.0056\n",
            "Iter: 15890  \tTraining Loss: -513.5657    \n",
            "    Negative Log Likelihood: 33.5202\tSigma2 Prior: -547.0915\tRegularization: 0.0056\n",
            "Iter: 15900  \tTraining Loss: -458.2804    \n",
            "    Negative Log Likelihood: 39.5192\tSigma2 Prior: -497.8052\tRegularization: 0.0056\n",
            "Iter: 15910  \tTraining Loss: -494.7268    \n",
            "    Negative Log Likelihood: 23.6473\tSigma2 Prior: -518.3798\tRegularization: 0.0056\n",
            "Iter: 15920  \tTraining Loss: -509.9860    \n",
            "    Negative Log Likelihood: 30.0163\tSigma2 Prior: -540.0080\tRegularization: 0.0056\n",
            "Iter: 15930  \tTraining Loss: -459.7696    \n",
            "    Negative Log Likelihood: 40.6972\tSigma2 Prior: -500.4725\tRegularization: 0.0056\n",
            "Iter: 15940  \tTraining Loss: -520.6972    \n",
            "    Negative Log Likelihood: 29.5482\tSigma2 Prior: -550.2510\tRegularization: 0.0056\n",
            "Iter: 15950  \tTraining Loss: -515.9556    \n",
            "    Negative Log Likelihood: 34.4931\tSigma2 Prior: -550.4544\tRegularization: 0.0056\n",
            "Iter: 15960  \tTraining Loss: -490.0728    \n",
            "    Negative Log Likelihood: 30.2495\tSigma2 Prior: -520.3280\tRegularization: 0.0056\n",
            "Iter: 15970  \tTraining Loss: -474.1539    \n",
            "    Negative Log Likelihood: 35.2514\tSigma2 Prior: -509.4109\tRegularization: 0.0056\n",
            "Iter: 15980  \tTraining Loss: -488.3594    \n",
            "    Negative Log Likelihood: 34.4626\tSigma2 Prior: -522.8277\tRegularization: 0.0057\n",
            "Iter: 15990  \tTraining Loss: -485.4503    \n",
            "    Negative Log Likelihood: 32.2346\tSigma2 Prior: -517.6906\tRegularization: 0.0057\n",
            "Iter: 16000  \tTraining Loss: -470.4836    \n",
            "    Negative Log Likelihood: 35.3403\tSigma2 Prior: -505.8295\tRegularization: 0.0057\n",
            "Iter: 16010  \tTraining Loss: -485.9615    \n",
            "    Negative Log Likelihood: 30.9021\tSigma2 Prior: -516.8693\tRegularization: 0.0057\n",
            "Iter: 16020  \tTraining Loss: -458.3699    \n",
            "    Negative Log Likelihood: 31.5453\tSigma2 Prior: -489.9209\tRegularization: 0.0057\n",
            "Iter: 16030  \tTraining Loss: -470.7475    \n",
            "    Negative Log Likelihood: 35.1522\tSigma2 Prior: -505.9054\tRegularization: 0.0057\n",
            "Iter: 16040  \tTraining Loss: -498.5629    \n",
            "    Negative Log Likelihood: 27.9066\tSigma2 Prior: -526.4751\tRegularization: 0.0057\n",
            "Iter: 16050  \tTraining Loss: -477.1251    \n",
            "    Negative Log Likelihood: 37.0725\tSigma2 Prior: -514.2033\tRegularization: 0.0057\n",
            "Iter: 16060  \tTraining Loss: -481.7385    \n",
            "    Negative Log Likelihood: 28.8630\tSigma2 Prior: -510.6072\tRegularization: 0.0057\n",
            "Iter: 16070  \tTraining Loss: -466.9859    \n",
            "    Negative Log Likelihood: 31.5083\tSigma2 Prior: -498.4999\tRegularization: 0.0057\n",
            "Iter: 16080  \tTraining Loss: -509.4860    \n",
            "    Negative Log Likelihood: 32.0672\tSigma2 Prior: -541.5588\tRegularization: 0.0057\n",
            "Iter: 16090  \tTraining Loss: -515.5638    \n",
            "    Negative Log Likelihood: 31.8037\tSigma2 Prior: -547.3732\tRegularization: 0.0057\n",
            "Iter: 16100  \tTraining Loss: -469.1585    \n",
            "    Negative Log Likelihood: 33.9942\tSigma2 Prior: -503.1584\tRegularization: 0.0057\n",
            "Iter: 16110  \tTraining Loss: -412.5134    \n",
            "    Negative Log Likelihood: 27.9785\tSigma2 Prior: -440.4975\tRegularization: 0.0057\n",
            "Iter: 16120  \tTraining Loss: -495.5655    \n",
            "    Negative Log Likelihood: 29.5944\tSigma2 Prior: -525.1656\tRegularization: 0.0057\n",
            "Iter: 16130  \tTraining Loss: -483.0748    \n",
            "    Negative Log Likelihood: 28.5720\tSigma2 Prior: -511.6524\tRegularization: 0.0057\n",
            "Iter: 16140  \tTraining Loss: -484.0649    \n",
            "    Negative Log Likelihood: 33.2308\tSigma2 Prior: -517.3014\tRegularization: 0.0057\n",
            "Iter: 16150  \tTraining Loss: -488.0892    \n",
            "    Negative Log Likelihood: 37.6902\tSigma2 Prior: -525.7850\tRegularization: 0.0057\n",
            "Iter: 16160  \tTraining Loss: -509.9753    \n",
            "    Negative Log Likelihood: 32.3138\tSigma2 Prior: -542.2947\tRegularization: 0.0057\n",
            "Iter: 16170  \tTraining Loss: -522.2799    \n",
            "    Negative Log Likelihood: 28.8396\tSigma2 Prior: -551.1252\tRegularization: 0.0057\n",
            "Iter: 16180  \tTraining Loss: -510.5537    \n",
            "    Negative Log Likelihood: 27.3054\tSigma2 Prior: -537.8648\tRegularization: 0.0057\n",
            "Iter: 16190  \tTraining Loss: -524.8727    \n",
            "    Negative Log Likelihood: 37.4635\tSigma2 Prior: -562.3420\tRegularization: 0.0057\n",
            "Iter: 16200  \tTraining Loss: -481.4446    \n",
            "    Negative Log Likelihood: 29.8043\tSigma2 Prior: -511.2546\tRegularization: 0.0057\n",
            "Iter: 16210  \tTraining Loss: -482.6721    \n",
            "    Negative Log Likelihood: 30.1434\tSigma2 Prior: -512.8212\tRegularization: 0.0057\n",
            "Iter: 16220  \tTraining Loss: -459.4456    \n",
            "    Negative Log Likelihood: 33.5252\tSigma2 Prior: -492.9765\tRegularization: 0.0057\n",
            "Iter: 16230  \tTraining Loss: -493.3752    \n",
            "    Negative Log Likelihood: 30.0542\tSigma2 Prior: -523.4351\tRegularization: 0.0057\n",
            "Iter: 16240  \tTraining Loss: -484.8947    \n",
            "    Negative Log Likelihood: 33.6645\tSigma2 Prior: -518.5649\tRegularization: 0.0057\n",
            "Iter: 16250  \tTraining Loss: -493.3950    \n",
            "    Negative Log Likelihood: 29.0598\tSigma2 Prior: -522.4605\tRegularization: 0.0057\n",
            "Iter: 16260  \tTraining Loss: -517.7241    \n",
            "    Negative Log Likelihood: 32.5322\tSigma2 Prior: -550.2620\tRegularization: 0.0057\n",
            "Iter: 16270  \tTraining Loss: -505.7611    \n",
            "    Negative Log Likelihood: 41.2143\tSigma2 Prior: -546.9811\tRegularization: 0.0057\n",
            "Iter: 16280  \tTraining Loss: -476.0706    \n",
            "    Negative Log Likelihood: 28.2833\tSigma2 Prior: -504.3596\tRegularization: 0.0057\n",
            "Iter: 16290  \tTraining Loss: -449.8439    \n",
            "    Negative Log Likelihood: 35.9614\tSigma2 Prior: -485.8110\tRegularization: 0.0057\n",
            "Iter: 16300  \tTraining Loss: -507.8026    \n",
            "    Negative Log Likelihood: 29.9933\tSigma2 Prior: -537.8016\tRegularization: 0.0057\n",
            "Iter: 16310  \tTraining Loss: -482.9392    \n",
            "    Negative Log Likelihood: 35.3104\tSigma2 Prior: -518.2553\tRegularization: 0.0057\n",
            "Iter: 16320  \tTraining Loss: -504.4952    \n",
            "    Negative Log Likelihood: 35.2667\tSigma2 Prior: -539.7676\tRegularization: 0.0057\n",
            "Iter: 16330  \tTraining Loss: -456.3795    \n",
            "    Negative Log Likelihood: 32.8042\tSigma2 Prior: -489.1893\tRegularization: 0.0057\n",
            "Iter: 16340  \tTraining Loss: -504.8420    \n",
            "    Negative Log Likelihood: 29.7957\tSigma2 Prior: -534.6434\tRegularization: 0.0057\n",
            "Iter: 16350  \tTraining Loss: -476.8730    \n",
            "    Negative Log Likelihood: 35.5734\tSigma2 Prior: -512.4521\tRegularization: 0.0057\n",
            "Iter: 16360  \tTraining Loss: -499.5421    \n",
            "    Negative Log Likelihood: 27.1766\tSigma2 Prior: -526.7245\tRegularization: 0.0057\n",
            "Iter: 16370  \tTraining Loss: -436.8567    \n",
            "    Negative Log Likelihood: 37.1897\tSigma2 Prior: -474.0521\tRegularization: 0.0057\n",
            "Iter: 16380  \tTraining Loss: -475.8030    \n",
            "    Negative Log Likelihood: 26.5674\tSigma2 Prior: -502.3762\tRegularization: 0.0057\n",
            "Iter: 16390  \tTraining Loss: -492.7971    \n",
            "    Negative Log Likelihood: 29.3539\tSigma2 Prior: -522.1567\tRegularization: 0.0057\n",
            "Iter: 16400  \tTraining Loss: -464.6126    \n",
            "    Negative Log Likelihood: 39.2662\tSigma2 Prior: -503.8846\tRegularization: 0.0057\n",
            "Iter: 16410  \tTraining Loss: -481.4467    \n",
            "    Negative Log Likelihood: 32.3142\tSigma2 Prior: -513.7666\tRegularization: 0.0057\n",
            "Iter: 16420  \tTraining Loss: -505.8394    \n",
            "    Negative Log Likelihood: 34.2410\tSigma2 Prior: -540.0862\tRegularization: 0.0057\n",
            "Iter: 16430  \tTraining Loss: -497.4511    \n",
            "    Negative Log Likelihood: 33.0897\tSigma2 Prior: -530.5465\tRegularization: 0.0057\n",
            "Iter: 16440  \tTraining Loss: -518.9465    \n",
            "    Negative Log Likelihood: 33.3732\tSigma2 Prior: -552.3254\tRegularization: 0.0057\n",
            "Iter: 16450  \tTraining Loss: -512.6989    \n",
            "    Negative Log Likelihood: 36.5095\tSigma2 Prior: -549.2142\tRegularization: 0.0057\n",
            "Iter: 16460  \tTraining Loss: -511.2559    \n",
            "    Negative Log Likelihood: 32.5370\tSigma2 Prior: -543.7986\tRegularization: 0.0057\n",
            "Iter: 16470  \tTraining Loss: -501.0623    \n",
            "    Negative Log Likelihood: 32.8228\tSigma2 Prior: -533.8909\tRegularization: 0.0057\n",
            "Iter: 16480  \tTraining Loss: -506.9225    \n",
            "    Negative Log Likelihood: 28.9446\tSigma2 Prior: -535.8728\tRegularization: 0.0057\n",
            "Iter: 16490  \tTraining Loss: -507.3527    \n",
            "    Negative Log Likelihood: 35.6480\tSigma2 Prior: -543.0064\tRegularization: 0.0057\n",
            "Iter: 16500  \tTraining Loss: -503.5312    \n",
            "    Negative Log Likelihood: 35.5601\tSigma2 Prior: -539.0971\tRegularization: 0.0058\n",
            "Iter: 16510  \tTraining Loss: -500.9469    \n",
            "    Negative Log Likelihood: 28.1718\tSigma2 Prior: -529.1245\tRegularization: 0.0058\n",
            "Iter: 16520  \tTraining Loss: -463.2566    \n",
            "    Negative Log Likelihood: 30.8696\tSigma2 Prior: -494.1320\tRegularization: 0.0058\n",
            "Iter: 16530  \tTraining Loss: -513.5270    \n",
            "    Negative Log Likelihood: 37.1032\tSigma2 Prior: -550.6359\tRegularization: 0.0058\n",
            "Iter: 16540  \tTraining Loss: -483.7473    \n",
            "    Negative Log Likelihood: 27.7440\tSigma2 Prior: -511.4971\tRegularization: 0.0058\n",
            "Iter: 16550  \tTraining Loss: -499.2539    \n",
            "    Negative Log Likelihood: 31.0780\tSigma2 Prior: -530.3376\tRegularization: 0.0058\n",
            "Iter: 16560  \tTraining Loss: -471.9861    \n",
            "    Negative Log Likelihood: 34.8340\tSigma2 Prior: -506.8259\tRegularization: 0.0058\n",
            "Iter: 16570  \tTraining Loss: -481.7739    \n",
            "    Negative Log Likelihood: 28.9808\tSigma2 Prior: -510.7604\tRegularization: 0.0058\n",
            "Iter: 16580  \tTraining Loss: -492.7563    \n",
            "    Negative Log Likelihood: 34.3411\tSigma2 Prior: -527.1032\tRegularization: 0.0058\n",
            "Iter: 16590  \tTraining Loss: -501.7878    \n",
            "    Negative Log Likelihood: 29.4143\tSigma2 Prior: -531.2078\tRegularization: 0.0058\n",
            "Iter: 16600  \tTraining Loss: -511.0855    \n",
            "    Negative Log Likelihood: 39.9506\tSigma2 Prior: -551.0419\tRegularization: 0.0058\n",
            "Iter: 16610  \tTraining Loss: -459.9140    \n",
            "    Negative Log Likelihood: 28.2338\tSigma2 Prior: -488.1536\tRegularization: 0.0058\n",
            "Iter: 16620  \tTraining Loss: -477.5332    \n",
            "    Negative Log Likelihood: 31.2942\tSigma2 Prior: -508.8331\tRegularization: 0.0058\n",
            "Iter: 16630  \tTraining Loss: -514.9132    \n",
            "    Negative Log Likelihood: 30.2659\tSigma2 Prior: -545.1849\tRegularization: 0.0058\n",
            "Iter: 16640  \tTraining Loss: -493.8771    \n",
            "    Negative Log Likelihood: 26.5379\tSigma2 Prior: -520.4208\tRegularization: 0.0058\n",
            "Iter: 16650  \tTraining Loss: -515.7819    \n",
            "    Negative Log Likelihood: 33.8340\tSigma2 Prior: -549.6216\tRegularization: 0.0058\n",
            "Iter: 16660  \tTraining Loss: -495.9823    \n",
            "    Negative Log Likelihood: 34.2622\tSigma2 Prior: -530.2503\tRegularization: 0.0058\n",
            "Iter: 16670  \tTraining Loss: -500.5211    \n",
            "    Negative Log Likelihood: 33.1710\tSigma2 Prior: -533.6978\tRegularization: 0.0058\n",
            "Iter: 16680  \tTraining Loss: -506.6317    \n",
            "    Negative Log Likelihood: 29.4453\tSigma2 Prior: -536.0828\tRegularization: 0.0058\n",
            "Iter: 16690  \tTraining Loss: -502.2560    \n",
            "    Negative Log Likelihood: 32.9768\tSigma2 Prior: -535.2386\tRegularization: 0.0058\n",
            "Iter: 16700  \tTraining Loss: -517.5178    \n",
            "    Negative Log Likelihood: 27.0420\tSigma2 Prior: -544.5656\tRegularization: 0.0058\n",
            "Iter: 16710  \tTraining Loss: -489.5242    \n",
            "    Negative Log Likelihood: 37.0927\tSigma2 Prior: -526.6227\tRegularization: 0.0058\n",
            "Iter: 16720  \tTraining Loss: -479.5535    \n",
            "    Negative Log Likelihood: 26.9133\tSigma2 Prior: -506.4725\tRegularization: 0.0058\n",
            "Iter: 16730  \tTraining Loss: -478.5822    \n",
            "    Negative Log Likelihood: 30.9361\tSigma2 Prior: -509.5241\tRegularization: 0.0058\n",
            "Iter: 16740  \tTraining Loss: -493.5638    \n",
            "    Negative Log Likelihood: 36.8730\tSigma2 Prior: -530.4425\tRegularization: 0.0058\n",
            "Iter: 16750  \tTraining Loss: -514.5880    \n",
            "    Negative Log Likelihood: 29.3916\tSigma2 Prior: -543.9854\tRegularization: 0.0058\n",
            "Iter: 16760  \tTraining Loss: -501.1253    \n",
            "    Negative Log Likelihood: 36.4463\tSigma2 Prior: -537.5773\tRegularization: 0.0058\n",
            "Iter: 16770  \tTraining Loss: -506.5950    \n",
            "    Negative Log Likelihood: 35.9423\tSigma2 Prior: -542.5430\tRegularization: 0.0058\n",
            "Iter: 16780  \tTraining Loss: -512.5043    \n",
            "    Negative Log Likelihood: 28.7341\tSigma2 Prior: -541.2442\tRegularization: 0.0058\n",
            "Iter: 16790  \tTraining Loss: -481.7716    \n",
            "    Negative Log Likelihood: 31.1654\tSigma2 Prior: -512.9428\tRegularization: 0.0058\n",
            "Iter: 16800  \tTraining Loss: -488.7330    \n",
            "    Negative Log Likelihood: 39.4334\tSigma2 Prior: -528.1722\tRegularization: 0.0058\n",
            "Iter: 16810  \tTraining Loss: -501.0421    \n",
            "    Negative Log Likelihood: 29.5790\tSigma2 Prior: -530.6268\tRegularization: 0.0058\n",
            "Iter: 16820  \tTraining Loss: -514.2711    \n",
            "    Negative Log Likelihood: 30.8184\tSigma2 Prior: -545.0953\tRegularization: 0.0058\n",
            "Iter: 16830  \tTraining Loss: -475.9271    \n",
            "    Negative Log Likelihood: 35.6716\tSigma2 Prior: -511.6046\tRegularization: 0.0058\n",
            "Iter: 16840  \tTraining Loss: -478.0452    \n",
            "    Negative Log Likelihood: 33.2387\tSigma2 Prior: -511.2896\tRegularization: 0.0058\n",
            "Iter: 16850  \tTraining Loss: -508.0814    \n",
            "    Negative Log Likelihood: 30.0829\tSigma2 Prior: -538.1701\tRegularization: 0.0058\n",
            "Iter: 16860  \tTraining Loss: -519.7468    \n",
            "    Negative Log Likelihood: 34.3321\tSigma2 Prior: -554.0847\tRegularization: 0.0058\n",
            "Iter: 16870  \tTraining Loss: -480.5421    \n",
            "    Negative Log Likelihood: 29.4085\tSigma2 Prior: -509.9565\tRegularization: 0.0058\n",
            "Iter: 16880  \tTraining Loss: -500.3784    \n",
            "    Negative Log Likelihood: 31.3737\tSigma2 Prior: -531.7579\tRegularization: 0.0058\n",
            "Iter: 16890  \tTraining Loss: -496.0833    \n",
            "    Negative Log Likelihood: 33.3366\tSigma2 Prior: -529.4257\tRegularization: 0.0058\n",
            "Iter: 16900  \tTraining Loss: -503.4493    \n",
            "    Negative Log Likelihood: 28.3564\tSigma2 Prior: -531.8115\tRegularization: 0.0058\n",
            "Iter: 16910  \tTraining Loss: -461.9116    \n",
            "    Negative Log Likelihood: 36.2596\tSigma2 Prior: -498.1769\tRegularization: 0.0058\n",
            "Iter: 16920  \tTraining Loss: -449.4962    \n",
            "    Negative Log Likelihood: 34.0284\tSigma2 Prior: -483.5305\tRegularization: 0.0058\n",
            "Iter: 16930  \tTraining Loss: -505.5105    \n",
            "    Negative Log Likelihood: 28.3390\tSigma2 Prior: -533.8553\tRegularization: 0.0058\n",
            "Iter: 16940  \tTraining Loss: -485.8430    \n",
            "    Negative Log Likelihood: 38.9674\tSigma2 Prior: -524.8162\tRegularization: 0.0058\n",
            "Iter: 16950  \tTraining Loss: -504.8958    \n",
            "    Negative Log Likelihood: 28.4267\tSigma2 Prior: -533.3283\tRegularization: 0.0058\n",
            "Iter: 16960  \tTraining Loss: -491.4482    \n",
            "    Negative Log Likelihood: 32.3465\tSigma2 Prior: -523.8005\tRegularization: 0.0058\n",
            "Iter: 16970  \tTraining Loss: -511.1401    \n",
            "    Negative Log Likelihood: 35.0693\tSigma2 Prior: -546.2152\tRegularization: 0.0058\n",
            "Iter: 16980  \tTraining Loss: -452.7895    \n",
            "    Negative Log Likelihood: 30.1348\tSigma2 Prior: -482.9301\tRegularization: 0.0058\n",
            "Iter: 16990  \tTraining Loss: -519.3046    \n",
            "    Negative Log Likelihood: 28.5804\tSigma2 Prior: -547.8909\tRegularization: 0.0058\n",
            "Iter: 17000  \tTraining Loss: -445.0160    \n",
            "    Negative Log Likelihood: 39.3982\tSigma2 Prior: -484.4200\tRegularization: 0.0058\n",
            "Iter: 17010  \tTraining Loss: -489.3326    \n",
            "    Negative Log Likelihood: 28.8847\tSigma2 Prior: -518.2231\tRegularization: 0.0058\n",
            "Iter: 17020  \tTraining Loss: -512.4131    \n",
            "    Negative Log Likelihood: 33.2198\tSigma2 Prior: -545.6388\tRegularization: 0.0058\n",
            "Iter: 17030  \tTraining Loss: -482.3591    \n",
            "    Negative Log Likelihood: 35.2224\tSigma2 Prior: -517.5873\tRegularization: 0.0058\n",
            "Iter: 17040  \tTraining Loss: -467.4874    \n",
            "    Negative Log Likelihood: 32.7291\tSigma2 Prior: -500.2223\tRegularization: 0.0058\n",
            "Iter: 17050  \tTraining Loss: -502.3768    \n",
            "    Negative Log Likelihood: 31.3821\tSigma2 Prior: -533.7647\tRegularization: 0.0059\n",
            "Iter: 17060  \tTraining Loss: -510.1430    \n",
            "    Negative Log Likelihood: 35.7145\tSigma2 Prior: -545.8633\tRegularization: 0.0059\n",
            "Iter: 17070  \tTraining Loss: -532.0638    \n",
            "    Negative Log Likelihood: 31.8118\tSigma2 Prior: -563.8815\tRegularization: 0.0059\n",
            "Iter: 17080  \tTraining Loss: -487.1437    \n",
            "    Negative Log Likelihood: 36.3441\tSigma2 Prior: -523.4937\tRegularization: 0.0059\n",
            "Iter: 17090  \tTraining Loss: -501.2099    \n",
            "    Negative Log Likelihood: 31.1166\tSigma2 Prior: -532.3323\tRegularization: 0.0059\n",
            "Iter: 17100  \tTraining Loss: -473.8208    \n",
            "    Negative Log Likelihood: 34.6818\tSigma2 Prior: -508.5085\tRegularization: 0.0059\n",
            "Iter: 17110  \tTraining Loss: -494.6642    \n",
            "    Negative Log Likelihood: 33.5381\tSigma2 Prior: -528.2081\tRegularization: 0.0059\n",
            "Iter: 17120  \tTraining Loss: -499.9673    \n",
            "    Negative Log Likelihood: 30.4190\tSigma2 Prior: -530.3922\tRegularization: 0.0059\n",
            "Iter: 17130  \tTraining Loss: -479.7450    \n",
            "    Negative Log Likelihood: 29.6429\tSigma2 Prior: -509.3937\tRegularization: 0.0059\n",
            "Iter: 17140  \tTraining Loss: -490.2002    \n",
            "    Negative Log Likelihood: 33.1089\tSigma2 Prior: -523.3150\tRegularization: 0.0059\n",
            "Iter: 17150  \tTraining Loss: -437.7031    \n",
            "    Negative Log Likelihood: 34.6488\tSigma2 Prior: -472.3578\tRegularization: 0.0059\n",
            "Iter: 17160  \tTraining Loss: -474.8428    \n",
            "    Negative Log Likelihood: 26.0678\tSigma2 Prior: -500.9165\tRegularization: 0.0059\n",
            "Iter: 17170  \tTraining Loss: -490.4027    \n",
            "    Negative Log Likelihood: 34.2808\tSigma2 Prior: -524.6893\tRegularization: 0.0059\n",
            "Iter: 17180  \tTraining Loss: -501.7127    \n",
            "    Negative Log Likelihood: 37.6332\tSigma2 Prior: -539.3518\tRegularization: 0.0059\n",
            "Iter: 17190  \tTraining Loss: -488.8389    \n",
            "    Negative Log Likelihood: 27.7067\tSigma2 Prior: -516.5516\tRegularization: 0.0059\n",
            "Iter: 17200  \tTraining Loss: -499.2269    \n",
            "    Negative Log Likelihood: 33.0706\tSigma2 Prior: -532.3033\tRegularization: 0.0059\n",
            "Iter: 17210  \tTraining Loss: -479.8135    \n",
            "    Negative Log Likelihood: 30.0828\tSigma2 Prior: -509.9022\tRegularization: 0.0059\n",
            "Iter: 17220  \tTraining Loss: -527.6863    \n",
            "    Negative Log Likelihood: 31.3135\tSigma2 Prior: -559.0057\tRegularization: 0.0059\n",
            "Iter: 17230  \tTraining Loss: -519.8524    \n",
            "    Negative Log Likelihood: 37.1018\tSigma2 Prior: -556.9600\tRegularization: 0.0059\n",
            "Iter: 17240  \tTraining Loss: -516.6619    \n",
            "    Negative Log Likelihood: 31.6264\tSigma2 Prior: -548.2941\tRegularization: 0.0059\n",
            "Iter: 17250  \tTraining Loss: -500.2007    \n",
            "    Negative Log Likelihood: 29.0300\tSigma2 Prior: -529.2365\tRegularization: 0.0059\n",
            "Iter: 17260  \tTraining Loss: -495.5766    \n",
            "    Negative Log Likelihood: 31.6773\tSigma2 Prior: -527.2598\tRegularization: 0.0059\n",
            "Iter: 17270  \tTraining Loss: -510.8916    \n",
            "    Negative Log Likelihood: 29.5722\tSigma2 Prior: -540.4697\tRegularization: 0.0059\n",
            "Iter: 17280  \tTraining Loss: -448.8102    \n",
            "    Negative Log Likelihood: 30.1352\tSigma2 Prior: -478.9513\tRegularization: 0.0059\n",
            "Iter: 17290  \tTraining Loss: -510.5169    \n",
            "    Negative Log Likelihood: 32.8463\tSigma2 Prior: -543.3691\tRegularization: 0.0059\n",
            "Iter: 17300  \tTraining Loss: -514.3576    \n",
            "    Negative Log Likelihood: 34.9167\tSigma2 Prior: -549.2803\tRegularization: 0.0059\n",
            "Iter: 17310  \tTraining Loss: -502.5649    \n",
            "    Negative Log Likelihood: 31.2869\tSigma2 Prior: -533.8577\tRegularization: 0.0059\n",
            "Iter: 17320  \tTraining Loss: -512.6285    \n",
            "    Negative Log Likelihood: 28.6104\tSigma2 Prior: -541.2449\tRegularization: 0.0059\n",
            "Iter: 17330  \tTraining Loss: -499.6493    \n",
            "    Negative Log Likelihood: 36.8307\tSigma2 Prior: -536.4858\tRegularization: 0.0059\n",
            "Iter: 17340  \tTraining Loss: -514.5478    \n",
            "    Negative Log Likelihood: 31.1917\tSigma2 Prior: -545.7454\tRegularization: 0.0059\n",
            "Iter: 17350  \tTraining Loss: -475.7593    \n",
            "    Negative Log Likelihood: 34.5645\tSigma2 Prior: -510.3297\tRegularization: 0.0059\n",
            "Iter: 17360  \tTraining Loss: -512.5991    \n",
            "    Negative Log Likelihood: 30.1849\tSigma2 Prior: -542.7899\tRegularization: 0.0059\n",
            "Iter: 17370  \tTraining Loss: -510.6353    \n",
            "    Negative Log Likelihood: 38.1903\tSigma2 Prior: -548.8315\tRegularization: 0.0059\n",
            "Iter: 17380  \tTraining Loss: -477.8418    \n",
            "    Negative Log Likelihood: 30.2219\tSigma2 Prior: -508.0696\tRegularization: 0.0059\n",
            "Iter: 17390  \tTraining Loss: -501.2316    \n",
            "    Negative Log Likelihood: 27.6381\tSigma2 Prior: -528.8756\tRegularization: 0.0059\n",
            "Iter: 17400  \tTraining Loss: -492.0945    \n",
            "    Negative Log Likelihood: 31.1552\tSigma2 Prior: -523.2557\tRegularization: 0.0059\n",
            "Iter: 17410  \tTraining Loss: -513.6672    \n",
            "    Negative Log Likelihood: 32.7896\tSigma2 Prior: -546.4627\tRegularization: 0.0059\n",
            "Iter: 17420  \tTraining Loss: -522.4785    \n",
            "    Negative Log Likelihood: 33.8948\tSigma2 Prior: -556.3792\tRegularization: 0.0059\n",
            "Iter: 17430  \tTraining Loss: -475.6899    \n",
            "    Negative Log Likelihood: 36.3385\tSigma2 Prior: -512.0343\tRegularization: 0.0059\n",
            "Iter: 17440  \tTraining Loss: -522.5466    \n",
            "    Negative Log Likelihood: 35.1503\tSigma2 Prior: -557.7028\tRegularization: 0.0059\n",
            "Iter: 17450  \tTraining Loss: -476.9961    \n",
            "    Negative Log Likelihood: 32.1807\tSigma2 Prior: -509.1827\tRegularization: 0.0059\n",
            "Iter: 17460  \tTraining Loss: -486.7633    \n",
            "    Negative Log Likelihood: 32.6087\tSigma2 Prior: -519.3779\tRegularization: 0.0059\n",
            "Iter: 17470  \tTraining Loss: -503.7693    \n",
            "    Negative Log Likelihood: 33.7021\tSigma2 Prior: -537.4773\tRegularization: 0.0059\n",
            "Iter: 17480  \tTraining Loss: -497.0798    \n",
            "    Negative Log Likelihood: 28.2900\tSigma2 Prior: -525.3757\tRegularization: 0.0059\n",
            "Iter: 17490  \tTraining Loss: -527.9861    \n",
            "    Negative Log Likelihood: 32.0314\tSigma2 Prior: -560.0234\tRegularization: 0.0059\n",
            "Iter: 17500  \tTraining Loss: -492.8625    \n",
            "    Negative Log Likelihood: 31.7884\tSigma2 Prior: -524.6569\tRegularization: 0.0059\n",
            "Iter: 17510  \tTraining Loss: -501.0909    \n",
            "    Negative Log Likelihood: 34.1994\tSigma2 Prior: -535.2963\tRegularization: 0.0059\n",
            "Iter: 17520  \tTraining Loss: -493.0394    \n",
            "    Negative Log Likelihood: 31.8323\tSigma2 Prior: -524.8776\tRegularization: 0.0059\n",
            "Iter: 17530  \tTraining Loss: -495.5221    \n",
            "    Negative Log Likelihood: 34.7082\tSigma2 Prior: -530.2362\tRegularization: 0.0059\n",
            "Iter: 17540  \tTraining Loss: -481.0130    \n",
            "    Negative Log Likelihood: 34.5962\tSigma2 Prior: -515.6151\tRegularization: 0.0059\n",
            "Iter: 17550  \tTraining Loss: -495.2252    \n",
            "    Negative Log Likelihood: 29.6780\tSigma2 Prior: -524.9091\tRegularization: 0.0059\n",
            "Iter: 17560  \tTraining Loss: -510.4397    \n",
            "    Negative Log Likelihood: 42.2866\tSigma2 Prior: -552.7322\tRegularization: 0.0059\n",
            "Iter: 17570  \tTraining Loss: -496.1320    \n",
            "    Negative Log Likelihood: 26.7225\tSigma2 Prior: -522.8604\tRegularization: 0.0059\n",
            "Iter: 17580  \tTraining Loss: -492.4041    \n",
            "    Negative Log Likelihood: 33.0418\tSigma2 Prior: -525.4518\tRegularization: 0.0059\n",
            "Iter: 17590  \tTraining Loss: -515.7471    \n",
            "    Negative Log Likelihood: 31.6681\tSigma2 Prior: -547.4212\tRegularization: 0.0059\n",
            "Iter: 17600  \tTraining Loss: -488.6903    \n",
            "    Negative Log Likelihood: 31.2206\tSigma2 Prior: -519.9168\tRegularization: 0.0059\n",
            "Iter: 17610  \tTraining Loss: -505.0800    \n",
            "    Negative Log Likelihood: 33.5886\tSigma2 Prior: -538.6746\tRegularization: 0.0059\n",
            "Iter: 17620  \tTraining Loss: -464.9032    \n",
            "    Negative Log Likelihood: 36.4191\tSigma2 Prior: -501.3282\tRegularization: 0.0059\n",
            "Iter: 17630  \tTraining Loss: -507.3934    \n",
            "    Negative Log Likelihood: 29.9275\tSigma2 Prior: -537.3268\tRegularization: 0.0059\n",
            "Iter: 17640  \tTraining Loss: -465.5588    \n",
            "    Negative Log Likelihood: 34.3460\tSigma2 Prior: -499.9108\tRegularization: 0.0060\n",
            "Iter: 17650  \tTraining Loss: -424.6100    \n",
            "    Negative Log Likelihood: 33.1256\tSigma2 Prior: -457.7416\tRegularization: 0.0060\n",
            "Iter: 17660  \tTraining Loss: -489.8813    \n",
            "    Negative Log Likelihood: 29.3394\tSigma2 Prior: -519.2266\tRegularization: 0.0060\n",
            "Iter: 17670  \tTraining Loss: -504.8551    \n",
            "    Negative Log Likelihood: 31.8739\tSigma2 Prior: -536.7350\tRegularization: 0.0060\n",
            "Iter: 17680  \tTraining Loss: -478.6698    \n",
            "    Negative Log Likelihood: 38.2162\tSigma2 Prior: -516.8920\tRegularization: 0.0060\n",
            "Iter: 17690  \tTraining Loss: -508.5967    \n",
            "    Negative Log Likelihood: 25.6524\tSigma2 Prior: -534.2551\tRegularization: 0.0060\n",
            "Iter: 17700  \tTraining Loss: -503.7862    \n",
            "    Negative Log Likelihood: 29.7868\tSigma2 Prior: -533.5790\tRegularization: 0.0060\n",
            "Iter: 17710  \tTraining Loss: -481.7522    \n",
            "    Negative Log Likelihood: 37.3748\tSigma2 Prior: -519.1329\tRegularization: 0.0060\n",
            "Iter: 17720  \tTraining Loss: -485.2169    \n",
            "    Negative Log Likelihood: 24.7847\tSigma2 Prior: -510.0076\tRegularization: 0.0060\n",
            "Iter: 17730  \tTraining Loss: -481.6610    \n",
            "    Negative Log Likelihood: 29.6792\tSigma2 Prior: -511.3462\tRegularization: 0.0060\n",
            "Iter: 17740  \tTraining Loss: -495.2504    \n",
            "    Negative Log Likelihood: 31.4851\tSigma2 Prior: -526.7415\tRegularization: 0.0060\n",
            "Iter: 17750  \tTraining Loss: -492.4839    \n",
            "    Negative Log Likelihood: 36.9222\tSigma2 Prior: -529.4121\tRegularization: 0.0060\n",
            "Iter: 17760  \tTraining Loss: -516.6032    \n",
            "    Negative Log Likelihood: 32.4402\tSigma2 Prior: -549.0494\tRegularization: 0.0060\n",
            "Iter: 17770  \tTraining Loss: -506.7591    \n",
            "    Negative Log Likelihood: 28.2120\tSigma2 Prior: -534.9771\tRegularization: 0.0060\n",
            "Iter: 17780  \tTraining Loss: -504.3546    \n",
            "    Negative Log Likelihood: 38.4379\tSigma2 Prior: -542.7985\tRegularization: 0.0060\n",
            "Iter: 17790  \tTraining Loss: -467.0947    \n",
            "    Negative Log Likelihood: 36.1354\tSigma2 Prior: -503.2361\tRegularization: 0.0060\n",
            "Iter: 17800  \tTraining Loss: -492.8671    \n",
            "    Negative Log Likelihood: 29.3358\tSigma2 Prior: -522.2089\tRegularization: 0.0060\n",
            "Iter: 17810  \tTraining Loss: -479.9987    \n",
            "    Negative Log Likelihood: 36.5432\tSigma2 Prior: -516.5479\tRegularization: 0.0060\n",
            "Iter: 17820  \tTraining Loss: -523.3298    \n",
            "    Negative Log Likelihood: 31.7934\tSigma2 Prior: -555.1291\tRegularization: 0.0060\n",
            "Iter: 17830  \tTraining Loss: -404.1609    \n",
            "    Negative Log Likelihood: 34.8410\tSigma2 Prior: -439.0079\tRegularization: 0.0060\n",
            "Iter: 17840  \tTraining Loss: -490.4209    \n",
            "    Negative Log Likelihood: 28.0005\tSigma2 Prior: -518.4274\tRegularization: 0.0060\n",
            "Iter: 17850  \tTraining Loss: -512.3376    \n",
            "    Negative Log Likelihood: 29.3228\tSigma2 Prior: -541.6664\tRegularization: 0.0060\n",
            "Iter: 17860  \tTraining Loss: -501.2993    \n",
            "    Negative Log Likelihood: 28.3203\tSigma2 Prior: -529.6255\tRegularization: 0.0060\n",
            "Iter: 17870  \tTraining Loss: -496.4902    \n",
            "    Negative Log Likelihood: 31.7277\tSigma2 Prior: -528.2239\tRegularization: 0.0060\n",
            "Iter: 17880  \tTraining Loss: -449.4020    \n",
            "    Negative Log Likelihood: 33.3234\tSigma2 Prior: -482.7314\tRegularization: 0.0060\n",
            "Iter: 17890  \tTraining Loss: -476.2794    \n",
            "    Negative Log Likelihood: 26.3787\tSigma2 Prior: -502.6641\tRegularization: 0.0060\n",
            "Iter: 17900  \tTraining Loss: -450.2698    \n",
            "    Negative Log Likelihood: 33.5291\tSigma2 Prior: -483.8049\tRegularization: 0.0060\n",
            "Iter: 17910  \tTraining Loss: -524.0187    \n",
            "    Negative Log Likelihood: 27.6621\tSigma2 Prior: -551.6868\tRegularization: 0.0060\n",
            "Iter: 17920  \tTraining Loss: -484.1176    \n",
            "    Negative Log Likelihood: 40.1451\tSigma2 Prior: -524.2687\tRegularization: 0.0060\n",
            "Iter: 17930  \tTraining Loss: -496.6888    \n",
            "    Negative Log Likelihood: 29.7077\tSigma2 Prior: -526.4025\tRegularization: 0.0060\n",
            "Iter: 17940  \tTraining Loss: -498.1638    \n",
            "    Negative Log Likelihood: 30.5569\tSigma2 Prior: -528.7267\tRegularization: 0.0060\n",
            "Iter: 17950  \tTraining Loss: -513.5543    \n",
            "    Negative Log Likelihood: 29.7226\tSigma2 Prior: -543.2830\tRegularization: 0.0060\n",
            "Iter: 17960  \tTraining Loss: -487.8423    \n",
            "    Negative Log Likelihood: 39.3296\tSigma2 Prior: -527.1779\tRegularization: 0.0060\n",
            "Iter: 17970  \tTraining Loss: -474.2745    \n",
            "    Negative Log Likelihood: 28.1378\tSigma2 Prior: -502.4183\tRegularization: 0.0060\n",
            "Iter: 17980  \tTraining Loss: -513.0240    \n",
            "    Negative Log Likelihood: 35.1694\tSigma2 Prior: -548.1994\tRegularization: 0.0060\n",
            "Iter: 17990  \tTraining Loss: -497.0519    \n",
            "    Negative Log Likelihood: 29.3153\tSigma2 Prior: -526.3732\tRegularization: 0.0060\n",
            "Iter: 18000  \tTraining Loss: -503.7834    \n",
            "    Negative Log Likelihood: 28.0423\tSigma2 Prior: -531.8318\tRegularization: 0.0060\n",
            "Iter: 18010  \tTraining Loss: -498.4149    \n",
            "    Negative Log Likelihood: 39.5565\tSigma2 Prior: -537.9774\tRegularization: 0.0060\n",
            "Iter: 18020  \tTraining Loss: -493.3309    \n",
            "    Negative Log Likelihood: 27.7055\tSigma2 Prior: -521.0424\tRegularization: 0.0060\n",
            "Iter: 18030  \tTraining Loss: -499.5540    \n",
            "    Negative Log Likelihood: 33.2588\tSigma2 Prior: -532.8188\tRegularization: 0.0060\n",
            "Iter: 18040  \tTraining Loss: -489.8098    \n",
            "    Negative Log Likelihood: 30.7419\tSigma2 Prior: -520.5577\tRegularization: 0.0060\n",
            "Iter: 18050  \tTraining Loss: -515.9424    \n",
            "    Negative Log Likelihood: 32.1985\tSigma2 Prior: -548.1470\tRegularization: 0.0060\n",
            "Iter: 18060  \tTraining Loss: -467.9010    \n",
            "    Negative Log Likelihood: 34.8596\tSigma2 Prior: -502.7666\tRegularization: 0.0060\n",
            "Iter: 18070  \tTraining Loss: -495.5080    \n",
            "    Negative Log Likelihood: 29.7290\tSigma2 Prior: -525.2430\tRegularization: 0.0060\n",
            "Iter: 18080  \tTraining Loss: -479.3437    \n",
            "    Negative Log Likelihood: 40.0528\tSigma2 Prior: -519.4025\tRegularization: 0.0060\n",
            "Iter: 18090  \tTraining Loss: -470.8295    \n",
            "    Negative Log Likelihood: 32.6370\tSigma2 Prior: -503.4726\tRegularization: 0.0060\n",
            "Iter: 18100  \tTraining Loss: -522.9830    \n",
            "    Negative Log Likelihood: 33.5106\tSigma2 Prior: -556.4996\tRegularization: 0.0060\n",
            "Iter: 18110  \tTraining Loss: -497.5253    \n",
            "    Negative Log Likelihood: 34.2582\tSigma2 Prior: -531.7896\tRegularization: 0.0060\n",
            "Iter: 18120  \tTraining Loss: -512.3792    \n",
            "    Negative Log Likelihood: 27.8778\tSigma2 Prior: -540.2630\tRegularization: 0.0060\n",
            "Iter: 18130  \tTraining Loss: -473.8096    \n",
            "    Negative Log Likelihood: 36.6644\tSigma2 Prior: -510.4800\tRegularization: 0.0060\n",
            "Iter: 18140  \tTraining Loss: -496.8970    \n",
            "    Negative Log Likelihood: 34.4620\tSigma2 Prior: -531.3650\tRegularization: 0.0060\n",
            "Iter: 18150  \tTraining Loss: -513.1403    \n",
            "    Negative Log Likelihood: 30.5285\tSigma2 Prior: -543.6748\tRegularization: 0.0060\n",
            "Iter: 18160  \tTraining Loss: -499.4978    \n",
            "    Negative Log Likelihood: 36.6203\tSigma2 Prior: -536.1241\tRegularization: 0.0060\n",
            "Iter: 18170  \tTraining Loss: -491.7608    \n",
            "    Negative Log Likelihood: 30.1339\tSigma2 Prior: -521.9007\tRegularization: 0.0060\n",
            "Iter: 18180  \tTraining Loss: -466.6953    \n",
            "    Negative Log Likelihood: 36.9065\tSigma2 Prior: -503.6078\tRegularization: 0.0060\n",
            "Iter: 18190  \tTraining Loss: -504.2558    \n",
            "    Negative Log Likelihood: 28.7639\tSigma2 Prior: -533.0258\tRegularization: 0.0060\n",
            "Iter: 18200  \tTraining Loss: -429.2651    \n",
            "    Negative Log Likelihood: 37.9979\tSigma2 Prior: -467.2690\tRegularization: 0.0060\n",
            "Iter: 18210  \tTraining Loss: -500.4076    \n",
            "    Negative Log Likelihood: 30.7251\tSigma2 Prior: -531.1387\tRegularization: 0.0060\n",
            "Iter: 18220  \tTraining Loss: -470.3755    \n",
            "    Negative Log Likelihood: 29.7979\tSigma2 Prior: -500.1795\tRegularization: 0.0060\n",
            "Iter: 18230  \tTraining Loss: -493.4038    \n",
            "    Negative Log Likelihood: 31.6255\tSigma2 Prior: -525.0353\tRegularization: 0.0060\n",
            "Iter: 18240  \tTraining Loss: -506.6187    \n",
            "    Negative Log Likelihood: 30.9498\tSigma2 Prior: -537.5746\tRegularization: 0.0060\n",
            "Iter: 18250  \tTraining Loss: -491.1960    \n",
            "    Negative Log Likelihood: 32.1818\tSigma2 Prior: -523.3839\tRegularization: 0.0061\n",
            "Iter: 18260  \tTraining Loss: -485.8043    \n",
            "    Negative Log Likelihood: 32.0864\tSigma2 Prior: -517.8967\tRegularization: 0.0061\n",
            "Iter: 18270  \tTraining Loss: -487.9716    \n",
            "    Negative Log Likelihood: 34.3414\tSigma2 Prior: -522.3190\tRegularization: 0.0061\n",
            "Iter: 18280  \tTraining Loss: -496.8983    \n",
            "    Negative Log Likelihood: 28.4127\tSigma2 Prior: -525.3170\tRegularization: 0.0061\n",
            "Iter: 18290  \tTraining Loss: -483.2410    \n",
            "    Negative Log Likelihood: 35.1936\tSigma2 Prior: -518.4406\tRegularization: 0.0061\n",
            "Iter: 18300  \tTraining Loss: -507.3130    \n",
            "    Negative Log Likelihood: 26.9401\tSigma2 Prior: -534.2592\tRegularization: 0.0061\n",
            "Iter: 18310  \tTraining Loss: -516.1705    \n",
            "    Negative Log Likelihood: 33.9163\tSigma2 Prior: -550.0928\tRegularization: 0.0061\n",
            "Iter: 18320  \tTraining Loss: -496.9890    \n",
            "    Negative Log Likelihood: 33.6839\tSigma2 Prior: -530.6790\tRegularization: 0.0061\n",
            "Iter: 18330  \tTraining Loss: -477.1475    \n",
            "    Negative Log Likelihood: 31.4076\tSigma2 Prior: -508.5611\tRegularization: 0.0061\n",
            "Iter: 18340  \tTraining Loss: -492.1473    \n",
            "    Negative Log Likelihood: 28.1291\tSigma2 Prior: -520.2825\tRegularization: 0.0061\n",
            "Iter: 18350  \tTraining Loss: -511.8249    \n",
            "    Negative Log Likelihood: 34.6427\tSigma2 Prior: -546.4737\tRegularization: 0.0061\n",
            "Iter: 18360  \tTraining Loss: -488.8057    \n",
            "    Negative Log Likelihood: 28.1576\tSigma2 Prior: -516.9694\tRegularization: 0.0061\n",
            "Iter: 18370  \tTraining Loss: -506.1272    \n",
            "    Negative Log Likelihood: 28.2687\tSigma2 Prior: -534.4020\tRegularization: 0.0061\n",
            "Iter: 18380  \tTraining Loss: -467.9785    \n",
            "    Negative Log Likelihood: 34.2999\tSigma2 Prior: -502.2845\tRegularization: 0.0061\n",
            "Iter: 18390  \tTraining Loss: -488.2373    \n",
            "    Negative Log Likelihood: 27.7179\tSigma2 Prior: -515.9613\tRegularization: 0.0061\n",
            "Iter: 18400  \tTraining Loss: -514.2492    \n",
            "    Negative Log Likelihood: 37.1308\tSigma2 Prior: -551.3861\tRegularization: 0.0061\n",
            "Iter: 18410  \tTraining Loss: -515.3870    \n",
            "    Negative Log Likelihood: 33.0389\tSigma2 Prior: -548.4321\tRegularization: 0.0061\n",
            "Iter: 18420  \tTraining Loss: -465.1984    \n",
            "    Negative Log Likelihood: 35.6364\tSigma2 Prior: -500.8409\tRegularization: 0.0061\n",
            "Iter: 18430  \tTraining Loss: -502.4613    \n",
            "    Negative Log Likelihood: 29.2297\tSigma2 Prior: -531.6971\tRegularization: 0.0061\n",
            "Iter: 18440  \tTraining Loss: -481.0298    \n",
            "    Negative Log Likelihood: 42.0187\tSigma2 Prior: -523.0546\tRegularization: 0.0061\n",
            "Iter: 18450  \tTraining Loss: -498.3685    \n",
            "    Negative Log Likelihood: 25.0240\tSigma2 Prior: -523.3986\tRegularization: 0.0061\n",
            "Iter: 18460  \tTraining Loss: -509.6243    \n",
            "    Negative Log Likelihood: 35.4386\tSigma2 Prior: -545.0690\tRegularization: 0.0061\n",
            "Iter: 18470  \tTraining Loss: -502.4078    \n",
            "    Negative Log Likelihood: 36.6298\tSigma2 Prior: -539.0436\tRegularization: 0.0061\n",
            "Iter: 18480  \tTraining Loss: -487.6079    \n",
            "    Negative Log Likelihood: 27.9660\tSigma2 Prior: -515.5800\tRegularization: 0.0061\n",
            "Iter: 18490  \tTraining Loss: -485.4096    \n",
            "    Negative Log Likelihood: 33.0364\tSigma2 Prior: -518.4520\tRegularization: 0.0061\n",
            "Iter: 18500  \tTraining Loss: -485.6886    \n",
            "    Negative Log Likelihood: 32.7430\tSigma2 Prior: -518.4377\tRegularization: 0.0061\n",
            "Iter: 18510  \tTraining Loss: -511.0545    \n",
            "    Negative Log Likelihood: 33.6623\tSigma2 Prior: -544.7230\tRegularization: 0.0061\n",
            "Iter: 18520  \tTraining Loss: -521.2501    \n",
            "    Negative Log Likelihood: 34.1951\tSigma2 Prior: -555.4513\tRegularization: 0.0061\n",
            "Iter: 18530  \tTraining Loss: -474.2615    \n",
            "    Negative Log Likelihood: 30.8216\tSigma2 Prior: -505.0892\tRegularization: 0.0061\n",
            "Iter: 18540  \tTraining Loss: -489.4818    \n",
            "    Negative Log Likelihood: 34.1676\tSigma2 Prior: -523.6555\tRegularization: 0.0061\n",
            "Iter: 18550  \tTraining Loss: -495.1617    \n",
            "    Negative Log Likelihood: 29.7899\tSigma2 Prior: -524.9576\tRegularization: 0.0061\n",
            "Iter: 18560  \tTraining Loss: -488.1189    \n",
            "    Negative Log Likelihood: 29.8625\tSigma2 Prior: -517.9875\tRegularization: 0.0061\n",
            "Iter: 18570  \tTraining Loss: -513.7828    \n",
            "    Negative Log Likelihood: 39.4095\tSigma2 Prior: -553.1984\tRegularization: 0.0061\n",
            "Iter: 18580  \tTraining Loss: -474.4802    \n",
            "    Negative Log Likelihood: 34.2686\tSigma2 Prior: -508.7549\tRegularization: 0.0061\n",
            "Iter: 18590  \tTraining Loss: -495.6506    \n",
            "    Negative Log Likelihood: 26.5480\tSigma2 Prior: -522.2048\tRegularization: 0.0061\n",
            "Iter: 18600  \tTraining Loss: -507.1393    \n",
            "    Negative Log Likelihood: 32.4804\tSigma2 Prior: -539.6259\tRegularization: 0.0061\n",
            "Iter: 18610  \tTraining Loss: -517.1682    \n",
            "    Negative Log Likelihood: 32.1462\tSigma2 Prior: -549.3205\tRegularization: 0.0061\n",
            "Iter: 18620  \tTraining Loss: -455.8492    \n",
            "    Negative Log Likelihood: 35.5805\tSigma2 Prior: -491.4359\tRegularization: 0.0061\n",
            "Iter: 18630  \tTraining Loss: -519.9992    \n",
            "    Negative Log Likelihood: 31.7840\tSigma2 Prior: -551.7893\tRegularization: 0.0061\n",
            "Iter: 18640  \tTraining Loss: -488.9105    \n",
            "    Negative Log Likelihood: 32.3291\tSigma2 Prior: -521.2457\tRegularization: 0.0061\n",
            "Iter: 18650  \tTraining Loss: -463.5584    \n",
            "    Negative Log Likelihood: 28.8351\tSigma2 Prior: -492.3995\tRegularization: 0.0061\n",
            "Iter: 18660  \tTraining Loss: -462.0252    \n",
            "    Negative Log Likelihood: 32.4766\tSigma2 Prior: -494.5079\tRegularization: 0.0061\n",
            "Iter: 18670  \tTraining Loss: -474.8831    \n",
            "    Negative Log Likelihood: 29.7744\tSigma2 Prior: -504.6636\tRegularization: 0.0061\n",
            "Iter: 18680  \tTraining Loss: -454.1521    \n",
            "    Negative Log Likelihood: 35.9049\tSigma2 Prior: -490.0631\tRegularization: 0.0061\n",
            "Iter: 18690  \tTraining Loss: -479.7268    \n",
            "    Negative Log Likelihood: 29.6459\tSigma2 Prior: -509.3788\tRegularization: 0.0061\n",
            "Iter: 18700  \tTraining Loss: -511.2641    \n",
            "    Negative Log Likelihood: 35.9030\tSigma2 Prior: -547.1733\tRegularization: 0.0061\n",
            "Iter: 18710  \tTraining Loss: -518.4990    \n",
            "    Negative Log Likelihood: 28.4686\tSigma2 Prior: -546.9736\tRegularization: 0.0061\n",
            "Iter: 18720  \tTraining Loss: -490.8763    \n",
            "    Negative Log Likelihood: 38.3997\tSigma2 Prior: -529.2821\tRegularization: 0.0061\n",
            "Iter: 18730  \tTraining Loss: -499.1109    \n",
            "    Negative Log Likelihood: 31.0346\tSigma2 Prior: -530.1516\tRegularization: 0.0061\n",
            "Iter: 18740  \tTraining Loss: -463.2488    \n",
            "    Negative Log Likelihood: 32.2625\tSigma2 Prior: -495.5174\tRegularization: 0.0061\n",
            "Iter: 18750  \tTraining Loss: -482.8193    \n",
            "    Negative Log Likelihood: 34.1542\tSigma2 Prior: -516.9796\tRegularization: 0.0061\n",
            "Iter: 18760  \tTraining Loss: -519.7635    \n",
            "    Negative Log Likelihood: 28.8903\tSigma2 Prior: -548.6600\tRegularization: 0.0061\n",
            "Iter: 18770  \tTraining Loss: -498.0260    \n",
            "    Negative Log Likelihood: 36.0197\tSigma2 Prior: -534.0518\tRegularization: 0.0061\n",
            "Iter: 18780  \tTraining Loss: -497.9390    \n",
            "    Negative Log Likelihood: 35.8517\tSigma2 Prior: -533.7968\tRegularization: 0.0061\n",
            "Iter: 18790  \tTraining Loss: -478.7977    \n",
            "    Negative Log Likelihood: 28.0956\tSigma2 Prior: -506.8995\tRegularization: 0.0061\n",
            "Iter: 18800  \tTraining Loss: -468.1931    \n",
            "    Negative Log Likelihood: 34.0926\tSigma2 Prior: -502.2917\tRegularization: 0.0061\n",
            "Iter: 18810  \tTraining Loss: -515.3615    \n",
            "    Negative Log Likelihood: 31.1427\tSigma2 Prior: -546.5103\tRegularization: 0.0061\n",
            "Iter: 18820  \tTraining Loss: -509.6592    \n",
            "    Negative Log Likelihood: 35.5008\tSigma2 Prior: -545.1661\tRegularization: 0.0061\n",
            "Iter: 18830  \tTraining Loss: -458.7166    \n",
            "    Negative Log Likelihood: 28.9091\tSigma2 Prior: -487.6319\tRegularization: 0.0061\n",
            "Iter: 18840  \tTraining Loss: -516.8109    \n",
            "    Negative Log Likelihood: 33.7984\tSigma2 Prior: -550.6155\tRegularization: 0.0061\n",
            "Iter: 18850  \tTraining Loss: -485.7416    \n",
            "    Negative Log Likelihood: 31.3678\tSigma2 Prior: -517.1155\tRegularization: 0.0061\n",
            "Iter: 18860  \tTraining Loss: -484.8692    \n",
            "    Negative Log Likelihood: 31.8785\tSigma2 Prior: -516.7538\tRegularization: 0.0061\n",
            "Iter: 18870  \tTraining Loss: -469.1141    \n",
            "    Negative Log Likelihood: 28.3161\tSigma2 Prior: -497.4363\tRegularization: 0.0061\n",
            "Iter: 18880  \tTraining Loss: -508.2942    \n",
            "    Negative Log Likelihood: 30.3635\tSigma2 Prior: -538.6638\tRegularization: 0.0061\n",
            "Iter: 18890  \tTraining Loss: -499.7948    \n",
            "    Negative Log Likelihood: 32.0920\tSigma2 Prior: -531.8929\tRegularization: 0.0061\n",
            "Iter: 18900  \tTraining Loss: -473.2705    \n",
            "    Negative Log Likelihood: 32.3390\tSigma2 Prior: -505.6157\tRegularization: 0.0062\n",
            "Iter: 18910  \tTraining Loss: -469.0490    \n",
            "    Negative Log Likelihood: 32.0927\tSigma2 Prior: -501.1478\tRegularization: 0.0062\n",
            "Iter: 18920  \tTraining Loss: -494.7104    \n",
            "    Negative Log Likelihood: 33.3754\tSigma2 Prior: -528.0920\tRegularization: 0.0062\n",
            "Iter: 18930  \tTraining Loss: -470.5379    \n",
            "    Negative Log Likelihood: 28.1354\tSigma2 Prior: -498.6795\tRegularization: 0.0062\n",
            "Iter: 18940  \tTraining Loss: -521.4743    \n",
            "    Negative Log Likelihood: 28.5711\tSigma2 Prior: -550.0516\tRegularization: 0.0062\n",
            "Iter: 18950  \tTraining Loss: -507.5674    \n",
            "    Negative Log Likelihood: 35.9608\tSigma2 Prior: -543.5344\tRegularization: 0.0062\n",
            "Iter: 18960  \tTraining Loss: -498.0995    \n",
            "    Negative Log Likelihood: 28.7884\tSigma2 Prior: -526.8940\tRegularization: 0.0062\n",
            "Iter: 18970  \tTraining Loss: -450.0027    \n",
            "    Negative Log Likelihood: 41.6363\tSigma2 Prior: -491.6452\tRegularization: 0.0062\n",
            "Iter: 18980  \tTraining Loss: -502.2411    \n",
            "    Negative Log Likelihood: 29.6131\tSigma2 Prior: -531.8604\tRegularization: 0.0062\n",
            "Iter: 18990  \tTraining Loss: -478.5732    \n",
            "    Negative Log Likelihood: 32.6997\tSigma2 Prior: -511.2790\tRegularization: 0.0062\n",
            "Iter: 19000  \tTraining Loss: -508.5050    \n",
            "    Negative Log Likelihood: 31.4450\tSigma2 Prior: -539.9562\tRegularization: 0.0062\n",
            "Iter: 19010  \tTraining Loss: -474.4815    \n",
            "    Negative Log Likelihood: 28.9693\tSigma2 Prior: -503.4570\tRegularization: 0.0062\n",
            "Iter: 19020  \tTraining Loss: -505.8607    \n",
            "    Negative Log Likelihood: 36.4178\tSigma2 Prior: -542.2846\tRegularization: 0.0062\n",
            "Iter: 19030  \tTraining Loss: -503.4519    \n",
            "    Negative Log Likelihood: 33.4782\tSigma2 Prior: -536.9363\tRegularization: 0.0062\n",
            "Iter: 19040  \tTraining Loss: -501.7720    \n",
            "    Negative Log Likelihood: 30.8224\tSigma2 Prior: -532.6006\tRegularization: 0.0062\n",
            "Iter: 19050  \tTraining Loss: -513.7784    \n",
            "    Negative Log Likelihood: 33.9366\tSigma2 Prior: -547.7212\tRegularization: 0.0062\n",
            "Iter: 19060  \tTraining Loss: -483.7578    \n",
            "    Negative Log Likelihood: 36.8371\tSigma2 Prior: -520.6011\tRegularization: 0.0062\n",
            "Iter: 19070  \tTraining Loss: -522.0310    \n",
            "    Negative Log Likelihood: 28.0451\tSigma2 Prior: -550.0823\tRegularization: 0.0062\n",
            "Iter: 19080  \tTraining Loss: -503.5434    \n",
            "    Negative Log Likelihood: 32.7277\tSigma2 Prior: -536.2772\tRegularization: 0.0062\n",
            "Iter: 19090  \tTraining Loss: -453.0739    \n",
            "    Negative Log Likelihood: 29.3388\tSigma2 Prior: -482.4189\tRegularization: 0.0062\n",
            "Iter: 19100  \tTraining Loss: -471.5331    \n",
            "    Negative Log Likelihood: 29.7168\tSigma2 Prior: -501.2561\tRegularization: 0.0062\n",
            "Iter: 19110  \tTraining Loss: -502.9025    \n",
            "    Negative Log Likelihood: 31.7796\tSigma2 Prior: -534.6883\tRegularization: 0.0062\n",
            "Iter: 19120  \tTraining Loss: -495.2877    \n",
            "    Negative Log Likelihood: 35.3637\tSigma2 Prior: -530.6575\tRegularization: 0.0062\n",
            "Iter: 19130  \tTraining Loss: -492.8255    \n",
            "    Negative Log Likelihood: 32.2053\tSigma2 Prior: -525.0370\tRegularization: 0.0062\n",
            "Iter: 19140  \tTraining Loss: -489.6812    \n",
            "    Negative Log Likelihood: 38.5689\tSigma2 Prior: -528.2563\tRegularization: 0.0062\n",
            "Iter: 19150  \tTraining Loss: -509.2507    \n",
            "    Negative Log Likelihood: 27.5249\tSigma2 Prior: -536.7818\tRegularization: 0.0062\n",
            "Iter: 19160  \tTraining Loss: -498.1952    \n",
            "    Negative Log Likelihood: 33.1723\tSigma2 Prior: -531.3736\tRegularization: 0.0062\n",
            "Iter: 19170  \tTraining Loss: -490.6029    \n",
            "    Negative Log Likelihood: 38.9736\tSigma2 Prior: -529.5828\tRegularization: 0.0062\n",
            "Iter: 19180  \tTraining Loss: -511.4606    \n",
            "    Negative Log Likelihood: 27.1422\tSigma2 Prior: -538.6091\tRegularization: 0.0062\n",
            "Iter: 19190  \tTraining Loss: -488.5874    \n",
            "    Negative Log Likelihood: 34.5615\tSigma2 Prior: -523.1551\tRegularization: 0.0062\n",
            "Iter: 19200  \tTraining Loss: -460.6786    \n",
            "    Negative Log Likelihood: 34.2674\tSigma2 Prior: -494.9523\tRegularization: 0.0062\n",
            "Iter: 19210  \tTraining Loss: -468.8597    \n",
            "    Negative Log Likelihood: 32.2857\tSigma2 Prior: -501.1516\tRegularization: 0.0062\n",
            "Iter: 19220  \tTraining Loss: -489.5722    \n",
            "    Negative Log Likelihood: 31.3449\tSigma2 Prior: -520.9233\tRegularization: 0.0062\n",
            "Iter: 19230  \tTraining Loss: -506.1822    \n",
            "    Negative Log Likelihood: 37.0298\tSigma2 Prior: -543.2182\tRegularization: 0.0062\n",
            "Iter: 19240  \tTraining Loss: -505.8154    \n",
            "    Negative Log Likelihood: 37.7120\tSigma2 Prior: -543.5336\tRegularization: 0.0062\n",
            "Iter: 19250  \tTraining Loss: -513.3468    \n",
            "    Negative Log Likelihood: 30.6199\tSigma2 Prior: -543.9730\tRegularization: 0.0062\n",
            "Iter: 19260  \tTraining Loss: -502.5293    \n",
            "    Negative Log Likelihood: 34.3434\tSigma2 Prior: -536.8789\tRegularization: 0.0062\n",
            "Iter: 19270  \tTraining Loss: -514.7738    \n",
            "    Negative Log Likelihood: 28.1421\tSigma2 Prior: -542.9221\tRegularization: 0.0062\n",
            "Iter: 19280  \tTraining Loss: -504.2181    \n",
            "    Negative Log Likelihood: 25.0527\tSigma2 Prior: -529.2770\tRegularization: 0.0062\n",
            "Iter: 19290  \tTraining Loss: -515.5511    \n",
            "    Negative Log Likelihood: 32.0733\tSigma2 Prior: -547.6307\tRegularization: 0.0062\n",
            "Iter: 19300  \tTraining Loss: -516.5742    \n",
            "    Negative Log Likelihood: 33.7549\tSigma2 Prior: -550.3354\tRegularization: 0.0062\n",
            "Iter: 19310  \tTraining Loss: -479.4665    \n",
            "    Negative Log Likelihood: 32.0884\tSigma2 Prior: -511.5612\tRegularization: 0.0062\n",
            "Iter: 19320  \tTraining Loss: -513.3641    \n",
            "    Negative Log Likelihood: 30.9783\tSigma2 Prior: -544.3486\tRegularization: 0.0062\n",
            "Iter: 19330  \tTraining Loss: -470.1054    \n",
            "    Negative Log Likelihood: 34.8801\tSigma2 Prior: -504.9917\tRegularization: 0.0062\n",
            "Iter: 19340  \tTraining Loss: -513.0112    \n",
            "    Negative Log Likelihood: 30.2135\tSigma2 Prior: -543.2310\tRegularization: 0.0062\n",
            "Iter: 19350  \tTraining Loss: -489.4689    \n",
            "    Negative Log Likelihood: 32.6065\tSigma2 Prior: -522.0817\tRegularization: 0.0062\n",
            "Iter: 19360  \tTraining Loss: -498.2195    \n",
            "    Negative Log Likelihood: 27.5414\tSigma2 Prior: -525.7670\tRegularization: 0.0062\n",
            "Iter: 19370  \tTraining Loss: -488.2817    \n",
            "    Negative Log Likelihood: 34.7704\tSigma2 Prior: -523.0583\tRegularization: 0.0062\n",
            "Iter: 19380  \tTraining Loss: -481.6786    \n",
            "    Negative Log Likelihood: 29.5888\tSigma2 Prior: -511.2736\tRegularization: 0.0062\n",
            "Iter: 19390  \tTraining Loss: -487.3002    \n",
            "    Negative Log Likelihood: 30.9959\tSigma2 Prior: -518.3024\tRegularization: 0.0062\n",
            "Iter: 19400  \tTraining Loss: -507.4283    \n",
            "    Negative Log Likelihood: 36.3876\tSigma2 Prior: -543.8221\tRegularization: 0.0062\n",
            "Iter: 19410  \tTraining Loss: -506.3017    \n",
            "    Negative Log Likelihood: 36.0316\tSigma2 Prior: -542.3395\tRegularization: 0.0062\n",
            "Iter: 19420  \tTraining Loss: -495.7177    \n",
            "    Negative Log Likelihood: 31.9289\tSigma2 Prior: -527.6528\tRegularization: 0.0062\n",
            "Iter: 19430  \tTraining Loss: -503.5686    \n",
            "    Negative Log Likelihood: 31.1930\tSigma2 Prior: -534.7678\tRegularization: 0.0062\n",
            "Iter: 19440  \tTraining Loss: -524.1868    \n",
            "    Negative Log Likelihood: 32.4125\tSigma2 Prior: -556.6055\tRegularization: 0.0062\n",
            "Iter: 19450  \tTraining Loss: -443.2278    \n",
            "    Negative Log Likelihood: 34.9959\tSigma2 Prior: -478.2299\tRegularization: 0.0062\n",
            "Iter: 19460  \tTraining Loss: -506.1009    \n",
            "    Negative Log Likelihood: 28.3648\tSigma2 Prior: -534.4719\tRegularization: 0.0062\n",
            "Iter: 19470  \tTraining Loss: -479.1081    \n",
            "    Negative Log Likelihood: 37.3310\tSigma2 Prior: -516.4453\tRegularization: 0.0062\n",
            "Iter: 19480  \tTraining Loss: -480.2446    \n",
            "    Negative Log Likelihood: 27.6873\tSigma2 Prior: -507.9382\tRegularization: 0.0062\n",
            "Iter: 19490  \tTraining Loss: -469.3883    \n",
            "    Negative Log Likelihood: 33.6508\tSigma2 Prior: -503.0453\tRegularization: 0.0062\n",
            "Iter: 19500  \tTraining Loss: -495.3912    \n",
            "    Negative Log Likelihood: 36.1105\tSigma2 Prior: -531.5079\tRegularization: 0.0062\n",
            "Iter: 19510  \tTraining Loss: -483.6181    \n",
            "    Negative Log Likelihood: 33.2353\tSigma2 Prior: -516.8597\tRegularization: 0.0062\n",
            "Iter: 19520  \tTraining Loss: -506.3986    \n",
            "    Negative Log Likelihood: 25.3032\tSigma2 Prior: -531.7080\tRegularization: 0.0062\n",
            "Iter: 19530  \tTraining Loss: -511.4709    \n",
            "    Negative Log Likelihood: 36.9967\tSigma2 Prior: -548.4739\tRegularization: 0.0062\n",
            "Iter: 19540  \tTraining Loss: -485.8022    \n",
            "    Negative Log Likelihood: 28.6111\tSigma2 Prior: -514.4195\tRegularization: 0.0062\n",
            "Iter: 19550  \tTraining Loss: -503.5370    \n",
            "    Negative Log Likelihood: 32.1840\tSigma2 Prior: -535.7273\tRegularization: 0.0062\n",
            "Iter: 19560  \tTraining Loss: -485.5495    \n",
            "    Negative Log Likelihood: 34.8967\tSigma2 Prior: -520.4525\tRegularization: 0.0062\n",
            "Iter: 19570  \tTraining Loss: -504.9337    \n",
            "    Negative Log Likelihood: 30.4458\tSigma2 Prior: -535.3857\tRegularization: 0.0062\n",
            "Iter: 19580  \tTraining Loss: -484.5992    \n",
            "    Negative Log Likelihood: 34.3979\tSigma2 Prior: -519.0034\tRegularization: 0.0062\n",
            "Iter: 19590  \tTraining Loss: -383.0148    \n",
            "    Negative Log Likelihood: 34.2439\tSigma2 Prior: -417.2650\tRegularization: 0.0062\n",
            "Iter: 19600  \tTraining Loss: -505.6341    \n",
            "    Negative Log Likelihood: 28.0260\tSigma2 Prior: -533.6663\tRegularization: 0.0062\n",
            "Iter: 19610  \tTraining Loss: -488.0704    \n",
            "    Negative Log Likelihood: 33.5049\tSigma2 Prior: -521.5815\tRegularization: 0.0063\n",
            "Iter: 19620  \tTraining Loss: -523.9144    \n",
            "    Negative Log Likelihood: 37.7041\tSigma2 Prior: -561.6248\tRegularization: 0.0063\n",
            "Iter: 19630  \tTraining Loss: -502.6329    \n",
            "    Negative Log Likelihood: 35.0655\tSigma2 Prior: -537.7047\tRegularization: 0.0063\n",
            "Iter: 19640  \tTraining Loss: -515.6346    \n",
            "    Negative Log Likelihood: 30.3840\tSigma2 Prior: -546.0248\tRegularization: 0.0063\n",
            "Iter: 19650  \tTraining Loss: -513.1328    \n",
            "    Negative Log Likelihood: 30.5242\tSigma2 Prior: -543.6633\tRegularization: 0.0063\n",
            "Iter: 19660  \tTraining Loss: -490.5837    \n",
            "    Negative Log Likelihood: 38.2402\tSigma2 Prior: -528.8302\tRegularization: 0.0063\n",
            "Iter: 19670  \tTraining Loss: -489.0148    \n",
            "    Negative Log Likelihood: 29.2358\tSigma2 Prior: -518.2568\tRegularization: 0.0063\n",
            "Iter: 19680  \tTraining Loss: -470.1384    \n",
            "    Negative Log Likelihood: 34.4887\tSigma2 Prior: -504.6333\tRegularization: 0.0063\n",
            "Iter: 19690  \tTraining Loss: -488.6241    \n",
            "    Negative Log Likelihood: 32.7864\tSigma2 Prior: -521.4167\tRegularization: 0.0063\n",
            "Iter: 19700  \tTraining Loss: -499.9782    \n",
            "    Negative Log Likelihood: 30.3543\tSigma2 Prior: -530.3387\tRegularization: 0.0063\n",
            "Iter: 19710  \tTraining Loss: -507.1849    \n",
            "    Negative Log Likelihood: 31.4548\tSigma2 Prior: -538.6460\tRegularization: 0.0063\n",
            "Iter: 19720  \tTraining Loss: -508.5654    \n",
            "    Negative Log Likelihood: 29.1460\tSigma2 Prior: -537.7177\tRegularization: 0.0063\n",
            "Iter: 19730  \tTraining Loss: -500.1688    \n",
            "    Negative Log Likelihood: 33.7254\tSigma2 Prior: -533.9005\tRegularization: 0.0063\n",
            "Iter: 19740  \tTraining Loss: -468.2664    \n",
            "    Negative Log Likelihood: 27.3772\tSigma2 Prior: -495.6498\tRegularization: 0.0063\n",
            "Iter: 19750  \tTraining Loss: -502.6833    \n",
            "    Negative Log Likelihood: 28.9490\tSigma2 Prior: -531.6385\tRegularization: 0.0063\n",
            "Iter: 19760  \tTraining Loss: -454.2011    \n",
            "    Negative Log Likelihood: 33.2581\tSigma2 Prior: -487.4655\tRegularization: 0.0063\n",
            "Iter: 19770  \tTraining Loss: -486.5738    \n",
            "    Negative Log Likelihood: 27.1732\tSigma2 Prior: -513.7533\tRegularization: 0.0063\n",
            "Iter: 19780  \tTraining Loss: -458.9803    \n",
            "    Negative Log Likelihood: 32.3077\tSigma2 Prior: -491.2943\tRegularization: 0.0063\n",
            "Iter: 19790  \tTraining Loss: -499.0270    \n",
            "    Negative Log Likelihood: 33.7694\tSigma2 Prior: -532.8027\tRegularization: 0.0063\n",
            "Iter: 19800  \tTraining Loss: -490.3617    \n",
            "    Negative Log Likelihood: 30.9522\tSigma2 Prior: -521.3201\tRegularization: 0.0063\n",
            "Iter: 19810  \tTraining Loss: -496.5987    \n",
            "    Negative Log Likelihood: 34.4482\tSigma2 Prior: -531.0532\tRegularization: 0.0063\n",
            "Iter: 19820  \tTraining Loss: -521.2409    \n",
            "    Negative Log Likelihood: 37.4711\tSigma2 Prior: -558.7183\tRegularization: 0.0063\n",
            "Iter: 19830  \tTraining Loss: -486.5187    \n",
            "    Negative Log Likelihood: 28.0295\tSigma2 Prior: -514.5546\tRegularization: 0.0063\n",
            "Iter: 19840  \tTraining Loss: -512.6821    \n",
            "    Negative Log Likelihood: 29.3833\tSigma2 Prior: -542.0717\tRegularization: 0.0063\n",
            "Iter: 19850  \tTraining Loss: -444.0156    \n",
            "    Negative Log Likelihood: 33.5076\tSigma2 Prior: -477.5295\tRegularization: 0.0063\n",
            "Iter: 19860  \tTraining Loss: -515.2830    \n",
            "    Negative Log Likelihood: 28.0322\tSigma2 Prior: -543.3214\tRegularization: 0.0063\n",
            "Iter: 19870  \tTraining Loss: -480.0473    \n",
            "    Negative Log Likelihood: 34.9055\tSigma2 Prior: -514.9591\tRegularization: 0.0063\n",
            "Iter: 19880  \tTraining Loss: -498.3796    \n",
            "    Negative Log Likelihood: 27.9289\tSigma2 Prior: -526.3148\tRegularization: 0.0063\n",
            "Iter: 19890  \tTraining Loss: -497.2370    \n",
            "    Negative Log Likelihood: 34.3180\tSigma2 Prior: -531.5613\tRegularization: 0.0063\n",
            "Iter: 19900  \tTraining Loss: -488.1443    \n",
            "    Negative Log Likelihood: 31.7274\tSigma2 Prior: -519.8780\tRegularization: 0.0063\n",
            "Iter: 19910  \tTraining Loss: -526.2749    \n",
            "    Negative Log Likelihood: 34.2455\tSigma2 Prior: -560.5267\tRegularization: 0.0063\n",
            "Iter: 19920  \tTraining Loss: -508.7828    \n",
            "    Negative Log Likelihood: 31.2843\tSigma2 Prior: -540.0734\tRegularization: 0.0063\n",
            "Iter: 19930  \tTraining Loss: -475.9241    \n",
            "    Negative Log Likelihood: 32.2812\tSigma2 Prior: -508.2115\tRegularization: 0.0063\n",
            "Iter: 19940  \tTraining Loss: -447.9976    \n",
            "    Negative Log Likelihood: 30.6618\tSigma2 Prior: -478.6657\tRegularization: 0.0063\n",
            "Iter: 19950  \tTraining Loss: -479.5019    \n",
            "    Negative Log Likelihood: 25.9997\tSigma2 Prior: -505.5079\tRegularization: 0.0063\n",
            "Iter: 19960  \tTraining Loss: -468.6825    \n",
            "    Negative Log Likelihood: 32.9161\tSigma2 Prior: -501.6049\tRegularization: 0.0063\n",
            "Iter: 19970  \tTraining Loss: -475.8102    \n",
            "    Negative Log Likelihood: 38.6126\tSigma2 Prior: -514.4291\tRegularization: 0.0063\n",
            "Iter: 19980  \tTraining Loss: -474.8456    \n",
            "    Negative Log Likelihood: 26.5236\tSigma2 Prior: -501.3755\tRegularization: 0.0063\n",
            "Iter: 19990  \tTraining Loss: -495.4340    \n",
            "    Negative Log Likelihood: 33.9770\tSigma2 Prior: -529.4173\tRegularization: 0.0063\n",
            "Iter: 19999  \tTraining Loss: -466.3390    \n",
            "    Negative Log Likelihood: 33.5743\tSigma2 Prior: -499.9196\tRegularization: 0.0063\n",
            "Done training with 20000 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ4iuIT34wmI"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcct4Q_7APML"
      },
      "source": [
        "\n",
        "\n",
        "*   평가 데이터를 사용해 각 sequence에 해당하는 화자 id를 얻음\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpm3yetWbG8s",
        "outputId": "8bd20be5-0384-4072-b3dc-aa65518e56c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuZtArnpqyTz"
      },
      "source": [
        "predict_cluster_ids = model.predict(test_sequences[1], inference_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_cluster_ids)\n",
        "print(test_cluster_ids[1])"
      ],
      "metadata": {
        "id": "9rcCZp8DsB2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac19a6aa-7a5b-454c-bb27-2944210938a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37]\n",
            "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_cluster_ids = []\n",
        "test_record = []\n",
        "\n",
        "for(test_sequence, test_cluster_id) in zip(test_sequences.tolist(), test_cluster_ids.tolist()):\n",
        "\n",
        "  predicted_cluster_id = model.predict(test_sequence, inference_args)\n",
        "  predicted_cluster_ids.append(predicted_cluster_id)\n",
        "  acc = uisrnn.compute_sequence_match_accuracy(test_cluster_id, predicted_cluster_id)\n",
        "  test_record.append(acc)\n",
        "  print(f'예측 값 : {predicted_cluster_id}')\n",
        "  print(f'실제 값 : {test_cluster_id}')\n",
        "  print('*'*100)"
      ],
      "metadata": {
        "id": "LrJ3xXBrLzAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45512ea6-2de4-4e1c-c686-4ecb1f478e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37]\n",
            "실제 값 : ['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
            "실제 값 : ['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 0, 16, 17, 12, 12, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12]\n",
            "실제 값 : ['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 21, 21, 23, 24, 25, 26, 27, 28, 29, 21, 31, 32, 33, 34, 35, 36, 37, 38, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 52, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 55, 56, 57, 58, 59, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 53, 53, 53, 53, 53, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
            "실제 값 : ['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 38, 38, 38, 38, 38, 38, 38, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38]\n",
            "실제 값 : ['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
            "****************************************************************************************************\n",
            "예측 값 : [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 11, 12, 13, 7, 7, 7, 7, 7, 7, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
            "실제 값 : ['140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_2', '140_2', '140_2', '140_2', '140_2', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
            "실제 값 : ['146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 0, 0, 0, 0, 0, 0, 0, 0, 31, 32, 33, 34]\n",
            "실제 값 : ['152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5]\n",
            "실제 값 : ['175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_1', '175_1', '175_1', '175_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 11, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "실제 값 : ['315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_5', '315_5', '315_5', '315_5']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 51, 52, 53, 54, 55, 56]\n",
            "실제 값 : ['333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 22, 51, 8, 8, 8, 8, 8, 8, 8, 8, 8, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]\n",
            "실제 값 : ['361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 11, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1, 1, 1]\n",
            "실제 값 : ['374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0']\n",
            "****************************************************************************************************\n",
            "예측 값 : [23, 23, 23, 23, 23, 23, 23, 7, 8, 9, 10, 11, 12, 13, 14, 17, 17, 17, 17, 17, 17, 17, 17, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 27, 27, 27, 27, 27, 27, 27, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 17, 17, 17, 17, 17, 17, 17, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 27, 27, 27, 27]\n",
            "실제 값 : ['405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 62, 62, 62, 62, 62, 62, 62, 62, 62, 83, 84, 85, 86, 87, 88, 89, 90, 91]\n",
            "실제 값 : ['439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_2', '439_2', '439_2', '439_2', '439_2', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_2', '439_2', '439_2', '439_2', '439_2', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 58, 59, 60, 61, 62, 63, 64, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "실제 값 : ['442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "실제 값 : ['459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 1, 1, 1, 1, 1, 1, 1, 14, 15, 16, 17, 18, 19, 20, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
            "실제 값 : ['491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1']\n",
            "****************************************************************************************************\n",
            "예측 값 : [0, 1, 2, 3, 4, 5, 6, 7, 30, 30, 30, 30, 30, 30, 30, 30, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 49, 50]\n",
            "실제 값 : ['494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_2', '494_2', '494_2', '494_2', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0']\n",
            "****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_record"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krCsuikienw6",
        "outputId": "fb8a39f6-9a61-46e0-82dc-17a4b06e9645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6526315789473685,\n",
              " 0.6067415730337079,\n",
              " 0.44680851063829785,\n",
              " 0.7872340425531915,\n",
              " 0.5096153846153846,\n",
              " 0.813953488372093,\n",
              " 0.4895833333333333,\n",
              " 1.0,\n",
              " 0.35294117647058826,\n",
              " 1.0,\n",
              " 0.5957446808510638,\n",
              " 0.47959183673469385,\n",
              " 0.3888888888888889,\n",
              " 0.9891304347826086,\n",
              " 0.7804878048780488,\n",
              " 1.0,\n",
              " 0.8785046728971962,\n",
              " 0.3404255319148936,\n",
              " 1.0,\n",
              " 0.2727272727272727,\n",
              " 0.4838709677419355,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.5346534653465347,\n",
              " 0.550561797752809]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nd53VQ8pnCvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}